# Random Variables and Distributions {#sec-randomvariables}

{{< include _setupcode.qmd >}}

In @sec-fundamentals, we discussed the probability of an "event."  For statisticians, the events of interests center on measurements, or functions of those measurements, that we plan to take.  In this chapter, we begin to connect probability to data analysis.  Our goal is to reexamine concepts introduced in a probability course, relating them to their data-centric analogues, which will be discussed further in the next unit. 

## Random Variables
Consider collecting data; before the data is collected, we cannot predict with certainty what we will observe.  Therefore, we can think of each observation as the result of a random process.  These observations are recorded as variables in our dataset.  In probability, a __random variable__ is used to represent a measurement that results from a random process.  

:::{#def-random-variable}
## Random Variable
Let $\mathcal{S}$ be the sample space corresponding to a random process; a random variable $X$ is a function mapping elements of the sample space to the real line.

Random variables represent a measurement that will be collected during the course of a study.  Random variables are typically represented by a capital letter.
:::

While for our purposes, it suffices to think of a random variable as a measurement, mathematically, it is a _function_.  The image (or range) of this function is used to broadly classify random variables as __continuous__ or __discrete__; we refer to this image as the __support__ of the random variable.

:::{#def-support}
## Support
The support of a random variable is the set of all possible values the random variable can take.
:::

:::{#def-rvtypes}
## Continuous and Discrete Random Variable
The random variable $X$ is said to be a discrete random variable if its corresponding support is countable.  The random variable $X$ is said to be a continuous random variable if the corresponding support is uncountable (such as an interval or a union of intervals on the real line).
:::

Discrete random variables are analogous to categorical (or qualitative) variables in data analysis; that is, discrete random variables are used to model the result of a random process which categorizes each unit of observation into a group.  Continuous random variables are analogous to numeric (or quantitative) variables in data analysis; continuous random variables are used to model the result of a random process which produces a number for which arithmetic makes sense.

:::{.callout-warning}
Whether we use a continuous or discrete random variable to represent a measurement is not always obvious.  Suppose we consider recording the age of a student selected from a class at a university that typically enrolls "traditional" students (those coming directly from high school).  Let the random variable $X$ denote the age of the student.

If we record the student's age in years since birth, $X$ can take on only a finite number of values (most likely $\{18, 19, 20, 21, 22, 23\}$), making it a discrete random variable.  However, if we record the student's age as the number of seconds since birth, we might well consider the support of $X$ to be a rather large interval, leading to a continuous random variable.  
:::

The goal of statistics is to use a sample to say something about the underlying population.  Consider taking a sample of size $n$ and measuring a single variable on each unit of observation.  Then, we might represent the measurements we will obtain (note the use of the future tense) as $X_1, X_2, \dots, X_n$.  While the majority of probability courses focus on a single, or maybe two, random variables, note that collecting data on a sample requires that we deal with at least $n$ random variables (one measurement for each of the observations in our sample).


## Characterizing a Distribution
Again, the goal of statistics is to use a sample to say something about the underlying population.  Consider the following research objective:

  > Estimate the cost (in US dollars) of a diamond for sale in the United States.
  
For this research objective, our population of interest is all diamonds for sale in the United States.  We would not expect every diamond for sale to have the same price; variability is inherent in any process.  As a result, the sale price of diamonds has a distribution across this population.  This is our primary use of probability theory in a statistical analysis --- to model distributions.  

Consider taking a sample of size 1 from the population; let $Y$ represent the cost of the diamond that is selected.  Since we have not yet observed the cost of this diamond, $Y$ is a random variable.  And, since this diamond is sampled from the population of interest, the support of $Y$ is determined by the cost of diamonds in the United States.  Further, the likelihood that $Y$ falls within any interval is determined by the distribution of the cost across the population.  That is, the distribution of $Y$ is the distribution of the population.

:::{.callout-tip}
## Big Idea
If a random variable $X$ represents a measurement for a single observation from a population, the distribution of the random variable corresponds to the distribution of the variable across the population.
:::

A key realization in statistical analysis is that we will never fully observe the distribution of the population; however, we can posit a model for this distribution.  In probability, the most common way to characterize the distribution of a random variable is through its density function.

:::{#def-density-function}
## Density Function
A density function $f$ relates the values in the support of a random variable with the probability of observing those values.  

Let $X$ be a continuous random variable, then its density function $f$ is the function such that

$$Pr(a \leq X \leq b) = \int_a^b f(x) dx$$

for any real numbers $a$ and $b$ in the support.

Let $X$ be a discrete random variable, then its density function $f$ is the function such that

$$Pr(X = u) = f(u)$$

for any real number $u$ in the support.
:::

:::{.callout-note}
## Properties of a Density Function
Let $X$ be a random variable with density function $f$ defined over support $\mathcal{S}$.  Then, 

  1. $f(x) \geq 0$ for all $x \in \mathcal{S}$.  That is, the density is non-negative for all values in the support.
  2. If $X$ is a continuous random variable, then $\int_{\mathcal{S}} f(x) dx = 1$; similarly, if $X$ is a discrete random variable, then $\sum_{\mathcal{S}} f(x) = 1$.  That is, $X$ must take a value in its support; so, $Pr(X \in \mathcal{S}) = 1$, similar to the second Axiom of Probability (@def-axioms).
  3. $f(x) = 0$ for all values of $x \notin \mathcal{S}$.  The density takes the value of 0 for all values outside the support.
:::

:::{.callout-note}
In a probability course, there is often a distinction made between probability "density" functions (used for continuous random variables) and probability "mass" functions (used for discrete random variables).  We do not make this distinction and instead rely on the context to determine whether we are dealing with a continuous or discrete random variable.  Throughout, we will note when the operations differ between these two types of variables.  Measure theory provides a unifying framework to these issues.
:::

When working with a continuous random variable, the density function is a smooth function over some region, and the actual value of the function is not interpretable; instead, we get at a probability by considering the area under the curve.  Again, drawing connections to data analysis, we can think of a density function as a mathematical formula representing a smooth histogram.  The area under the curve for any region gives the proportion of the population which has a value in that region.  That is, we get the probability that a random variable will be in an interval by integrating the density function over that interval.  

Figure @fig-randomvariables-density illustrates this idea; we have data from a sample of diamonds from the population of interest.  The sample is summarized with a histogram; we have overlayed a posited density (with the corresponding mathematical function that describes this density) for the population.  The sample (summarized with the histogram) is approximating the population (modeled using the density function).

```{r}
#| label: fig-randomvariables-density
#| fig-cap: Illustration of a density function representing the posited distribution of the population alongside a histogram summarizing the cost of diamonds using a sample of 53940 diamonds.
#| fig-alt: Histogram with a density function overlayed.

diamonds |>
  drop_na(price) |>
  ggplot(mapping = aes(x = price)) +
  geom_histogram(aes(y = after_stat(density)), 
                 binwidth = 500,
                 fill = 'grey75', 
                 color = 'black') +
  stat_function(fun = dgamma, color = "blue", linewidth = 1.1,
                args = list(shape = 1, scale = 4000)) +
  annotate("label", x = 7500, y = 2e-04, 
           label = 'f(x) == frac(1, 4000) * e^(-frac(x, 4000))',
           color = 'blue',
           parse = TRUE) +
  labs(y = NULL,
       x = 'Cost of a Diamond (US Dollars)') +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank())
```

You may recognize the particular form of the density function in @fig-randomvariables-density.  The general form is

$$f(x) = \frac{1}{\sigma} e^{-x / \sigma} \qquad \text{for } x > 0$$

where $\sigma$ is the _scale_ parameter that defines the distribution (set at 4000 in @fig-randomvariables-density).  This is known as the Exponential distribution with scale parameter $\sigma$.  This illuminates another connection between probability and statistics.

Note that our research objective describe above is an ill-posed question as stated.  The answer is "it depends" since each individual diamond in the population has a different value.  Well-posed questions in statistics are centered on an appropriately chosen __parameter__ (@def-parameter).

In probability, the parameters are values that are tuned or set within a problem; we then work forward to compute the probability of an event of interest.  In practice, however, when we posit a functional form for a density function to describe the distribution of the population, the parameters are unknown.  We plan to use the data to estimate or characterize the parameter; but, the parameter itself will remain unknown.  In both cases, however, the parameter is a _fixed quantity_, even if we are ignorant of that value.

:::{.callout-tip}
## Big Idea
When a probability model is specified for a population, it is generally specified up to some unknown parameter(s).  Making inference on the unknown parameter(s) therefore characterizes the entire distribution.
:::


### Common Parameters
Most scientific questions are focused on the location or spread of a distribution.  For example, we are interested in estimating the average cost of a diamond sold in the United States.  Introductory statistics introduces summaries of location and spread within the sample (e.g., sample mean for location and sample variance for spread).  Analogous summaries exist for density functions.  As stated above, parameters are unknown constants that govern the form of the density function.  Because they govern the form of the density, the parameters are also related to those summarizing the location or spread of the distribution.

:::{#def-mean}
## Expected Value (Mean)
Let $X$ be a random variable with density function $f$ defined over the support $\mathcal{S}$.  The expected value of a random variable, also called the mean and denoted $E(X)$, is given by

$$E(X) = \int_{\mathcal{S}} x f(x) dx$$

for continuous random variables and 

$$E(X) = \sum_{\mathcal{S}} x f(x)$$

for discrete random variables.
:::

Notice the similarity between the form of the sample mean and the population mean.  A sample mean takes the sum of each value in the sample, weighting each value by $1/n$ (where $n$ is the sample size).  Without information about the underlying population, the sample must treat each value observed as equally likely; values become more likely if they appear multiple times.  In the population, however, when the form of $f$ is known, the density provides information about the likelihood of each value giving us a better weight than $1/n$.  That is, the population mean is a sum of the values in the support, weighting each value by the corresponding value of the density function.

:::{#def-variance}
## Variance
Let $X$ be a random variable with density function $f$ defined over the support $\mathcal{S}$.  The variance of a random variable, denoted $Var(X)$, is given by

$$Var(X) = E\left[X - E(X)\right]^2 = E\left(X^2\right) - E^2(X).$$

If we let $\mu = E(X)$, then this is equivalent to

$$\int_{\mathcal{S}} (x - \mu)^2 f(x) dx$$

for continuous random variables and 

$$\sum_{\mathcal{S}} (x - \mu)^2 f(x)$$

for discrete random variables.
:::

:::{.callout-warning}
Pay careful attention to the notation.  $E^2(X)$ represents the square of the expected value; that is, 

$$E^2(X) = \left[E(X)\right]^2.$$

However, $E(X)^2$ represents the expected value of the square of $X$; that is,

$$E(X)^2 = E\left(X^2\right).$$
:::

The variance provides a measure of spread; in particular, it is capturing distance from the mean.  Notice that the form of the variance involves taking the expectation of a squared term; in general, we will need to consider expectations of functions.

:::{#def-expectation}
## Expectation of a Function
Let $X$ be a random variable with density function $f$ over the support $\mathcal{S}$, and let $g$ be a real-valued function.  Then, 

$$E\left[g(X)\right] = \int_{\mathcal{S}} g(x) f(x) dx$$

for continuous random variables and

$$E\left[g(X)\right] = \sum_{\mathcal{S}} g(x) f(x)$$

for discrete random variables.
:::

:::{.callout-note}
@def-expectation is sometimes referred to as the "Law of the Unconscious Statistician" in probability texts.  We find the name somewhat insulting, as it suggests statisticians do not appreciate the mathematical underpinnings of their field.  In reality, the expected value of a function is such a common operation in statistical theory that statisticians often present @def-expectation as the definition of an expectation (as we have done) instead of deriving it as a result of @def-mean after applying a variable transformation (see @exm-transformations below).  It is possible this is where the slight in the naming convention originated.
:::

A result of @def-expectation is the following, very useful theorem, which states that expectations are linear operators.

:::{#thm-expectation}
## Expectation of a Linear Combination
Let $X$ be a random variable, and let $a_1, a_2, \dotsc, a_m$ be real-valued constants and $g_1, g_2, \dotsc, g_m$ be real-valued functions; then,

$$E\left[\sum_{i=1}^{m} a_i g_i(X)\right] = \sum_{i=1}^{m} a_i E\left[g_i(X)\right].$$
:::

The mean and variance play an important role in characterizing a distribution, especially within statistical theory (as we will see in future chapters).  However, there is another set of parameters which are important.

:::{#def-population-percentile}
## Percentile for a Random Variable
Let $X$ be a random variable with density function $f$.  The $100k$ percentile is the value $q$ such that

$$Pr(X \leq q) = k.$$
:::

:::{#exm-parameters}
## Parameters of Exponential Distribution
Let $X$ be an Exponential distribution with scale parameter $\sigma$; that is, the density function $f$ is given by

$$f(x) = \frac{1}{\sigma} e^{-x/\sigma} \qquad x > 0$$

where $\sigma > 0.$  Compute the mean, variance, and median of this distribution, as a function of the unknown scale parameter.
:::

The solution to this problem is particularly important as it illustrates a very useful technique when working with known distributions in statistical theory.

:::{.solution}
We note that the function

$$g(y) = \frac{1}{\beta^{\alpha} \Gamma(\alpha)} y^{\alpha - 1} e^{-y/\beta}$$

is a valid density function over the positive real line provided that $\alpha,\beta > 0$; in particular, this is known as a Gamma distribution.  Since $g$ is a valid density function, then we know that 

$$\int_{0}^{\infty} g(y) dy = 1$$

for all values of $\alpha,\beta > 0$. 

Now, let $X$ be an Exponential random random variable with scale parameter $\sigma$.  Then, the expected value of $X$ is given by

$$
\begin{aligned}
  E(X)
    &= \int_{0}^{\infty} x \frac{1}{\sigma} e^{-x/\sigma} dx \\
    &= \int_{0}^{\infty} \frac{1}{\sigma} x^{2-1} e^{-x/\sigma} dx
\end{aligned}
$$

where we have simply rewritten the exponent in the second line.  Notice that expression within the integral shares a striking similarity to the form of the density function of a Gamma distribution; however, they are not exactly the same.  To coerce the expression into that of the Gamma density function, we "do nothing" --- multiplying and dividing the expression by the quantity $\sigma\Gamma(2)$.  This gives

$$
\begin{aligned}
  E(X)
    &= \int_{0}^{\infty} x \frac{1}{\sigma} e^{-x/\sigma} dx \\
    &= \int_{0}^{\infty} \frac{1}{\sigma} x^{2-1} e^{-x/\sigma} dx \\
    &= \int_{0}^{\infty} \sigma \Gamma(2) \frac{1}{\sigma^2 \Gamma(2)} x^{2-1} e^{-x/\sigma} dx \\
    &= \sigma \Gamma(2) \int_{0}^{\infty} \frac{1}{\sigma^2 \Gamma(2)} x^{2-1} e^{-x/\sigma} dx \\
    &= \sigma \Gamma(2) \\
    &= \sigma.
\end{aligned}
$$

In line 3, we have multiplied and divided by $\sigma \Gamma(2)$, which does not change the problem.  In line 4, we have pulled out the terms $\sigma \Gamma(2)$ since it is a constant with respect to the integral; what is left inside the integral is the form of the density function for a Gamma distribution where $\alpha = 2$ and $\beta = \sigma$.  In line 5, we make use of the fact that the integral of any density function over the entire support for which it is defined must be 1.  Finally, in line 6, we recognize that $\Gamma(k) = (k-1)!$ if $k$ is a natural number.  

Applying the same process, we also have that

$$
\begin{aligned}
  E\left(X^2\right)
    &= \int_{0}^{\infty} x^2 \frac{1}{\sigma} e^{-x/\sigma} dx \\
    &= \sigma^2 \Gamma(3)\int_{0}^{\infty} \frac{1}{\sigma^3 \Gamma(3)} x^{3-1} e^{-x/\sigma} dx \\
    &= 2\sigma^2.
\end{aligned}
$$

Therefore, 

$$Var(X) = E\left(X^2\right) - E^2(X) = 2\sigma^2 - \sigma^2 = \sigma^2.$$

Finally, the median is the value $q$ such that $Pr(X \leq q) = 0.5$; but, we recognize that

$$
\begin{aligned}
  Pr(X \leq q) 
    &= \int_{0}^{q} \frac{1}{\sigma} e^{-x/\sigma} dx \\
    &= \left. -e^{-x/\sigma} \right|_{0}^{q} \\
    &= -e^{-q/\sigma} + 1.
\end{aligned}
$$

Setting this expression equal to 0.5 and solving for $q$ yields $q = -\sigma \log(0.5)$, where $\log(\cdot)$ represents the _natural_ logarithm.
:::

:::{.callout-tip}
## Big Idea
Suppose the density $f$ is a function of the parameters $\boldsymbol{\theta}$; then, the mean, variance, and median (as well as any other parameters of interest in a research objective) will be functions of $\boldsymbol{\theta}$.
:::

@exm-parameters highlighted a useful technique for simplifying integrals in statistical applications, which makes use of the "do nothing" strategy discussed in the previous chapter.  


### Kernels
One of the characteristics common to any density function that we noted above was that if we sum the density across the entire support, we get a value of 1.  That is, if $X$ is a discrete random variable, then

$$\sum_{x \in \mathcal{S}_X} f(x) = 1,$$

and if $X$ is a continuous random variable, then

$$\int_{\mathcal{S}_X} f(x) dx = 1.$$

Any density function can be written as

$$f(x) = a k(x)$$

where $a > 0$ is a constant and $k(x)$ is a function of $x$.  Specifically, if $X$ is a continuous random variable, then 

$$a = \frac{1}{\int k(x) dx}$$

since $\int f(x) dx = 1$.  We call $k(x)$ the __kernel__ of the distribution.  Kernels are helpful for quickly identifying distributions[^tables].

:::{#def-kernel}
## Kernel of a Distribution
Let $k(x)$ be a non-negative function of $x$ over some region $\mathcal{S}_X$.  Then, a valid density function $f$ over the support $\mathcal{S}_X$ can be constructed by taking

$$f(x) = a k(x)$$

where $a > 0$ is a suitably chosen scaling constant to ensure the density integrates (or sums) to 1 over the support.  The function $k$ is known as the kernel of the distribution, and it can be used to identify the distributional family for a random variable.
:::

:::{#exm-kernel}
## Kernel of an Exponential Random Variable
In @exm-parameters, we let $X$ be an Exponential random variable with scale parameter $\sigma$.  Identify the kernel of this distribution.
:::

:::{.solution}
Let $a = \sigma^{-1}$ and 

$$k(x) = e^{-x/\sigma};$$

then, we have that $f(x) = a k(x)$. We note that $k(x)$ has no leading constants; therefore, the kernel for an Exponential distribution is

$$e^{-x/\sigma}.$$
:::

As @exm-parameters illustrated, being able to identify a kernel can help us quickly evaluate an integral.  In particular, notice that we immediately have that

$$\int e^{-x/\sigma} dx = \sigma$$

for any value of $\sigma > 0$ because we know that

$$
\begin{aligned}
  \int e^{-x / \sigma} dx 
    &= \sigma \int \frac{1}{\sigma} e^{-x / \sigma} dx \\
    &= \sigma.
\end{aligned}
$$

The first line multiplies and divides by the appropriate scaling term so that the kernel becomes a valid density function.  Once we have a valid density, we know it integrates to 1, simplifying the expression.

In addition to motivating the use of kernels in integration applications, the solution to @exm-parameters also shows that there is more than one way to characterize a distribution.


### Distribution Function
Especially for visualization, the density function is the most common way of characterizing a probability model.  However, computing the probability using the density is problematic due to the integration required.  Many software address this by working with the cumulative distribution function (CDF).

:::{#def-cdf}
## Cumulative Distribution Function (CDF)
Let $X$ be a random variable; the cumulative distribution function (CDF) is defined as

$$F(u) = Pr(X \leq u).$$

For a continuous random variable, we have that

$$F(u) = \int_{-\infty}^{u} f(x) dx$$

implying that the density function is the derivative of the CDF.  For a discrete random variable

$$F(u) = \sum_{x \leq u} f(x).$$
:::

Working with the CDF improves computation because it avoids the need to integrate each time; instead, the integral is computed once (and stored internally in the computer) and we use the result to compute probabilities directly.

:::{.callout-tip}
## Big Idea
Density functions are the mathematical models for distributions; they link values of the variable with the likelihood of occurrence.  However, for computational reasons, we often work with the cumulative distribution function which provides the probability of being less than or equal to a value.
:::



## Transformations of a Random Variable
Occasionally, we are interested in a transformation of a particular characteristic.  That is, we have a model for the distribution of $X$, but we are interested in $Y = g(X)$.  In this section, we examine one method for determining the density of $Y$ from the density of $X$.   While relationships between many common distributions have been well studied[^relationships], it is useful to know the process for addressing transformations.

While there various approaches to this problem, we find this method the most reliable.  Further, it does not require the memorization of a formula, but instead builds on fundamental ideals.  This is known as the __Method of Distribution Functions__.

:::{#def-method-of-distribution-functions}
## Method of Distribution Functions
Let $X$ be a continuous random variable with density $f$ and cumulative distribution function $F$.  Consider $Y = h(X)$.  The following process provides the density function $g$ of $Y$ by first finding its cumulative distribution function $G$.

  1. Find the set $A$ for which $h(X) \leq t$ if and only if $X \in A$.
  2. Recognize that $G(y) = Pr(Y \leq y) = Pr\left(h(X) \leq y\right) = Pr(X \in A)$.
  3. If interested in $g(y)$, note that $g(y) = \frac{\partial}{\partial y} G(y)$.
:::

When $h$ is a strictly monotone function (unique inverse exists), then step 1-2 is much easier because we can apply $h^{-1}$.  In step 2 of the above process, the final expression is often left in terms of $F$, the CDF of $X$; then, when we find the density in step 3, we can apply the chain rule (avoiding the need to actually have an expression for $F$).

:::{#exm-transformations}
## Transformation of a Random Variable
Previously, we posited the following model for the distribution of the cost of a diamond sold in the US:

$$f(x) = \frac{1}{\sigma} e^{-x/\sigma} \qquad x > 0$$

for some $\sigma > 0$.  As cost is generally a heavily skewed variable, we may be interested in taking the (natural) logarithm before proceeding with an analysis.  Find the density of $Y = \log(X)$; then, write an expression for $E(Y)$.
:::

:::{.solution}
We note that $\log(x)$ is a strictly monotone function.  Therefore, we have that

$$
\begin{aligned}
  G(y) &= Pr(Y \leq y) \\
    &= Pr(\log(X) \leq y) \\
    &= Pr\left(X \leq e^y\right).
\end{aligned}
$$

Just to place this within the method described above, since $\log(x) \leq y$ if and only if $x \leq e^y$, then $A = \{t: x \leq e^t\}$.  Of course, we didn't really need to identify this because we were able to apply the inverse of $\log(x)$ directly within the probability expression.  We now recognize that we have a probability of the form "$X$ less than or equal to something."  And, this matches the form of the CDF of $X$.  That is, we have that

$$G(y) = F\left(e^y\right).$$

This completes step 2 of the procedure; we have expressed the CDF of $Y$ as a function of the CDF of $X$.  Now, to find the density, we apply the chain rule.

$$
\begin{aligned}
  g(y) 
    &= \frac{\partial}{\partial y} G(y) \\
    &= \left[\left.\frac{\partial}{\partial x} F(x)\right|_{x = e^y}\right] \frac{\partial}{\partial y} e^y \\
    &= \left[\left. f(x) \right|_{x = e^y}\right] e^y \\
    &= f\left(e^y\right) e^y \\
    &= \frac{1}{\sigma} e^{-e^y/\sigma} e^y
\end{aligned}
$$

which will be valid for all real values of $y$; that is, the support of $Y$ is all real numbers.  In line 2 above, we applied the chain rule to compute the derivative, avoiding the need to explicitly state the CDF of $X$.  

Given the density of $Y$, we know (by @def-mean) that

$$E(Y) = \int y g(y) dy = \int y \frac{1}{\sigma} e^{-e^{y}/\sigma} e^{y} dy.$$

Letting $u = e^y$ (and therefore $du = e^y dy$) and performing a u-substitution, we have that

$$
\begin{aligned}
  E(Y)
    &= \int y\frac{1}{\sigma} e^{-e^{y}/\sigma} e^{y} dy \\
    &= \int \log(u) \frac{1}{\sigma} e^{-u / \sigma} du.
\end{aligned}
$$

Since the variable of integration is arbitrary, we recognize that this integral as what we defined (in @def-expectation) as $E\left[\log(X)\right]$ where $X$ has density $f(x)$ defined in @exm-transformations.
:::

:::{.callout-warning}
While mathematicians distinguish between a derivative $\frac{d}{dx}$ and a partial derivative $\frac{\partial}{\partial x}$, we do not make that distinction.
:::

[^tables]: A good [table of common distributions](https://qiangbo-workspace.oss-cn-shanghai.aliyuncs.com/2018-11-11-common-probability-distributions/distab.pdf) is given in Casella and Berger, a popular text for statistical theory at the graduate level.

[^relationships]: An excellent [summary of the relationships between Distributions](http://www.math.wm.edu/~leemis/chart/UDR/UDR.html) was developed by faculty at the College of William and Mary.
