{
  "hash": "2568cc05bdad7a415ba26a7224afe93c",
  "result": {
    "markdown": "# QR Factorization {#sec-qr-factorization}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\providecommand{\\norm}[1]{\\lVert#1\\rVert}\n\\providecommand{\\abs}[1]{\\lvert#1\\rvert}\n\\providecommand{\\iid}{\\stackrel{\\text{IID}}{\\sim}}\n\\providecommand{\\ind}{\\stackrel{\\text{Ind}}{\\sim}}\n  \n\\providecommand{\\bm}[1]{\\mathbf{#1}}\n\\providecommand{\\bs}[1]{\\boldsymbol{#1}}\n\\providecommand{\\bbeta}{\\bs{\\beta}}\n  \n\\providecommand{\\Ell}{\\mathcal{L}}\n\\providecommand{\\indep}{\\perp\\negthickspace\\negmedspace\\perp}\n\n\n\nAs a regression model grows in complexity, we need to consider the computational efficiency of the algorithms used to fit the model.  QR factorization is a well-known computational step that can increase the efficiency of an MCMC algorithm in a regression model.\n\nConsider a regression model in which the parameter $\\theta$ is allowed to vary according to a linear function of the predictors:\n\n$$\\theta_i = \\sum_{j=1}^{p} \\beta_j (\\text{Predictor } j)_i.$$\n\nWe have eliminated the \"intercept\" term $\\beta_0$, but this is done without loss of generality as we could view \"Predictor 1\" as the value 1 for all subjects, resulting in $\\beta_1$ acting as the intercept.  We can represent this model in matrix notation as\n\n$$\\bs{\\theta} = \\bm{X} \\bs{\\beta},$$\n\nwhere $\\bs{\\beta} = \\left(\\beta_1, \\dotsc, \\beta_p\\right)^\\top$ is the parameter vector; and, the $j$-th column of $\\bm{X}$ is the vector of length $n$ containing the values of the $j$-th predictor for the $n$ subjects.  That is,\n\n$$\\bm{X}_{i, j} = (\\text{Predictor } j)_i \\qquad j=1,2,\\dots,p.$$\n\nThe matrix $\\bm{X}$ is known as the design matrix.\n\nAs long as the number of observations $n$ exceeds the number of predictors $p$ in the model, we can decompose the design matrix $\\bm{X}$ as\n\n$$\\bm{X} = \\bm{Q}\\bm{R}$$\n\nwhere $\\bm{Q}$ is an orthogonal $n \\times p$ matrix and $\\bm{R}$ is an upper-triangular $p \\times p$ matrix.  Then, we can write the linear predictor for the mean response as\n\n$$\\bm{X}\\bs{\\beta} = \\bm{Q}\\bm{R}\\bs{\\beta}.$$\n\nTherefore, the regression is conducted with the \"new\" design matrix $\\bm{Q}$ with parameters $\\bs{\\eta} = \\bm{R}\\bs{\\beta}$.  These parameters are then transformed back into the parameters of interest $\\bs{\\beta}$ by acknowledging that\n\n$$\\bs{\\beta} = \\bm{R}^{-1} \\bs{\\eta}.$$\n\nAdmittedly, this feels like only algebraic manipulation.  The reason this works is that the columns of the \"new\" design matrix $\\bm{Q}$ are orthogonal.  This allows the MCMC algorithm to move more easily through the parameter space because changing one column has no effect on the other columns in the optimization routine.  The columns of this new design matrix are also on the same scale; that is, the impact of one variable (like yearly income) taking on extreme values while another (like an indicator variable) taking on smaller units is reduced.  Having variables on the same scale allows the MCMC algorithm to move around the parameter space with a small number of large steps.  As a result, the numerical accuracy of the algorithm is improved.  \n\n:::{.callout-tip}\n## Big Idea\nQR factorization improves the computational efficiency of the regression and MCMC algorithms.  There is no change to the actual distribution of the parameters.\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}