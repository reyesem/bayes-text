{
  "hash": "89a0c21a4708a7b0a6997d9045e02ca7",
  "result": {
    "markdown": "# Hypothesis Testing {#sec-hypothesis-testing}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\providecommand{\\norm}[1]{\\lVert#1\\rVert}\n\\providecommand{\\abs}[1]{\\lvert#1\\rvert}\n\\providecommand{\\iid}{\\stackrel{\\text{IID}}{\\sim}}\n\\providecommand{\\ind}{\\stackrel{\\text{Ind}}{\\sim}}\n  \n\\providecommand{\\bm}[1]{\\mathbf{#1}}\n\\providecommand{\\bs}[1]{\\boldsymbol{#1}}\n\\providecommand{\\bbeta}{\\bs{\\beta}}\n  \n\\providecommand{\\Ell}{\\mathcal{L}}\n\\providecommand{\\indep}{\\perp\\negthickspace\\negmedspace\\perp}\n\n\n\nWe have considered both estimation and prediction at this point. The third type of question often asked by researchers is which model (out of some pre-defined set) is most supported by the data.  As with previous estimation and prediction, the Bayesian framework seeks to characterize the evidence for each model, given the data.\n\n:::{.callout-warning}\nFor those who are familiar with a classical Frequentist approach to hypothesis testing, you will note that the above language is fundamentally different than the Frequentist perspective.  The Frequentist perspective does not allow us to characterize the \"evidence\" for the null hypothesis, and it would certainly never allow us to quantify the probability of a hypothesis being true.\n:::\n\nYou may recall that for the Naive Classification of College Students example (@exm-naive), we derived the following posterior distribution (see @eq-naive-posterior):\n\n$$\\pi(\\theta \\mid y) = (0.7169)\\delta(\\theta - 0.246) + (0.2831)\\delta(\\theta - 0.563).$$\n\nThis posterior distribution followed from learning 3 of the 10 students identify as female combined with the prior belief that there was a 60% chance the students were from ISU.\n\nThis example is nice for illustrating hypothesis testing because baked into the problem were essentially two hypotheses:\n\n$$H_0: \\theta = 0.246 \\qquad \\text{vs.} \\qquad H_1: \\theta = 0.563$$\n\nwhere the null hypothesis represents the belief that the students are from Rose-Hulman (where 24.6% of students identify as female) and the alterntative hypothesis represents the belief that the students are from ISU (where 56.3% of students identify as female).  In this example, each hypothesis specifies a single value for the parameter; in general, each hypothesis could specify a region for the parameter.  That is, we are generally interested in testing\n\n$$H_0: \\theta \\in \\Theta_0 \\qquad \\text{vs.} \\qquad H_1: \\theta \\in \\Theta_1.$$\n\nWhile not a requirement, in practice $\\Theta_0$ and $\\Theta_1$ are typically mutually exclusive sets which form a partition of the parameter space.  \n\n:::{.callout-note}\nNothing prohibits us from having more than two hypotheses in this framework, and nothing requires the hypotheses form a partition of the parameter space.\n:::\n\nUnder the Bayesian framework, it is straightforward to compute probabilities of the form \"the probability $H_j$ is true given the data observed.\"  \n\n:::{.callout-note}\nUnder the classical Frequentist perspective, the probability a hypothesis is true is nonsensical; however, under the Bayesian approach, we are using probability simply to characterize our uncertainty about the statement.\n:::\n\nFor the Naive Classification of College Students, the posterior naturally allows us to address the hypothesis.  Notice that\n\n$$\n\\begin{aligned}\n  Pr\\left(H_0 \\mid y\\right) &= Pr(\\theta = 0.246 \\mid y) = 0.7169 \\\\\n  Pr\\left(H_1 \\mid y\\right) &= Pr(\\theta = 0.563 \\mid y) = 0.2831.\n\\end{aligned}\n$$\n\nThat is, after observing the data, we believe it is much more likely the the students are from Rose-Hulman than from ISU.  \n\n:::{.callout-note}\nAgain, contrasting the conclusion in the Bayesian framework with a classical Frequentist framework, we are saying that the data is convincing us to believe the null hypothesis over the alternative.  The idea of \"accepting\" the null hypothesis in the classical Frequentist framework is heretical.\n:::\n\nIn addition to computing the probability of each hypothesis, we can compute how much more likely we are to believe the students are from Rose-Hulman compared to believing they are from ISU:\n\n$$\\frac{Pr\\left(H_0 \\mid y\\right)}{Pr\\left(H_1 \\mid y\\right)} = 2.53;$$\n\nthat is, given the data, we believe it is 2.53 times more likely that the students are from Rose-Hulman than from ISU.  This is known as the posterior odds.\n\n:::{#def-posterior-odds}\n## Posterior Odds\nLet $H_j$ denote the hypothesis that $\\theta \\in \\Theta_j$ for some region $\\Theta_j$.  Then, the posterior odds _in favor of_ $H_j$ is given by\n\n$$\\frac{Pr\\left(\\theta \\in \\Theta_j \\mid \\bm{y}\\right)}{Pr\\left(\\theta \\notin \\Theta_j \\mid \\bm{y}\\right)}.$$\n:::\n\n:::{.callout-note}\nWhen there are only two hypotheses, and they partition the parameter space, then the posterior odds captures how strongly we favor one hypothesis over the other given the data.  \n:::\n\nThe posterior odds is useful, but it only suggests how we feel after we have observed the data.  We may be more interested in how much the data _impacted_ our prior beliefs.  For example, if the data simply confirmed our beliefs, that is not nearly as extraordinary as the data completely reversing our beliefs.  The Bayes Factor captures this impact.\n\n:::{#def-bayes-factor}\n## Bayes Factor\nA measure of how the observed data _alters_ your prior beliefs about a hypothesis.  Let $H_j$ denote the hypothesis that $\\theta \\in \\Theta_j$ for some region $\\Theta_j$.  The Bayes Factor _in favor of_ $H_j$ is the ratio of the posterior odds in favor of $H_j$ to the prior odds in favor of $H_j$:\n\n$$BF_j = \\left(\\frac{Pr\\left(\\theta \\in \\Theta_j \\mid \\bm{y}\\right)}{Pr\\left(\\theta \\notin \\Theta_j \\mid \\bm{y}\\right)}\\right)\\left(\\frac{Pr\\left(\\theta \\notin \\Theta_j\\right)}{Pr\\left(\\theta \\in \\Theta_j\\right)}\\right).$$\n:::\n\n:::{.callout-note}\nSome prefer to report the logarithm of the Bayes Factor, as it is a quantity that is easier to work with in theoretical derivations.\n:::\n\nKeep in mind, the Bayesian framework is about quantifying our uncertainty about the unknown parameters.  The Bayes Factor measures how that uncertainty has been impacted by the observed data.  Two independent papers made recommendations for how to interpret a Bayes Factor; however, we should remember that these are simply \"rules of thumb.\"\n\n| Strength of Evidence | Jeffreys' Scale | Kass and Raftery Scale |\n|:---------------------|:---------------:|:----------------------:|\n| Weak | $0 \\leq \\log_{10}(BF) < 0.5$ | $0 \\leq \\log(BF) < 1$ |\n| Substantial | $0.5 \\leq \\log_{10}(BF) < 1$ | $1 \\leq \\log(BF) < 3$ |\n| Strong | $1 \\leq \\log_{10}(BF) < 2$ | $3 \\leq \\log(BF) < 5$ |\n| Decisive | $2 \\leq \\log_{10}(BF)$ | $5 \\leq \\log(BF)$ |\n\n: Rules of thumb for interpreting the Bayes Factor as suggested by Jeffreys and Kass and Raftery. {#tbl-bayes-factor .striped .responsive}\n\n\n:::{.callout-warning}\nTake caution when interpreting a Bayes Factor; it quantifies the degree to which the data altered your prior beliefs.  It is possible to have a really small Bayes Factor in favor of a hypothesis and yet simultaneously believe overwhelmingly in that hypothesis given the data; the small Bayes Factor simply implies that you believed in that hypothesis before collecting the data as well.  Conversely, it is possible to have a really large Bayes Factor in favor of a hypothesis and yet simultaneously not distinguish between that hypothesis and another given the data; the large Bayes Factor simply implies that your beliefs about that hypothesis have dramatically changed.\n:::\n\nFor the Classification of College Students example (@exm-naive), our Bayes Factor, in favor of students being from Rose-Hulman, is given by\n\n$$BF_{0} = \\left(\\frac{0.7169}{0.2831}\\right)\\left(\\frac{0.6}{0.4}\\right) = 3.80.$$\n\nThe log-Bayes Factor is then 1.33, which falls under the \"substantial evidence\" category in the Kass and Raftery scale.  This Bayes Factor is capturing the idea that we are still somewhat divided on the issue, but we have switched from favoring that the students are from ISU to favoring that the students are from Rose-Hulman.  The data led to a shift in our beliefs.\n\n\n## Point-Null Hypotheses\nTechnically speaking, hypothesis testing in the Bayesian framework requires little; the posterior distribution allows us to quantify our uncertainty about parameters (or corresponding hypotheses) given the data.  However, there is a common scenario which has a potential pitfall worth discussion.  Consider testing the following hypotheses:\n\n$$H_0: \\theta = \\theta_0 \\qquad \\text{vs.} \\qquad H_1: \\theta \\neq \\theta_0$$\n\nwhere here $\\Theta_0 = \\{\\theta_0\\}$ is a singleton set.  This is known as a \"point-null hypothesis,\" and it poses a problem for many prior distributions.  To illustrate, consider computing the prior odds in favor of $H_1$:\n\n$$\\frac{Pr\\left(\\theta \\neq \\theta_0\\right)}{Pr\\left(\\theta = \\theta_0\\right)} = \\frac{\\int_{\\theta \\neq \\theta_0}^{} \\pi(\\theta) d\\theta}{\\int_{\\theta = \\theta_0}^{} \\pi(\\theta) d\\theta}.$$ {#eq-hyp-post-odds}\n\nFor any continuous prior distribution, the numerator in @eq-hyp-post-odds will be 1 and the denominator will be 0!  For any continuous distribution, the probability the random variable takes a specific value is 0; therefore, a continuous prior distribution is incompatible with a point-null hypothesis.  The continuous prior distribution communicates you are infinitely more likely to believe the parameter takes any value other than $\\theta_0$.  There is a misalignment of beliefs; if you are truly interested in testing a point-null hypothesis, you must believe the null hypothesis has some probability of describing reality.  Therefore, the prior distribution must incorporate the belief that $$Pr\\left(H_0\\right) > 0$$.\n\n:::{.callout-tip}\n## Big Idea\nWe can only test a hypothesis if, a priori, we believe there is some chance the hypothesis is true.  If we go into a study believing something is impossible, no amount of data will convince us otherwise.\n:::\n\nOne way of incorporating our prior beliefs about a point-null hypothesis is specifying a mixture distribution.  \n\n:::{#def-mixture-distribution}\n## Mixture Distribution\nLet $X$ be a random variable and $f(x)$ and $g(x)$ be valid density functions defined on a common support.  Then, \n\n$$h(x) = wf(x) + (1 - w) g(x),$$\n\nwhere $0 < w < 1$, is known as a mixture distribution.\n:::\n\nAppropriately applied, a mixture distribution allows us to place mass on the value of $\\theta_0$ in a point-null hypothesis and spread out the remaining probability along the support.\n\n:::{.callout-tip}\n## Mixture Prior for Point-Null Hypotheses\nLet $\\theta$ be a parameter which has support $\\Theta$, and consider testing the hypotheses\n\n$$H_0: \\theta = \\theta_0 \\qquad \\text{vs.} \\qquad H_1: \\theta \\neq \\theta_0$$\n\nfor some $\\theta_0 \\in \\Theta$.  Suppose, a priori, we believe $Pr\\left(\\theta = \\theta_0\\right) = u$ for some $0 < u < 1$.  Then, \n\n$$\\pi(\\theta) = u \\delta\\left(\\theta - \\theta_0\\right) + (1 - u) \\pi_0(\\theta)$$\n\nis a suitable family of prior distributions, where $\\pi_0(\\theta)$ is any continuous distribution on the support $\\Theta$.\n:::\n\n:::{.callout-note}\nA mixture prior with a point mass is not a continuous distribution, nor is it a discrete distribution.\n:::\n\n:::{#exm-csec-hypotheses}\n## C-section Deliveries Continued\n@exm-csec introduced a study, a component of which includes estimating the probability of a mother undergoing a C-section delivery at a particular hospital.  \n\nSuppose the hospital is interested in testing\n\n$$H_0: \\theta = 0.304 \\qquad \\text{vs.} \\qquad H_1: \\theta \\neq 0.304.$$\n\nThe null hypothesis represents the C-section rate at the hospital being equivalent to the rate across the state of Indiana.  Suppose we believe, prior to observing any data, either hypothesis is equally likely.  Combining this belief with those expressed in @exm-csec-prior, a reasonable prior distribution has the form\n\n$$\\pi(\\theta) = 0.5 \\delta(\\theta - 0.304) + 0.5 \\frac{\\Gamma(a + b)}{\\Gamma(a)\\Gamma(b)}\\theta^{a-1}(1-\\theta)^{b-1},$$\n\n\n::: {.cell}\n\n:::\n\n\n\nwhere $a = 10.3$ and $b = 23.6$.  Note that the mass at $\\theta_0 = 0.304$ led to different values of hyperparameters in $\\pi_0(\\theta)$ compared to @exm-csec-prior.  The resulting posterior will have the form\n\n$$\\pi(\\theta \\mid \\bm{x}) = w \\delta(\\theta - 0.304) + (1 - w) \\frac{\\Gamma(a + b + n + n\\bar{x})}{\\Gamma(a + n)\\Gamma(b + n\\bar{x})} \\theta^{a + n - 1} (1 - \\theta)^{b + n\\bar{x} - 1},$$\n\nwhere\n\n$$w = \\frac{(0.5)(0.304)^n (1 - 0.304)^{n\\bar{x}}}{(0.5)(0.304)^n(1-0.304)^{n\\bar{x}} + (0.5) \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}\\frac{\\Gamma(a + n)\\Gamma(b + n\\bar{x})}{\\Gamma(a + b + n + n\\bar{x})}},$$\n\nthe sample size $n$ and sample mean $\\bar{x}$ are given by the data, and the hyperparameters $a$ and $b$ were defined above.\n\nUsing this posterior distribution, describe our belief about the hypotheses.\n:::\n\n:::{.solution}\nNotice that the form of the posterior distribution directly encodes our belief about the null hypothesis given the data.  Specifically,\n\n$$Pr\\left(\\theta = \\theta_0 \\mid \\bm{x}\\right) = w = 0.6098.$$\n\nThe posterior odds in favor of the null hypothesis is\n\n$$\\frac{Pr\\left(\\theta = \\theta_0 \\mid \\bm{x}\\right)}{Pr\\left(\\theta \\neq \\theta_0 \\mid \\bm{x}\\right)} = 1.56,$$\n\nand the Bayes Factor is the same (since the prior odds were 1).  The data strengthened our belief in the null hypothesis, but not by much.\n:::\n\n\n## Model Comparison\nHypothesis testing is a special case of model comparison, where in the Bayesian framework the \"model\" consists of _both_ the likelihood and the prior.\n\n:::{.callout-tip}\n## Big Idea\nA Bayesian model consists of the choice for the likelihood as well as the choice for the prior.\n:::\n\nConsider the Naive Classification of College Students (@exm-naive); we can reframe the problem as a choice between two models:\n\n$$\n\\begin{aligned}\n  \\text{Model 0}:& \\quad \\pi(\\theta) = \\delta(\\theta - 0.246) \\\\\n    &\\quad f(y \\mid \\theta) = \\binom{n}{\\theta} \\theta^y (1-\\theta)^{n-y} \\\\\n  \\text{Model 1}:& \\quad \\pi(\\theta) = \\delta(\\theta - 0.563) \\\\\n    &\\quad f(y \\mid \\theta) = \\binom{n}{\\theta} \\theta^y (1-\\theta)^{n-y},\n\\end{aligned}\n$$\n\nwhere we believed, a priori, that $Pr(\\text{Model 1}) = 0.4$ and $Pr(\\text{Model 2}) = 0.6$.  In this case, the models differed only in their choice of prior (which here simplifies further to which value of the parameter to select).  This simplification allowed us to choose between these two models by working only with the posterior distribution.    \n\nWe would like to generalize this process to allow the model to alter both the likelihood and the prior distribution, and for us to place some prior probability on the entirety of the model.  We are then interested in using the data to quantify the evidence for each model.  To do this, we essentially consider the model $\\mathcal{M}$ to be a parameter.  This is known as a _hierarchical model_ because the priors on the parameter are conditional on the model, and then a further prior is placed on the model itself (it is a multi-level model).\n\n:::{.callout-important}\n## Model Comparison\nLet $\\mathcal{M}_j$ represent the $j$-th potential model for a data generating process.  Reflect the likelihood and prior as a function of the model.  For example, write\n\n$$\n\\begin{aligned}\n  \\text{Model 0}:& \\quad f_0(\\bm{y} \\mid \\theta_0, \\mathcal{M}_0) \\\\\n    & \\quad \\pi_0(\\theta_0 \\mid \\mathcal{M}_0) \\\\\n    & \\quad \\pi(\\mathcal{M}_0) = Pr(\\mathcal{M}_0) \\\\\n  \\text{Model 1}:& \\quad f_1(\\bm{y} \\mid \\theta_1, \\mathcal{M}_1) \\\\\n    & \\quad \\pi_1(\\theta_1 \\mid \\mathcal{M}_1) \\\\\n    & \\quad \\pi(\\mathcal{M}_1) = Pr(\\mathcal{M}_1). \\\\\n\\end{aligned}\n$$\n\nNotice we (potentially) allow the form of the likelihood, the parameters governing that likelihood, and the form of the prior to differ for each model.  Our prior beliefs about the parameter are captured within each prior, but we also have prior beliefs about the model itself --- how likely each model is.  We are interested in determining $Pr(\\mathcal{M}_j \\mid \\bm{Y})$ for each $j$.\n:::\n\nAn iterated application of Bayes Theorem allows us to write the likelihood of each model, given the data, as\n\n$$\n\\begin{aligned}\n  Pr\\left(\\mathcal{M}_j \\mid \\bm{Y}\\right) \n    &= \\frac{f_j(\\bm{y} \\mid \\mathcal{M}_j) Pr(\\mathcal{M}_j)}{\\sum_{j} f_j(\\bm{y} \\mid \\mathcal{M}_j) Pr(\\mathcal{M}_j)} \\\\\n  f_j(\\bm{y} \\mid \\mathcal{M}_j) \n    &= \\int f_j(\\bm{y} \\mid \\theta_j, \\mathcal{M}_j) \\pi_j(\\theta_j \\mid \\mathcal{M}_j) d\\theta_j.\n\\end{aligned}\n$$\n\n:::{#def-evidence}\n## Evidence for a Model\nUnder the Model Comparison framework defined above, the evidence for model $\\mathcal{M}_j$ is defined as\n\n$$f_j(\\bm{y} \\mid \\mathcal{M}_j) = \\int f_j(\\bm{y} \\mid \\theta_j, \\mathcal{M}_j) \\pi_j(\\theta_j \\mid \\mathcal{M}_j) d\\theta_j.$$\n:::\n\nThe evidence for a model is a number; since it is a function only of observed data, once we observe the data, the evidence is a constant.  We can also think of the evidence as the prior predictive distribution under a particular model evaluated at the observed data. \n\n:::{.callout-warning}\nIt may seem strange to use the prior predictive distribution when defining the evidence instead of the posterior predictive, but keep in mind that within model comparison, the _model itself_ is the parameter of interest.  Changing either the likelihood or the prior will impact the evidence.\n:::\n\nThe evidence can also be used to compute the Bayes Factor for one model over another.\n\n:::{#def-bayes-factor-models}\n## Bayes Factor for Model Comparison\nThe Bayes Factor, in favor of Model 1, is\n\n$$\n\\begin{aligned}\n  BF_{1} &= \\left(\\frac{Pr(\\mathcal{M}_1 \\mid \\bm{y})}{Pr(\\mathcal{M}_0 \\mid \\bm{y})}\\right)\\left(\\frac{Pr(\\mathcal{M}_0)}{Pr(\\mathcal{M}_1)}\\right) \\\\\n    &= \\left(\\frac{f_1(\\bm{y} \\mid \\mathcal{M}_1) Pr(\\mathcal{M}_1)}{f_0(\\bm{y} \\mid \\mathcal{M}_0) Pr(\\mathcal{M}_0)}\\right)\\left(\\frac{Pr(\\mathcal{M}_0)}{Pr(\\mathcal{M}_1)}\\right) \\\\\n    &= \\frac{f_1(\\bm{y} \\mid \\mathcal{M}_1)}{f_0(\\bm{y} \\mid \\mathcal{M}_0)}.\n\\end{aligned}\n$$\n\nThat is, the Bayes Factor is a ratio of the evidence for each model.\n:::\n\nModel comparison simply extends our framework by the inclusion of another parameter.  That parameter, the model itself, just happens to be discrete.  Our goal is to use our machinery to quantify the uncertainty in each model given the data observed.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}