[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Data Analysis",
    "section": "",
    "text": "Preface\nData is all around us. And, that data will be subject to variability; that is, measured characteristics will vary from one observation to the next. Learning to characterize that variability and make decisions in its presence is the idea behind statistics. The text emphasizes statistical literacy (interpretation and clear communication of statistical concepts, methods, and results) and statistical reasoning (defining the need for data to address questions, modeling variability in a process, and choosing the appropriate methodology to address a question of interest).\nSpecifically, this text introduces the Bayesian framework for statistical inference. Building from Bayes’ Rule for probability computations, we develop a framework of estimation and hypothesis testing. We examine inference in several scenarios, including regression analysis. The heart of Bayesian inference is quantifying our beliefs about the data generating process prior to collecting data, and then using the observed data to update those beliefs. We discuss the construction of prior distributions given prior information about a parameter and give an introduction to computational tools for Bayesian inference, including Markov Chain Monte Carlo (MCMC) methods.\nWhile we do work through derivations when introducing the fundamental elements of Bayesian inference, the text is applied. We therefore move quickly to computational approaches for the Bayesian approach. We focus on choosing an appropriate modeling strategy and interpreting the results of an analysis. Our aim is to provide a strong foundation in statistical ideas enabling readers to engage with research encountered in their field."
  },
  {
    "objectID": "01a-probability.html",
    "href": "01a-probability.html",
    "title": "Unit I: Essential Probability",
    "section": "",
    "text": "Probability is the field within mathematics that studies and models random processes. In contrast, Statistics is a discipline separate from mathematics that uses data to make inference on a population. Like many other disciplines (e.g., Engineering and the Sciences), while Statistics is a separate discipline, the theory underlying the discipline relies heavily on mathematics; for Statistics, probability plays a pivotal role. In fact, we once heard an author describe the Bayesian framework as “probability in action.” The key components of a Bayesian approach to inference involve characterizing uncertainty and variability. And, Probability can be used to develop analytical models to describe that uncertainty and variability. A firm foundation in probability is necessary to approach inference from a Bayesian perspective.\nThis unit is not a replacement for a text in Probability; instead, it provides a brief review of key aspects of a Probability course that will be most frequently referenced in the remainder of the text. Our interest is in illustrating how Probability is applied to support statistical methodology. While we assume the reader has taken a course in Probability, we review key results as needed. As this is meant to be used in a Statistics course, our goals are much different than those of a mathematician. Instead of a rigorous treatment of Probability theory (axioms, etc.), our focus is on the application of Probability to Statistics."
  },
  {
    "objectID": "01b-fundamentals.html#probability-of-an-event",
    "href": "01b-fundamentals.html#probability-of-an-event",
    "title": "1  Essential Probability",
    "section": "1.1 Probability of an Event",
    "text": "1.1 Probability of an Event\nAny process for which the outcome cannot be predicted with certainty is a random process. The collection of all possible results from this random process is known as the sample space, and elementary probability is centered on events (results of interest) within this sample space.\n\nDefinition 1.1 (Sample Space) The sample space for a random process is the collection of all possible results that we might observe.\n\n\nDefinition 1.2 (Event) A subset of the sample space that is of particular interest.\n\nThe Axioms of Probability are discussed in terms of such events.\n\nDefinition 1.3 (Axioms of Probability) Let \\(\\mathcal{S}\\) be the sample space of a random process. Suppose that to each event \\(A\\) within \\(\\mathcal{S}\\), a number denoted by \\(Pr(A)\\) is associated with \\(A\\). If the map \\(Pr(\\cdot)\\) satisfies the following three axioms, then it is called a probability:\n\n\\(Pr(A) \\geq 0\\)\n\\(Pr(\\mathcal{S}) = 1\\)\nIf \\(\\left\\{A_1, A_2, \\dotsc\\right\\}\\) is a sequence of mutually exclusive events in \\(\\mathcal{S}\\), then\n\n\\[Pr\\left(\\bigcup_{i = 1}^{\\infty} A_i\\right) = \\sum_{i = 1}^{\\infty} Pr\\left(A_i\\right).\\]\n\\(Pr(A)\\) is said to be the “probability of \\(A\\)” or the “probability \\(A\\) occurs.”\n\nThe first axiom states that probabilities cannot be negative. The second states that probabilities cannot exceed 1 and that something must result from a random process. The third states that if two events do not overlap, the probability of the combination of the events is found by adding up the individual probabilities. This third axiom begins to develop an idea of probability as an area. Figure 1.1 illustrates a hypothetical sample space \\(\\mathcal{S}\\) with two events \\(A\\) and \\(B\\) of interest. In the figure, the two events share some overlap. Variations of this graphic are used in probability courses to develop intuition for several probability rules. What we emphasize is that we are using the area of each event in the figure to represent probability. The applications of probability we will be studying continue to build on this idea of probability as an area.\n\n\n\nFigure 1.1: Venn-Diagram illustrating two events, \\(A\\) and \\(B\\), within a sample space \\(\\mathcal{S}\\).\n\n\n\n\n\n\n\n\nBig Idea\n\n\n\nProbability represents an area."
  },
  {
    "objectID": "01b-fundamentals.html#essential-results",
    "href": "01b-fundamentals.html#essential-results",
    "title": "1  Essential Probability",
    "section": "1.2 Essential Results",
    "text": "1.2 Essential Results\nWhile the Axioms of Probability (Definition 1.3) set the foundation, we can combine these axioms to form a set of rules which can be employed to describe a myriad of scenarios. The first rule we review states that the probability of an event not occurring is equivalent to subtracting the probability it does occur from 1.\n\nTheorem 1.1 (Complement Rule) For any event \\(A\\), the probability of its complement \\(A^c\\) is given by\n\\[Pr\\left(A^c\\right) = 1 - Pr(A).\\]\n\nOur interest is not in rigorously developing probability theory; so, we will offer many results without proof. However, to illustrate the connection to the axioms, note that the Complement Rule is a result of the second and third axioms. The second axiom tells us the probability of the sample space is 1, and the third axiom allows us to consider the probability of the union of two mutually exclusive events (which an event and its complement are by definition).\nThe second rule we consider generalizes the third axiom. The third axiom considers the union of mutually exclusive events, and the Addition Rule defines the probability for the union of arbitrary events.\n\nTheorem 1.2 (Addition Rule) Let \\(A\\) and \\(B\\) be arbitrary events, the probability of the union \\(A \\cup B\\) is given by\n\\[Pr(A \\cup B) = Pr(A) + Pr(B) - Pr(A \\cap B)\\]\nwhere \\(A \\cap B\\) represents the intersection of the two events.\n\nA very helpful technique in mathematical proofs is to “do nothing.” This technique will be a recurring theme later in the text and manifests itself in adding nothing (adding and subtracting the same quantity to an expression) or multiplying by one (multiplying and dividing an expression by the same quantity).\n\nTheorem 1.3 (Total Probability Rule) Let \\(A\\) and \\(B\\) be arbitrary events. Then,\n\\[Pr(A) = Pr(A \\cap B) + Pr\\left(A \\cap B^c\\right).\\]\n\nThough different than the proof you would likely encounter in a Probability text, we provide the proof below because it illustrates the “do nothing” technique that will be helpful later on.\n\nProof. Let \\(A\\) and \\(B\\) be arbitrary events. We note that \\(A \\cap \\mathcal{S}\\) is the set \\(A\\). And, since the intersection of any set with itself is itself (like multiplying by 1, or “doing nothing” to the set), we have\n\\[Pr(A) = Pr(A \\cap \\mathcal{S}).\\]\nNow, we recognize that an event and its complement together form the sample space; therefore, we can write\n\\[Pr(A \\cap \\mathcal{S}) = Pr\\left(A \\cap \\left(B \\cup B^c\\right)\\right).\\]\nUsing a distributive law from set theory, we write this probability as\n\\[Pr\\left(A \\cap \\left(B \\cup B^c\\right)\\right) = Pr\\left((A \\cap B) \\cup \\left(A \\cap B^c\\right)\\right).\\]\nWe now recognize that the events \\((A \\cap B)\\) and \\(\\left(A \\cap B^c\\right)\\) are mutually exclusive. Therefore, applying the third axiom of probability, we have that\n\\[Pr\\left((A \\cap B) \\cup \\left(A \\cap B^c\\right)\\right) = Pr(A \\cap B) + Pr\\left(A \\cap B^c\\right)\\]\ngiving the desired result.\n\nWe have described probability as an “area,” and the above results describe various ways of computing that area. However, occasionally we are given additional information that changes the likelihood of an event. Suppose we are interested in the probability an individual makes a shot from the half-court line on a basketball court. Now, suppose we are told the individual plays for the NBA; our probability should reflect this additional knowledge. This is the idea of “conditional probability.”\n\nTheorem 1.4 (Conditional Probability) Let \\(A\\) and \\(B\\) be arbitrary events. Then, the probability of \\(A\\) given that \\(B\\) will occur is given by\n\\[Pr(A \\mid B) = \\frac{Pr(A \\cap B)}{Pr(B)},\\]\nwhere we read \\(A \\mid B\\) as “A given B.”\n\nConditional probability assumes \\(Pr(B) &gt; 0\\); it would not make sense to condition on an event that will not occur. While there are many other rules that are interesting and useful in application, the above rules suffice for our purposes."
  },
  {
    "objectID": "01b-fundamentals.html#interpretation-of-probability",
    "href": "01b-fundamentals.html#interpretation-of-probability",
    "title": "1  Essential Probability",
    "section": "1.3 Interpretation of Probability",
    "text": "1.3 Interpretation of Probability\nAgain, most probability courses are focused on the mathematics of probability; as a result, rarely is the interpretation of probability discussed. In fact, most individuals rarely think about what they mean by “the probability an event occurs.” From a mathematical perspective, as long as we obey the Axioms of Probability (Definition 1.3), we have a probability; its meaning is irrelevant. But, for practitioners, the interpretation is critical. As it turns out, there are multiple interpretations of probability1. Two interpretations are of particular interest to us. To illustrate, consider the following scenario.\n\nExample 1.1 (Sugar Packets) Restaurants can be sources of anxiety for small children. After placing their order, they must wait (for what seems like an eternity) for that food to arrive. This is different from their experience at home where they typically are not brought to the table until it is time to eat. Parents spend a lot of effort entertaining their children while waiting for their food to arrive. For parents who do not want to limit screen time, the following simple game is surprisingly effective:\n\nTake one of the sugar packets that is generally available at the table. Denote the side with the brand name as the “top side” and denote the side with the ingredient list as the “bottom side.” The parent then takes the sugar packet and, hidden from view, tumbles the packet randomly in their hands. The packet is then placed on the table under the cover of the parent’s hand. The child then declares which side of the packet is facing up by saying “top side” or “bottom side.”\n\nThis is similar to flipping a coin, but who carries change with them these days? Consider one round of the above game; suppose the (covered) packet has been placed on the table and the child says “top side.” The question we ask is then “what is the probability the child is correct?”\n\nThis simple example illustrates the two commonly applied interpretations of probability. Most people will say the probability the child is correct is 0.5. The reasoning is that there are two possibilities (the top of the sugar packet is face up; or, the bottom of the sugar packet is face up), and these two possibilities are equally likely (since it was randomly shuffled before being placed on the table). Therefore, the probability the child is correct is 0.5. From a classical view of statistics, this interpretation is incorrect. The complication here is what we believe probability is capturing.\nFrom a classical (or “Frequentist”) perspective, probability is capturing the likelihood of an event across repeated trials. From a Bayesian perspective, however, probability is used to quantify our uncertainty. That is, from the Bayesian perspective the phrase “the probability the child is correct is 0.5” is not actually quantifying the likelihood the child is correct — it is quantifying our uncertainty the child is correct. We are only “50% sure” the child is correct. This relies on the “subjective interpretation” of probability.\n\nDefinition 1.4 (Subjective Interpretation of Probability) In this perspective, the probability of \\(A\\) describes the individual’s uncertainty about event \\(A\\).\n\nBecause the subjective interpretation is quantifying an individual’s uncertainty, and since each individual may have different beliefs/information/expertise about the random process, each individual observing the same process may have a different probability. For example, consider asking the question “what is the probability that Netflix saves the latest television series dropped by ABC?” A casual viewer may have little information regarding this process and will rely solely on what they perceive the popularity of the show was among its fan base and news reports they have read online; they may quantify their uncertainty by saying the probability is 0.65. In contrast, an executive at Netflix who is deeply familiar with both the show, its fan base, its ratings in various markets, the interest of leadership to invest in a new series, and the amount they stand to earn by acquiring the property has a different set of knowledge; they may quantify their uncertainty by saying the probability is 0.05. The same process is viewed differently by different observers, leading to different answers.\nStatisticians who adhere to the subjective interpretation of probability are known as Bayesians. Classically, statistical theory was developed under the frequentist interpretation, and statisticians who adhere to this perspective are known as Frequentists.\n\nDefinition 1.5 (Frequentist Interpretation of Probability) In this perspective, the probability of \\(A\\) describes the long-run behavior of the event. Specifically, consider repeating the random process \\(m\\) times, and let \\(f(A)\\) represent the number of times the event \\(A\\) occurs out of those \\(m\\) replications. Then,\n\\[Pr(A) = \\lim_{m \\rightarrow \\infty} \\frac{f(A)}{m}.\\]\n\nThe frequentist interpretation requires repeating a process infinitely often. When characterizing the probability of an event, the frequentist perspective leans on the future-oriented nature of probability. When we are characterizing the probability an event will occur (future-oriented), we are really thinking about repeating that process infinitely often and determining what fraction of the time the event occurs; we then apply that to the specific process we are about to observe. Of course, this does not always make sense in practice. For example, asking “what is the probability that Candidate A will win the upcoming election” is a one time event. The election cannot be held infinitely often; it will only be held once. In these cases, the frequentist interpretation still imagines infinitely many of these elections. For those who are fans of science fiction, you can think of the frequentist perspective as finding the limit over the infinitely many instances in the multiverse (the proportion of times Candidate A wins the election across all instances of the election in the multiverse). The frequentist perspective is “objective” in the sense that it does not incorporate the observer’s personal beliefs/information/expertise regarding the process.\nReturning to Example 1.1, since the result has already occurred, probability does not make sense. Further, since the frequentist perspective does not quantify our uncertainty about the result (as the subjective perspective does), we are left saying that the probability that the child is correct is either 1 (they are correct) or 0 (they are not correct). Admittedly, this is unsatisfying, but we must remember that the frequentist interpretation is not interested in quantifying our uncertainty; it is only interested in the proportion of times the result will occur, and since the result is in the past, it either has occurred (proportion of 1) or it has not (proportion of 0).\nThis may seem like arguing over semantics, and admittedly, the importance of this discussion is not yet clear. But, we will see that how probability is interpreted impacts how we interpret the results of our statistical analyses.\n\n\n\n\n\n\nBig Idea\n\n\n\nThe frequentist interpretation of probability quantifies the likelihood of an event in repeated observation, and the subjective interpretation quantifies our uncertainty of an event.\n\n\nAs this text focuses on the Bayesian perspective, we adopt the subjective interpretation of probability throughout. Note that this means that two analysts can approach the same problem in the same way and end up with a different conclusion if they have different beliefs!"
  },
  {
    "objectID": "01b-fundamentals.html#footnotes",
    "href": "01b-fundamentals.html#footnotes",
    "title": "1  Essential Probability",
    "section": "",
    "text": "See the “Interpretations of Probability” entry in the Stanford Encyclopedia of Philosophy.↩︎"
  },
  {
    "objectID": "01c-randomvariables.html#random-variables",
    "href": "01c-randomvariables.html#random-variables",
    "title": "2  Random Variables and Distributions",
    "section": "2.1 Random Variables",
    "text": "2.1 Random Variables\nConsider collecting data; before the data is collected, we cannot predict with certainty what we will observe. Therefore, we can think of each observation as the result of a random process. These observations are recorded as variables in our dataset. In probability, a random variable is used to represent a measurement that results from a random process.\n\nDefinition 2.1 (Random Variable) Let \\(\\mathcal{S}\\) be the sample space corresponding to a random process; a random variable \\(X\\) is a function mapping elements of the sample space to the real line.\nRandom variables represent a measurement that will be collected during the course of a study. Random variables are typically represented by a capital letter.\n\nWhile for our purposes, it suffices to think of a random variable as a measurement, mathematically, it is a function. The image (or range) of this function is used to broadly classify random variables as continuous or discrete; we refer to this image as the support of the random variable.\n\nDefinition 2.2 (Support) The support of a random variable is the set of all possible values the random variable can take.\n\n\nDefinition 2.3 (Continuous and Discrete Random Variable) The random variable \\(X\\) is said to be a discrete random variable if its corresponding support is countable. The random variable \\(X\\) is said to be a continuous random variable if the corresponding support is uncountable (such as an interval or a union of intervals on the real line).\n\nDiscrete random variables are analogous to categorical (or qualitative) variables in data analysis; that is, discrete random variables are used to model the result of a random process which categorizes each unit of observation into a group. Continuous random variables are analogous to numeric (or quantitative) variables in data analysis; continuous random variables are used to model the result of a random process which produces a number for which arithmetic makes sense.\n\n\n\n\n\n\nWarning\n\n\n\nWhether we use a continuous or discrete random variable to represent a measurement is not always obvious. Suppose we consider recording the age of a student selected from a class at a university that typically enrolls “traditional” students (those coming directly from high school). Let the random variable \\(X\\) denote the age of the student.\nIf we record the student’s age in years since birth, \\(X\\) can take on only a finite number of values (most likely \\(\\{18, 19, 20, 21, 22, 23\\}\\)), making it a discrete random variable. However, if we record the student’s age as the number of seconds since birth, we might well consider the support of \\(X\\) to be a rather large interval, leading to a continuous random variable.\n\n\nThe goal of statistics is to use a sample to say something about the underlying population. Consider taking a sample of size \\(n\\) and measuring a single variable on each unit of observation. Then, we might represent the measurements we will obtain (note the use of the future tense) as \\(X_1, X_2, \\dots, X_n\\). While the majority of probability courses focus on a single, or maybe two, random variables, note that collecting data on a sample requires that we deal with at least \\(n\\) random variables (one measurement for each of the observations in our sample)."
  },
  {
    "objectID": "01c-randomvariables.html#characterizing-a-distribution",
    "href": "01c-randomvariables.html#characterizing-a-distribution",
    "title": "2  Random Variables and Distributions",
    "section": "2.2 Characterizing a Distribution",
    "text": "2.2 Characterizing a Distribution\nAgain, the goal of statistics is to use a sample to say something about the underlying population. Consider the following research objective:\n\nEstimate the cost (in US dollars) of a diamond for sale in the United States.\n\nFor this research objective, our population of interest is all diamonds for sale in the United States. We would not expect every diamond for sale to have the same price; variability is inherent in any process. As a result, the sale price of diamonds has a distribution across this population. This is our primary use of probability theory in a statistical analysis — to model distributions.\nConsider taking a sample of size 1 from the population; let \\(Y\\) represent the cost of the diamond that is selected. Since we have not yet observed the cost of this diamond, \\(Y\\) is a random variable. And, since this diamond is sampled from the population of interest, the support of \\(Y\\) is determined by the cost of diamonds in the United States. Further, the likelihood that \\(Y\\) falls within any interval is determined by the distribution of the cost across the population. That is, the distribution of \\(Y\\) is the distribution of the population.\n\n\n\n\n\n\nBig Idea\n\n\n\nIf a random variable \\(X\\) represents a measurement for a single observation from a population, the distribution of the random variable corresponds to the distribution of the variable across the population.\n\n\nA key realization in statistical analysis is that we will never fully observe the distribution of the population; however, we can posit a model for this distribution. In probability, the most common way to characterize the distribution of a random variable is through its density function.\n\nDefinition 2.4 (Density Function) A density function \\(f\\) relates the values in the support of a random variable with the probability of observing those values.\nLet \\(X\\) be a continuous random variable, then its density function \\(f\\) is the function such that\n\\[Pr(a \\leq X \\leq b) = \\int_a^b f(x) dx\\]\nfor any real numbers \\(a\\) and \\(b\\) in the support.\nLet \\(X\\) be a discrete random variable, then its density function \\(f\\) is the function such that\n\\[Pr(X = u) = f(u)\\]\nfor any real number \\(u\\) in the support.\n\n\n\n\n\n\n\nProperties of a Density Function\n\n\n\nLet \\(X\\) be a random variable with density function \\(f\\) defined over support \\(\\mathcal{S}\\). Then,\n\n\\(f(x) \\geq 0\\) for all \\(x \\in \\mathcal{S}\\). That is, the density is non-negative for all values in the support.\nIf \\(X\\) is a continuous random variable, then \\(\\int_{\\mathcal{S}} f(x) dx = 1\\); similarly, if \\(X\\) is a discrete random variable, then \\(\\sum_{\\mathcal{S}} f(x) = 1\\). That is, \\(X\\) must take a value in its support; so, \\(Pr(X \\in \\mathcal{S}) = 1\\), similar to the second Axiom of Probability (Definition 1.3).\n\\(f(x) = 0\\) for all values of \\(x \\notin \\mathcal{S}\\). The density takes the value of 0 for all values outside the support.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn a probability course, there is often a distinction made between probability “density” functions (used for continuous random variables) and probability “mass” functions (used for discrete random variables). We do not make this distinction and instead rely on the context to determine whether we are dealing with a continuous or discrete random variable. Throughout, we will note when the operations differ between these two types of variables. Measure theory provides a unifying framework to these issues.\n\n\nWhen working with a continuous random variable, the density function is a smooth function over some region, and the actual value of the function is not interpretable; instead, we get at a probability by considering the area under the curve. Again, drawing connections to data analysis, we can think of a density function as a mathematical formula representing a smooth histogram. The area under the curve for any region gives the proportion of the population which has a value in that region. That is, we get the probability that a random variable will be in an interval by integrating the density function over that interval.\nFigure Figure 2.1 illustrates this idea; we have data from a sample of diamonds from the population of interest. The sample is summarized with a histogram; we have overlayed a posited density (with the corresponding mathematical function that describes this density) for the population. The sample (summarized with the histogram) is approximating the population (modeled using the density function).\n\n\n\n\n\nFigure 2.1: Illustration of a density function representing the posited distribution of the population alongside a histogram summarizing the cost of diamonds using a sample of 53940 diamonds.\n\n\n\n\nYou may recognize the particular form of the density function in Figure 2.1. The general form is\n\\[f(x) = \\frac{1}{\\sigma} e^{-x / \\sigma} \\qquad \\text{for } x &gt; 0\\]\nwhere \\(\\sigma\\) is the scale parameter that defines the distribution (set at 4000 in Figure 2.1). This is known as the Exponential distribution with scale parameter \\(\\sigma\\). This illuminates another connection between probability and statistics.\nNote that our research objective describe above is an ill-posed question as stated. The answer is “it depends” since each individual diamond in the population has a different value. Well-posed questions in statistics are centered on an appropriately chosen parameter (Definition 5.6).\nIn probability, the parameters are values that are tuned or set within a problem; we then work forward to compute the probability of an event of interest. In practice, however, when we posit a functional form for a density function to describe the distribution of the population, the parameters are unknown. We plan to use the data to estimate or characterize the parameter; but, the parameter itself will remain unknown. In both cases, however, the parameter is a fixed quantity, even if we are ignorant of that value.\n\n\n\n\n\n\nBig Idea\n\n\n\nWhen a probability model is specified for a population, it is generally specified up to some unknown parameter(s). Making inference on the unknown parameter(s) therefore characterizes the entire distribution.\n\n\n\n2.2.1 Common Parameters\nMost scientific questions are focused on the location or spread of a distribution. For example, we are interested in estimating the average cost of a diamond sold in the United States. Introductory statistics introduces summaries of location and spread within the sample (e.g., sample mean for location and sample variance for spread). Analogous summaries exist for density functions. As stated above, parameters are unknown constants that govern the form of the density function. Because they govern the form of the density, the parameters are also related to those summarizing the location or spread of the distribution.\n\nDefinition 2.5 (Expected Value (Mean)) Let \\(X\\) be a random variable with density function \\(f\\) defined over the support \\(\\mathcal{S}\\). The expected value of a random variable, also called the mean and denoted \\(E(X)\\), is given by\n\\[E(X) = \\int_{\\mathcal{S}} x f(x) dx\\]\nfor continuous random variables and\n\\[E(X) = \\sum_{\\mathcal{S}} x f(x)\\]\nfor discrete random variables.\n\nNotice the similarity between the form of the sample mean and the population mean. A sample mean takes the sum of each value in the sample, weighting each value by \\(1/n\\) (where \\(n\\) is the sample size). Without information about the underlying population, the sample must treat each value observed as equally likely; values become more likely if they appear multiple times. In the population, however, when the form of \\(f\\) is known, the density provides information about the likelihood of each value giving us a better weight than \\(1/n\\). That is, the population mean is a sum of the values in the support, weighting each value by the corresponding value of the density function.\n\nDefinition 2.6 (Variance) Let \\(X\\) be a random variable with density function \\(f\\) defined over the support \\(\\mathcal{S}\\). The variance of a random variable, denoted \\(Var(X)\\), is given by\n\\[Var(X) = E\\left[X - E(X)\\right]^2 = E\\left(X^2\\right) - E^2(X).\\]\nIf we let \\(\\mu = E(X)\\), then this is equivalent to\n\\[\\int_{\\mathcal{S}} (x - \\mu)^2 f(x) dx\\]\nfor continuous random variables and\n\\[\\sum_{\\mathcal{S}} (x - \\mu)^2 f(x)\\]\nfor discrete random variables.\n\n\n\n\n\n\n\nWarning\n\n\n\nPay careful attention to the notation. \\(E^2(X)\\) represents the square of the expected value; that is,\n\\[E^2(X) = \\left[E(X)\\right]^2.\\]\nHowever, \\(E(X)^2\\) represents the expected value of the square of \\(X\\); that is,\n\\[E(X)^2 = E\\left(X^2\\right).\\]\n\n\nThe variance provides a measure of spread; in particular, it is capturing distance from the mean. Notice that the form of the variance involves taking the expectation of a squared term; in general, we will need to consider expectations of functions.\n\nDefinition 2.7 (Expectation of a Function) Let \\(X\\) be a random variable with density function \\(f\\) over the support \\(\\mathcal{S}\\), and let \\(g\\) be a real-valued function. Then,\n\\[E\\left[g(X)\\right] = \\int_{\\mathcal{S}} g(x) f(x) dx\\]\nfor continuous random variables and\n\\[E\\left[g(X)\\right] = \\sum_{\\mathcal{S}} g(x) f(x)\\]\nfor discrete random variables.\n\n\n\n\n\n\n\nNote\n\n\n\nDefinition 2.7 is sometimes referred to as the “Law of the Unconscious Statistician” in probability texts. We find the name somewhat insulting, as it suggests statisticians do not appreciate the mathematical underpinnings of their field. In reality, the expected value of a function is such a common operation in statistical theory that statisticians often present Definition 2.7 as the definition of an expectation (as we have done) instead of deriving it as a result of Definition 2.5 after applying a variable transformation (see Example 2.3 below). It is possible this is where the slight in the naming convention originated.\n\n\nA result of Definition 2.7 is the following, very useful theorem, which states that expectations are linear operators.\n\nTheorem 2.1 (Expectation of a Linear Combination) Let \\(X\\) be a random variable, and let \\(a_1, a_2, \\dotsc, a_m\\) be real-valued constants and \\(g_1, g_2, \\dotsc, g_m\\) be real-valued functions; then,\n\\[E\\left[\\sum_{i=1}^{m} a_i g_i(X)\\right] = \\sum_{i=1}^{m} a_i E\\left[g_i(X)\\right].\\]\n\nThe mean and variance play an important role in characterizing a distribution, especially within statistical theory (as we will see in future chapters). However, there is another set of parameters which are important.\n\nDefinition 2.8 (Percentile for a Random Variable) Let \\(X\\) be a random variable with density function \\(f\\). The \\(100k\\) percentile is the value \\(q\\) such that\n\\[Pr(X \\leq q) = k.\\]\n\n\nExample 2.1 (Parameters of Exponential Distribution) Let \\(X\\) be an Exponential distribution with scale parameter \\(\\sigma\\); that is, the density function \\(f\\) is given by\n\\[f(x) = \\frac{1}{\\sigma} e^{-x/\\sigma} \\qquad x &gt; 0\\]\nwhere \\(\\sigma &gt; 0.\\) Compute the mean, variance, and median of this distribution, as a function of the unknown scale parameter.\n\nThe solution to this problem is particularly important as it illustrates a very useful technique when working with known distributions in statistical theory.\n\nSolution. We note that the function\n\\[g(y) = \\frac{1}{\\beta^{\\alpha} \\Gamma(\\alpha)} y^{\\alpha - 1} e^{-y/\\beta}\\]\nis a valid density function over the positive real line provided that \\(\\alpha,\\beta &gt; 0\\); in particular, this is known as a Gamma distribution. Since \\(g\\) is a valid density function, then we know that\n\\[\\int_{0}^{\\infty} g(y) dy = 1\\]\nfor all values of \\(\\alpha,\\beta &gt; 0\\).\nNow, let \\(X\\) be an Exponential random random variable with scale parameter \\(\\sigma\\). Then, the expected value of \\(X\\) is given by\n\\[\n\\begin{aligned}\n  E(X)\n    &= \\int_{0}^{\\infty} x \\frac{1}{\\sigma} e^{-x/\\sigma} dx \\\\\n    &= \\int_{0}^{\\infty} \\frac{1}{\\sigma} x^{2-1} e^{-x/\\sigma} dx\n\\end{aligned}\n\\]\nwhere we have simply rewritten the exponent in the second line. Notice that expression within the integral shares a striking similarity to the form of the density function of a Gamma distribution; however, they are not exactly the same. To coerce the expression into that of the Gamma density function, we “do nothing” — multiplying and dividing the expression by the quantity \\(\\sigma\\Gamma(2)\\). This gives\n\\[\n\\begin{aligned}\n  E(X)\n    &= \\int_{0}^{\\infty} x \\frac{1}{\\sigma} e^{-x/\\sigma} dx \\\\\n    &= \\int_{0}^{\\infty} \\frac{1}{\\sigma} x^{2-1} e^{-x/\\sigma} dx \\\\\n    &= \\int_{0}^{\\infty} \\sigma \\Gamma(2) \\frac{1}{\\sigma^2 \\Gamma(2)} x^{2-1} e^{-x/\\sigma} dx \\\\\n    &= \\sigma \\Gamma(2) \\int_{0}^{\\infty} \\frac{1}{\\sigma^2 \\Gamma(2)} x^{2-1} e^{-x/\\sigma} dx \\\\\n    &= \\sigma \\Gamma(2) \\\\\n    &= \\sigma.\n\\end{aligned}\n\\]\nIn line 3, we have multiplied and divided by \\(\\sigma \\Gamma(2)\\), which does not change the problem. In line 4, we have pulled out the terms \\(\\sigma \\Gamma(2)\\) since it is a constant with respect to the integral; what is left inside the integral is the form of the density function for a Gamma distribution where \\(\\alpha = 2\\) and \\(\\beta = \\sigma\\). In line 5, we make use of the fact that the integral of any density function over the entire support for which it is defined must be 1. Finally, in line 6, we recognize that \\(\\Gamma(k) = (k-1)!\\) if \\(k\\) is a natural number.\nApplying the same process, we also have that\n\\[\n\\begin{aligned}\n  E\\left(X^2\\right)\n    &= \\int_{0}^{\\infty} x^2 \\frac{1}{\\sigma} e^{-x/\\sigma} dx \\\\\n    &= \\sigma^2 \\Gamma(3)\\int_{0}^{\\infty} \\frac{1}{\\sigma^3 \\Gamma(3)} x^{3-1} e^{-x/\\sigma} dx \\\\\n    &= 2\\sigma^2.\n\\end{aligned}\n\\]\nTherefore,\n\\[Var(X) = E\\left(X^2\\right) - E^2(X) = 2\\sigma^2 - \\sigma^2 = \\sigma^2.\\]\nFinally, the median is the value \\(q\\) such that \\(Pr(X \\leq q) = 0.5\\); but, we recognize that\n\\[\n\\begin{aligned}\n  Pr(X \\leq q)\n    &= \\int_{0}^{q} \\frac{1}{\\sigma} e^{-x/\\sigma} dx \\\\\n    &= \\left. -e^{-x/\\sigma} \\right|_{0}^{q} \\\\\n    &= -e^{-q/\\sigma} + 1.\n\\end{aligned}\n\\]\nSetting this expression equal to 0.5 and solving for \\(q\\) yields \\(q = -\\sigma \\log(0.5)\\), where \\(\\log(\\cdot)\\) represents the natural logarithm.\n\n\n\n\n\n\n\nBig Idea\n\n\n\nSuppose the density \\(f\\) is a function of the parameters \\(\\boldsymbol{\\theta}\\); then, the mean, variance, and median (as well as any other parameters of interest in a research objective) will be functions of \\(\\boldsymbol{\\theta}\\).\n\n\nExample 2.1 highlighted a useful technique for simplifying integrals in statistical applications, which makes use of the “do nothing” strategy discussed in the previous chapter.\n\n\n2.2.2 Kernels\nOne of the characteristics common to any density function that we noted above was that if we sum the density across the entire support, we get a value of 1. That is, if \\(X\\) is a discrete random variable, then\n\\[\\sum_{x \\in \\mathcal{S}_X} f(x) = 1,\\]\nand if \\(X\\) is a continuous random variable, then\n\\[\\int_{\\mathcal{S}_X} f(x) dx = 1.\\]\nAny density function can be written as\n\\[f(x) = a k(x)\\]\nwhere \\(a &gt; 0\\) is a constant and \\(k(x)\\) is a function of \\(x\\). Specifically, if \\(X\\) is a continuous random variable, then\n\\[a = \\frac{1}{\\int k(x) dx}\\]\nsince \\(\\int f(x) dx = 1\\). We call \\(k(x)\\) the kernel of the distribution. Kernels are helpful for quickly identifying distributions1.\n\nDefinition 2.9 (Kernel of a Distribution) Let \\(k(x)\\) be a non-negative function of \\(x\\) over some region \\(\\mathcal{S}_X\\). Then, a valid density function \\(f\\) over the support \\(\\mathcal{S}_X\\) can be constructed by taking\n\\[f(x) = a k(x)\\]\nwhere \\(a &gt; 0\\) is a suitably chosen scaling constant to ensure the density integrates (or sums) to 1 over the support. The function \\(k\\) is known as the kernel of the distribution, and it can be used to identify the distributional family for a random variable.\n\n\nExample 2.2 (Kernel of an Exponential Random Variable) In Example 2.1, we let \\(X\\) be an Exponential random variable with scale parameter \\(\\sigma\\). Identify the kernel of this distribution.\n\n\nSolution. Let \\(a = \\sigma^{-1}\\) and\n\\[k(x) = e^{-x/\\sigma};\\]\nthen, we have that \\(f(x) = a k(x)\\). We note that \\(k(x)\\) has no leading constants; therefore, the kernel for an Exponential distribution is\n\\[e^{-x/\\sigma}.\\]\n\nAs Example 2.1 illustrated, being able to identify a kernel can help us quickly evaluate an integral. In particular, notice that we immediately have that\n\\[\\int e^{-x/\\sigma} dx = \\sigma\\]\nfor any value of \\(\\sigma &gt; 0\\) because we know that\n\\[\n\\begin{aligned}\n  \\int e^{-x / \\sigma} dx\n    &= \\sigma \\int \\frac{1}{\\sigma} e^{-x / \\sigma} dx \\\\\n    &= \\sigma.\n\\end{aligned}\n\\]\nThe first line multiplies and divides by the appropriate scaling term so that the kernel becomes a valid density function. Once we have a valid density, we know it integrates to 1, simplifying the expression.\nIn addition to motivating the use of kernels in integration applications, the solution to Example 2.1 also shows that there is more than one way to characterize a distribution.\n\n\n2.2.3 Distribution Function\nEspecially for visualization, the density function is the most common way of characterizing a probability model. However, computing the probability using the density is problematic due to the integration required. Many software address this by working with the cumulative distribution function (CDF).\n\nDefinition 2.10 (Cumulative Distribution Function (CDF)) Let \\(X\\) be a random variable; the cumulative distribution function (CDF) is defined as\n\\[F(u) = Pr(X \\leq u).\\]\nFor a continuous random variable, we have that\n\\[F(u) = \\int_{-\\infty}^{u} f(x) dx\\]\nimplying that the density function is the derivative of the CDF. For a discrete random variable\n\\[F(u) = \\sum_{x \\leq u} f(x).\\]\n\nWorking with the CDF improves computation because it avoids the need to integrate each time; instead, the integral is computed once (and stored internally in the computer) and we use the result to compute probabilities directly.\n\n\n\n\n\n\nBig Idea\n\n\n\nDensity functions are the mathematical models for distributions; they link values of the variable with the likelihood of occurrence. However, for computational reasons, we often work with the cumulative distribution function which provides the probability of being less than or equal to a value."
  },
  {
    "objectID": "01c-randomvariables.html#transformations-of-a-random-variable",
    "href": "01c-randomvariables.html#transformations-of-a-random-variable",
    "title": "2  Random Variables and Distributions",
    "section": "2.3 Transformations of a Random Variable",
    "text": "2.3 Transformations of a Random Variable\nOccasionally, we are interested in a transformation of a particular characteristic. That is, we have a model for the distribution of \\(X\\), but we are interested in \\(Y = g(X)\\). In this section, we examine one method for determining the density of \\(Y\\) from the density of \\(X\\). While relationships between many common distributions have been well studied2, it is useful to know the process for addressing transformations.\nWhile there various approaches to this problem, we find this method the most reliable. Further, it does not require the memorization of a formula, but instead builds on fundamental ideals. This is known as the Method of Distribution Functions.\n\nDefinition 2.11 (Method of Distribution Functions) Let \\(X\\) be a continuous random variable with density \\(f\\) and cumulative distribution function \\(F\\). Consider \\(Y = h(X)\\). The following process provides the density function \\(g\\) of \\(Y\\) by first finding its cumulative distribution function \\(G\\).\n\nFind the set \\(A\\) for which \\(h(X) \\leq t\\) if and only if \\(X \\in A\\).\nRecognize that \\(G(y) = Pr(Y \\leq y) = Pr\\left(h(X) \\leq y\\right) = Pr(X \\in A)\\).\nIf interested in \\(g(y)\\), note that \\(g(y) = \\frac{\\partial}{\\partial y} G(y)\\).\n\n\nWhen \\(h\\) is a strictly monotone function (unique inverse exists), then step 1-2 is much easier because we can apply \\(h^{-1}\\). In step 2 of the above process, the final expression is often left in terms of \\(F\\), the CDF of \\(X\\); then, when we find the density in step 3, we can apply the chain rule (avoiding the need to actually have an expression for \\(F\\)).\n\nExample 2.3 (Transformation of a Random Variable) Previously, we posited the following model for the distribution of the cost of a diamond sold in the US:\n\\[f(x) = \\frac{1}{\\sigma} e^{-x/\\sigma} \\qquad x &gt; 0\\]\nfor some \\(\\sigma &gt; 0\\). As cost is generally a heavily skewed variable, we may be interested in taking the (natural) logarithm before proceeding with an analysis. Find the density of \\(Y = \\log(X)\\); then, write an expression for \\(E(Y)\\).\n\n\nSolution. We note that \\(\\log(x)\\) is a strictly monotone function. Therefore, we have that\n\\[\n\\begin{aligned}\n  G(y) &= Pr(Y \\leq y) \\\\\n    &= Pr(\\log(X) \\leq y) \\\\\n    &= Pr\\left(X \\leq e^y\\right).\n\\end{aligned}\n\\]\nJust to place this within the method described above, since \\(\\log(x) \\leq y\\) if and only if \\(x \\leq e^y\\), then \\(A = \\{t: x \\leq e^t\\}\\). Of course, we didn’t really need to identify this because we were able to apply the inverse of \\(\\log(x)\\) directly within the probability expression. We now recognize that we have a probability of the form “\\(X\\) less than or equal to something.” And, this matches the form of the CDF of \\(X\\). That is, we have that\n\\[G(y) = F\\left(e^y\\right).\\]\nThis completes step 2 of the procedure; we have expressed the CDF of \\(Y\\) as a function of the CDF of \\(X\\). Now, to find the density, we apply the chain rule.\n\\[\n\\begin{aligned}\n  g(y)\n    &= \\frac{\\partial}{\\partial y} G(y) \\\\\n    &= \\left[\\left.\\frac{\\partial}{\\partial x} F(x)\\right|_{x = e^y}\\right] \\frac{\\partial}{\\partial y} e^y \\\\\n    &= \\left[\\left. f(x) \\right|_{x = e^y}\\right] e^y \\\\\n    &= f\\left(e^y\\right) e^y \\\\\n    &= \\frac{1}{\\sigma} e^{-e^y/\\sigma} e^y\n\\end{aligned}\n\\]\nwhich will be valid for all real values of \\(y\\); that is, the support of \\(Y\\) is all real numbers. In line 2 above, we applied the chain rule to compute the derivative, avoiding the need to explicitly state the CDF of \\(X\\).\nGiven the density of \\(Y\\), we know (by Definition 2.5) that\n\\[E(Y) = \\int y g(y) dy = \\int y \\frac{1}{\\sigma} e^{-e^{y}/\\sigma} e^{y} dy.\\]\nLetting \\(u = e^y\\) (and therefore \\(du = e^y dy\\)) and performing a u-substitution, we have that\n\\[\n\\begin{aligned}\n  E(Y)\n    &= \\int y\\frac{1}{\\sigma} e^{-e^{y}/\\sigma} e^{y} dy \\\\\n    &= \\int \\log(u) \\frac{1}{\\sigma} e^{-u / \\sigma} du.\n\\end{aligned}\n\\]\nSince the variable of integration is arbitrary, we recognize that this integral as what we defined (in Definition 2.7) as \\(E\\left[\\log(X)\\right]\\) where \\(X\\) has density \\(f(x)\\) defined in Example 2.3.\n\n\n\n\n\n\n\nWarning\n\n\n\nWhile mathematicians distinguish between a derivative \\(\\frac{d}{dx}\\) and a partial derivative \\(\\frac{\\partial}{\\partial x}\\), we do not make that distinction."
  },
  {
    "objectID": "01c-randomvariables.html#footnotes",
    "href": "01c-randomvariables.html#footnotes",
    "title": "2  Random Variables and Distributions",
    "section": "",
    "text": "A good table of common distributions is given in Casella and Berger, a popular text for statistical theory at the graduate level.↩︎\nAn excellent summary of the relationships between Distributions was developed by faculty at the College of William and Mary.↩︎"
  },
  {
    "objectID": "02a-language.html",
    "href": "02a-language.html",
    "title": "Unit II: Language of Data",
    "section": "",
    "text": "Children learn the alphabet before tackling The Odyssey. Musicians become proficient in scales before playing in a symphony. And, chefs create world-class culinary experiences because they are experts at working with their ingredients. Similarly, working with statistical models benefits from understanding the language of data.\nThis first unit introduces the key components of any analysis — asking well-posed questions, collecting useful data, summarizing your data to tell a story. Once we are familiar with these ingredients, we can begin putting them together to address a range of interesting questions."
  },
  {
    "objectID": "02b-basics.html#overview-of-drawing-inference",
    "href": "02b-basics.html#overview-of-drawing-inference",
    "title": "3  The Statistical Process",
    "section": "3.1 Overview of Drawing Inference",
    "text": "3.1 Overview of Drawing Inference\nLet’s begin by taking a step back and considering the big picture of how data is turned into information. Every research question we pose, at its heart, is trying to characterize a population, the group of subjects of ultimate interest.\n\nDefinition 3.1 (Population) The collection of subjects we would like to say something about.\n\nIn the Organ Donation study (Example 3.1), the researchers would like to say something about Americans who are of the age to consent to organ donation; in particular, they would like to quantify how likely it is that someone from this group agrees to organ donation. Therefore, the population is all Americans who are of the age to consent to organ donation.\nIn general, the subjects (or units of observation) in a population need not be people; in some studies, the population could be a collection of screws, cell phones, sheet metal…whatever characterizes the objects from which we would like to obtain measurements. We use the phrase “like to” because in reality it is often impossible (or impractical) to observe the entire population. Instead, we make observations on a subset of the population; this smaller group is known as the sample.\n\nDefinition 3.2 (Sample) The collection of subjects for which we actually obtain measurements (data).\n\n\n\n\n\n\n\nNote\n\n\n\nSome readers may associate “subjects” with people; to avoid this confusion, you may prefer “unit of observation” to subject. In this text, we use subject to mean any unit on which observations could be taken.\n\n\nFor each subject within the sample, we obtain a collection of measurements forming our set of data. The goal of statistical modeling is to use the sample (the group we actually observe) to say something about the population of interest (the group we wish we had observed); this process is known as statistical inference (illustrated in Figure 3.2).\n\nDefinition 3.3 (Statistical Inference) The process of using a sample to characterize some aspect of the underlying population.\n\n\n\n\n\n\nFigure 3.2: Illustration of the statistical process, using a sample to characterize some aspect of the underlying population."
  },
  {
    "objectID": "02b-basics.html#anatomy-of-a-dataset",
    "href": "02b-basics.html#anatomy-of-a-dataset",
    "title": "3  The Statistical Process",
    "section": "3.2 Anatomy of a Dataset",
    "text": "3.2 Anatomy of a Dataset\nOnce we have our sample, we take measurements on each of the subjects within this sample. These measurements form the data. When we hear the word “data,” most of us envision a large spreadsheet. In reality, data can take on many forms — spreadsheets, images, text files, unstructured text from a social media feed, etc. Regardless of the form, all datasets contain information for each subject in the sample; this information, the various measurements, are called variables.\n\nDefinition 3.4 (Variable) A measurement, or category, describing some aspect of the subject.\n\nVariables come in one of two flavors. Categorical variables are those which denote a grouping to which the subject belongs. Examples include marital status, manufacturer, and experimental treatment group. Numeric variables are those which take on values for which ordinary arithmetic (e.g., addition and multiplication) makes sense. Examples include height, age of a product, and diameter. Note that sometimes numeric values are used to represent the levels of a categorical variable in a dataset; for example, 0 may indicate “No” and 1 may indicate “Yes” for a variable capturing whether a person is a registered organ donor. Therefore, just because a variable has a numeric value does not make it a numeric variable; the key here is that numeric variables are those for which arithmetic makes sense.\n\nDefinition 3.5 (Categorical Variable) Also called a “qualitative variable,” a measurement on a subject which denotes a grouping or categorization.\n\n\nDefinition 3.6 (Numeric Variable) Also called a “quantitative variable,” a measurement on a subject which takes on a numeric value and for which ordinary arithmetic makes sense.\n\nWhile it may be natural to think of a dataset as a spreadsheet, not all spreadsheets are created equal.\n\n\n\n\n\n\nCharacteristics of Well-Structured Data\n\n\n\nA well-structured dataset should adhere to the following characteristics:\n\nEach column contains a unique variable.\nEach record (row in the dataset) corresponds to a different observation of the variables.\nIf you have multiple datasets, they should include a column in the table that allows them to be linked (subject identifier).\n\n\n\nThese characteristics ensure the data is properly formatted for an analysis. Even unstructured data such as images or text files must be processed prior to performing a statistical analysis.\n\n\n\n\n\n\nWarning\n\n\n\nWe note the above description eliminates a common method of storing data in engineering and scientific disciplines — storing each sample in a different column.\n\n\nTo illustrate the above description, suppose we conduct a study comparing the lifetime (in hours) of two brands of batteries. We measure the lifetime of five batteries of Brand A and six of Brand B. It is common to see a dataset like that in Table 3.1; the problem here is that the first record of the dataset contains information on two different units of observation. We have the lifetime from a battery of Brand A in the same row as the lifetime from a battery of Brand B. This violates the second characteristic of datasets described above.\n\n\n\n\n\nTable 3.1: Example of a common data structure which does not correspond to the characteristics of well-structured data we recommend. The data is from a hypothetical study comparing battery lifetimes (hours).\n\n\nBrand A\nBrand B\n\n\n\n\n8.3\n8.4\n\n\n5.1\n8.6\n\n\n3.3\n3.8\n\n\n5.3\n4.1\n\n\n5.7\n4.5\n\n\n\n4.0\n\n\n\n\n\n\n\n\n\nIn order to adhere to the characteristics of well-structured data outlined above, we can reformat the data in Table 3.1 to that shown in Table 3.2. Here, each record represents a unique observation and each column is a different variable. We have also added a unique identifier.\n\n\n\n\n\nTable 3.2: Example of a well-structured dataset. The data is from a hypothetical study comparing battery lifetimes (hours).\n\n\nBattery\nBrand\nLifetime\n\n\n\n\n1\nA\n8.3\n\n\n2\nA\n5.1\n\n\n3\nA\n3.3\n\n\n4\nA\n5.3\n\n\n5\nA\n5.7\n\n\n6\nB\n8.4\n\n\n7\nB\n8.6\n\n\n8\nB\n3.8\n\n\n9\nB\n4.1\n\n\n10\nB\n4.5\n\n\n11\nB\n4.0\n\n\n\n\n\n\n\n\n\nIt may take some time to get used to storing data in this format, but it makes analysis easier and avoids time spent managing the data later."
  },
  {
    "objectID": "02b-basics.html#a-note-on-codebooks",
    "href": "02b-basics.html#a-note-on-codebooks",
    "title": "3  The Statistical Process",
    "section": "3.3 A Note on Codebooks",
    "text": "3.3 A Note on Codebooks\nA dataset on its own is meaningless if you cannot understand what the values represent. Before you access a dataset, you should always review any available codebooks.\n\nDefinition 3.7 (Codebook) Also called a “data dictionary,” these provide complete information regarding the variables contained within a dataset.\n\nSome codebooks are excellent, with detailed descriptions of how the variables were collected alongside appropriate units for the measurements. Other codebooks give only an indication of what each variable represents. Whenever you are working with previously collected data, reviewing a codebook is the first step; and, you should be prepared to revisit the codebook often throughout an analysis. When you are collecting your own dataset, constructing a codebook is essential for others to make use of your data.\n\n\n\n\nJohnson, Eric J, and Daniel Goldstein. 2003. “Do Defaults Save Lives?” Science 302: 1338–39.\n\n\nTintle, Nathan, Beth L Chance, A J Rossman, S Roy, T Swanson, and J VanderStoep. 2015. Introduction to Statistical Investigations. Wiley."
  },
  {
    "objectID": "02c-casedeepwater.html#footnotes",
    "href": "02c-casedeepwater.html#footnotes",
    "title": "4  Case Study: Health Effects of the Deepwater Horizon Oil Spill",
    "section": "",
    "text": "http://www.nytimes.com/2010/04/22/us/22rig.html?rref=collection%2Ftimestopic%2FOil%20Spills&action=click&contentCollection=timestopics&region=stream&module=stream_unit&version=search&contentPlacement=1&pgtype=collection↩︎"
  },
  {
    "objectID": "02d-questions.html#characterizing-a-variable",
    "href": "02d-questions.html#characterizing-a-variable",
    "title": "5  Asking the Right Questions",
    "section": "5.1 Characterizing a Variable",
    "text": "5.1 Characterizing a Variable\nRecall that the goal of statistical inference is to say something about the population; as a result, any question we ask should then be about on this larger group. The first step to constructing a well-posed question is then to identify the population of interest for the study. For the Deepwater Horizon Case Study, it is unlikely that we are only interested in these 54 observed volunteers assigned to wildlife cleaning. In reality, we probably want to say something about volunteers for any oil spill. The 54 volunteers in our dataset form the sample, a subset from all volunteers who clean wildlife following an oil spill. Our population of interest is comprised of all volunteers who clean wildlife following an oil spill.\n\n\n\n\n\n\nNote\n\n\n\nWhen identifying the population of interest for a research question you have, be specific! Suppose you are trying to estimate the average height of trees. Are you really interested in all trees? Or, are you interested in Maple trees within the city limits of Terre Haute, Indiana?\n\n\nSince we expect that the reaction to oil exposure — the primary variable of interest for this study, sometimes called the response — to vary from one individual to another, we cannot ask a question about the value of the reaction (whether they experienced symptoms or not). Instead, we want to characterize the distribution of the response.\n\nDefinition 5.2 (Response) The primary variable of interest within a study. This is the variable you would either like to explain or estimate.\n\n\nDefinition 5.3 (Distribution) The pattern of variability corresponding to a set of values.\n\nNotice that in this case, the response is a categorical variable; describing the distribution of such a variable is equivalent to describing how individuals are divided among the possible groups. With a finite number of observations, we could present the number of observations, the frequency, within each group. For example, of the 54 volunteers, 15 experienced adverse symptoms and 39 did not. This works well within the sample; however, as our population is infinitely large (all volunteers cleaning wildlife following an oil spill), reporting the frequencies is not appropriate. In this case, we report the fraction of observations, the relative frequency, falling within each group; this helps convey information about the distribution of this variable. That is, the relative frequencies give us a sense of which values of the variable are more or less common in the sample.\n\nDefinition 5.4 (Frequency) The number of observations in a sample falling into a particular group (level) defined by a categorical variable.\n\n\nDefinition 5.5 (Relative Frequency) Also called the “proportion,” the fraction of observations falling into a particular group (level) of a categorical variable.\n\nNumeric quantities, like the proportion, which summarize the distribution of a variable within the population are known as parameters.\n\nDefinition 5.6 (Parameter) Numeric quantity which summarizes the distribution of a variable within the population of interest. Generally denoted by Greek letters in statistical formulas.\n\nWhile the value of a variable may vary across the population, the parameter is a single fixed constant which summarizes the variable for that population. For example, the grade received on an exam varies from one student to another in a class; but, the average exam grade is a fixed number which summarizes the class as a whole. Well-posed questions can be constructed if we limit ourselves to questions about the parameter. The second step in constructing well-posed questions is then to identify the parameter of interest.\nThe questions we ask generally fall into one of two categories:\n\nEstimation: what proportion of volunteers who clean wildlife following an oil spill will experience adverse respiratory symptoms?\nHypothesis Testing: is it reasonable no more than 1 in 5 volunteers who clean wildlife following an oil spill will experience adverse respiratory symptoms; or, is there evidence more than 1 in 5 volunteers who clean wildlife following an oil spill will experience adverse respiratory symptoms?\n\n\nDefinition 5.7 (Estimation) Using the sample to approximate the value of a parameter from the underlying population.\n\n\nDefinition 5.8 (Hypothesis Testing) Using a sample to determine if the data is consistent with a working theory or if there is evidence to suggest the data is not consistent with the theory.\n\nSince we do not get to observe the population (we only see the sample), we cannot observe the value of the parameter. That is, we will never know the true proportion of volunteers who experience symptoms. However, we can determine what the data suggests about the population (that is what inference is all about).\n\n\n\n\n\n\nBig Idea\n\n\n\nParameters are unknown values and can never, in general, be known.\n\n\nIt turns out, the vast majority of research questions can be framed in terms of a parameter. This is the first of what we consider the Five Fundamental Ideas of Inference.\n\n\n\n\n\n\nFundamental Idea I\n\n\n\nA research question can often be framed in terms of a parameter that characterizes the population. Framing the question should then guide our analysis.\n\n\nWe now have a way of describing a well-posed question, a question which can be addressed using data. Well posed questions are about the population and can be framed in terms of a parameter which summarizes that population. We now describe how these questions are typically framed."
  },
  {
    "objectID": "02d-questions.html#framing-the-question",
    "href": "02d-questions.html#framing-the-question",
    "title": "5  Asking the Right Questions",
    "section": "5.2 Framing the Question",
    "text": "5.2 Framing the Question\nIn engineering and scientific applications, many questions fall under the second category of hypothesis testing, which is a form of model comparison in which data is collected to help the researcher choose between two competing theories for the parameter of interest. In this section, we consider the terminology surrounding specifying such questions.\nFor the Deepwater Horizon Case Study suppose we are interested in addressing the following question:\n\nIs there evidence that more than 1 in 5 volunteers who clean wildlife following an oil spill will develop adverse respiratory symptoms?\n\nThe question itself is about the population (all volunteers assigned to clean wildlife following an oil spill) and is centered on a parameter (the proportion who develop adverse respiratory symptoms). That is, this is a well-posed question that can be answered with appropriate data. The overall process for addressing these types of questions is similar to conducting a trial in a court of law. In the United States, a trial has the following essential steps:\n\nAssume the defendant is innocent.\nPresent evidence to establish guilt, to the contrary of innocence (prosecution’s responsibility).\nConsider the weight of the evidence presented (jury’s responsibility).\nMake a decision. If the evidence is “beyond a reasonable doubt,” the jury declares the defendant guilty; otherwise, the jury declares the defendant not guilty.\n\nThe process of conducting a hypothesis test has similar essential steps:\n\nAssume the opposite of what we want the data to show (develop a working theory).\nGather data and compare it to the proposed model from step (1).\nQuantify the likelihood of our data from step (2) under the proposed model.\nIf the likelihood is small, conclude the data is not consistent with the working model (there is evidence for what we want to show); otherwise, conclude the data is consistent with the working model (there is no evidence for what we want to show).\n\nNotice that a trial focuses not on proving guilt but on disproving innocence; similarly, in statistics, we are able to establish evidence against a specified theory. This is one of several subtle points in hypothesis testing. We will discuss these subtleties at various points throughout the text and revisit the overall concepts often. Here, we focus solely on that first step — developing a working theory that we want to disprove.\n\n\n\n\n\n\nNote\n\n\n\nThis process may seem counter-intuitive; it is natural to ask “why can’t we prove guilt directly?” However, when you disprove one statement, you are proving that statement’s opposite — a technique known in mathematics as “proof by contradiction.” So, our approach to proving a statement is to disprove all other possibilities. It is similar to the technique of the fictional detective Sherlock Holmes (Doyle 1890, pg. 92): “Eliminate all other factors, and the one which remains must be the truth.”\n\n\nConsider the above question for the Deepwater Horizon Case Study. We want to find evidence that the proportion experiencing adverse symptoms exceeds 0.20 (1 in 5). Therefore, we would like to disprove (or provide evidence against) the statement that the proportion experiencing adverse symptoms is no more than 0.20. This statement that we would like to disprove is known as the null hypothesis; the opposite of this statement, called the alternative hypothesis, captures what we as the researchers would like to establish.\n\nDefinition 5.9 (Null Hypothesis) The statement (or theory) about the parameter that we would like to disprove. This is denoted \\(H_0\\), read “H-naught” or “H-zero”.\n\n\nDefinition 5.10 (Alternative Hypothesis) The statement (or theory) about the parameter capturing what we would like to provide evidence for; this is the opposite of the null hypothesis. This is denoted \\(H_1\\) or \\(H_a\\), read “H-one” and “H-A” respectively.\n\nFor the Deepwater Horizon Case Study, we write:\n\n\\(H_0:\\) The proportion of volunteers assigned to clean wildlife following an oil spill who experience adverse respiratory symptoms is no more than 0.20.\n\\(H_1:\\) The proportion of volunteers assigned to clean wildlife following an oil spill who experience adverse respiratory symptoms exceeds 0.20.\n\nEach hypothesis is a well-posed statement (about a parameter characterizing the entire population), and the two statements are exactly opposite of one another meaning only one can be a true statement.\n\n\n\n\n\n\nNote\n\n\n\nWhen framing your questions, be sure your null hypothesis and alternative hypothesis are exact opposites of one another, and ensure the “equality” component always goes in the null hypothesis.\n\n\nWe can now collect data and determine if it is consistent with the null hypothesis (a statement similar to “not guilty”) or if the data provides evidence against the null hypothesis and in favor of the alternative (a statement similar to “guilty”).\n\n\n\n\n\n\nConsistent vs. Evidence\n\n\n\nThe term “consistent” and “reasonable” will be used interchangeably throughout the text; however, these terms differ substantially from the term “evidence,” particularly in the Frequentist perspective The data is said to be consistent with a statement if the data is aligned with that statement. In general, evidence is a stronger statement.\n\n\nOften these statements are written in a bit more of a mathematical structure in which a Greek letter is used to represent the parameter of interest. For example, we might write\n\nLet \\(\\theta\\) represent the proportion of volunteers (assigned to clean wildlife following an oil spill) who experience adverse respiratory symptoms.\n\\(H_0: \\theta \\leq 0.20\\)\n\\(H_1: \\theta &gt; 0.20\\)\n\nIn the above statements, \\(\\theta\\) represents the parameter of interest; the value 0.20 is known as the null value.\n\nDefinition 5.11 (Null Value) The value associated with the equality component of the null hypothesis; it forms the threshold or boundary between the hypotheses. Note: not all questions of interest require a null value be specified.\n\n\n\n\n\n\n\nBig Idea\n\n\n\nHypothesis testing is a form of statistical inference in which we quantify the evidence against a working theory (captured by the null hypothesis). We essentially argue that the data supports the alternative if it is not consistent with the working theory.\n\n\nThis section has focused on developing the null and alternative hypothesis when our question of interest is best characterized as one of comparing models or evaluating a particular statement. If our goal is estimation, a null and alternative hypothesis are not applicable. For example, we might have the following goal:\n\nEstimate the proportion of volunteers (assigned to clean wildlife following an oil spill) who experience adverse respiratory symptoms.\n\nIn this version of our research “question” there is no statement which needs to be evaluated. We are interested in estimation, not hypothesis testing and thus there is no corresponding null and alternative hypothesis.\n\n\n\n\n\n\nProcess for Framing a Question\n\n\n\nIn order to frame a research question, consider the following steps:\n\nIdentify the population of interest.\n\nIdentify the parameter(s) of interest.\nDetermine if you are interested in estimating the parameter(s) or quantifying the evidence against some working theory.\n\nIf you are interested in testing a working theory, make the null hypothesis the working theory and the alternative hypothesis the exact opposite statement (capturing what you want to provide evidence for).\n\n\n\n\n\n\n\nDoyle, Sir Arthur Conan. 1890. The Sign of the Four. Spencer Blackett."
  },
  {
    "objectID": "02e-data.html#what-makes-a-sample-reliable",
    "href": "02e-data.html#what-makes-a-sample-reliable",
    "title": "6  Gathering the Evidence (Data Collection)",
    "section": "6.1 What Makes a Sample Reliable",
    "text": "6.1 What Makes a Sample Reliable\nIf we are going to have some amount of faith in the statistical results we produce, we must have data in which we can place our trust. The Treachery of Images (Figure 6.2) is a canvas painting depicting a pipe, below which the artist wrote the French phrase “This is not a pipe.” Regarding the painting, the artist said\n\nThe famous pipe. How people reproached me for it! And yet, could you stuff my pipe? No, it’s just a representation, is it not? So if I had written on my picture “This is a pipe,” I’d have been lying!\n\n\n\n\n\n\nFigure 6.2: The Treachery of Images by René Magritte.\n\n\n\n\nJust as a painting is a representation of the object it depicts, so a sample should be a representation of the population under study. This is the primary requirement if we are to rely on the resulting data.\n\n\n\n\n\n\nBig Idea\n\n\n\nIn order for a statistical analysis to be reliable, the sample must be representative of the population under study.\n\n\nWe need to be careful to not get carried away in our expectations. What constitutes “representative” really depends on the question, just as an artist chooses their depiction based on how they want to represent the object. Let’s consider the following example.\n\nExample 6.1 (School Debt) In addition to a degree, college graduates also tend to leave with a large amount of debt due to college loans. In 2012, a graduate with a student loan had an average debt of $29,400; for graduates from private non-profit institutions, the average debt was $32,3001.\nSuppose we are interested in determining the average amount of debt in student loans carried by a graduating senior from Rose-Hulman Institute of Technology, a small private non-profit engineering school. There are many faculty at Rose-Hulman who choose to send their children to the institute. Suppose we were to ask 25 such faculty members who have a child that attended the institute to report the amount of student loans their children carried upon graduation from Rose-Hulman. Further, suppose we compile the responses and compute the average amount of debt. Using the data, we might report that based on our study, there is significant evidence the average debt carried by a graduate of Rose-Hulman is far below the $32,300 reported above (great news for this year’s graduating class)!\nWhy should we be hesitant to trust the results from our study?\n\nMany objections to statistical results stem from a distrust of whether the data (the sample) is really representative of the population of interest. Rose-Hulman, like many other universities, has a policy that the children of faculty may attend their university (assuming admittance) tuition-free. We would therefore expect their children to carry much less debt than the typical graduating senior. There is a mismatch between the group we would like to study and the data we have collected.\nThis example provides a nice backdrop for discussing what it means to be representative. First, let’s define our population; in this case, we are interested in graduating seniors from Rose-Hulman. The variable of interest is the amount of debt carried in student loans; the parameter of interest is then the average amount of debt in student loans carried by graduating seniors of Rose-Hulman. However, the sample consists of only graduating seniors of Rose-Hulman who have a parent employed by the institute.\nWith regard to grade point average, the students in our sample are probably similar to all graduating seniors; the starting salary of the students in our sample is probably similar to all graduating seniors; the fraction of mechanical engineering majors versus math majors is probably similar. So, in many regards the sample is representative of the population; however, it fails to be representative with regard to the variable of interest. This is our concern. The amount of debt carried by students in our sample is not representative of that debt carried by all graduating seniors from the university.\n\n\n\n\n\n\nNote\n\n\n\nWhen thinking about whether a sample is representative, focus your attention to the characteristics specific to your research question or with regard to how you intend to generalize the results.\n\n\nDoes that mean the sample we collected in Example 6.1 is useless? Yes and no. The sample collected cannot be used to answer our initial question of interest since it is not representative of our population. No statistical method can fix bad data; statistics adheres to the “garbage-in, garbage-out” phenomena. If the data is bad, no analysis will undo that. However, while the sample cannot be used to answer our initial question, it could be used to address a different question:\n\nWhat is the average amount of debt in student loans carried by graduating seniors from Rose-Hulman whose parent is a faculty member at the institute?\n\nFor this revised question, the sample may indeed be representative. If we are working with previously collected data, we must consider the population to which our results will generalize. That is, for what population is the given sample representative? If we are collecting our data, we need to be sure we collect data in such a way that the data is representative of our target population. Let’s first look at what not to do."
  },
  {
    "objectID": "02e-data.html#poor-methods-of-data-collection",
    "href": "02e-data.html#poor-methods-of-data-collection",
    "title": "6  Gathering the Evidence (Data Collection)",
    "section": "6.2 Poor Methods of Data Collection",
    "text": "6.2 Poor Methods of Data Collection\nExample 6.1 is an example of a “convenience sample,” when the subjects in the sample are chosen simply due to ease of collection. Examples include surveying students only in your sorority when you are interested in all students who are part of a sorority on campus; taking soil samples from only your city when you are interested in the soil for the entire state; and, obtaining measurements from only one brand of phone, because it was the only one you could afford on your budget, when you are interested in studying all cell phones on the market. A convenience sample is unlikely to be representative if there is a relationship between the ease of collection and the variable under study. This was true in the School Debt example; the relationship of a student to a faculty member, which is what increased the ease of collection, was directly related to the amount of debt they carried. As a result, the resulting sample was not representative of the population.\nWhen conducting a survey with human subjects, it is common to only illicit responses from volunteers. Such “volunteer samples” tend to draw in those with extreme opinions. Consider product ratings on Amazon. Individual ratings tend to cluster around 5’s and 1’s. This is because those customers who take time to submit a review (which is voluntary) tend to be those who are really thrilled with their product (and want to encourage others to purchase it) and those who are really disappointed with their purchase (and want to encourage others to avoid it). Such surveys often fail to capture those individuals in the population who have “middle of the road” opinions.\nWe could not possibly name all the poor methods for collecting a sample; but, poor methods all share something in common — it is much more likely the resulting sample is not representative. Failing to be representative results in biased estimates of the parameter.\n\nDefinition 6.1 (Bias) A set of measurements is said to be biased if they are consistently too high (or too low). Similarly, an estimate of a parameter is said to be biased if it is consistently too high (or too low).\n\nTo illustrate the concept of bias, consider shooting at a target as in Figure 6.3. We can consider the center of our target to be the parameter we would like to estimate within the population; in this case, some measure of center. The values in our sample (the strikes on the target) will vary around the parameter; while we do not expect any one value to hit the target precisely, a “representative” sample is one in which the values tend to be clustered about the parameter (unbiased). When the sample is not representative, the values in the sample tend to cluster off the mark (biased). Notice that to be unbiased, it may be that not a single value in the sample is perfect, but aggregated together, they point in the right direction. So, bias is not about an individual measurement being an “outlier,” (more on those in Chapter 7) but about consistently shooting in the wrong direction.\n\n\n\n\n\nFigure 6.3: Illustration of bias and precision.\n\n\n\n\n\n\n\n\n\n\nAccuracy vs. Precision\n\n\n\nThere is a difference between accuracy and precision. Generally, accuracy refers to location (and therefore relates to bias); we say a process is accurate when it is unbiased. Precision refers to the variability; data which is more precise has less variability.\n\n\n\n\n\n\n\n\nBig Idea\n\n\n\nBiased results are typically due to poor sampling methods that result in a sample which is not representative of the population of interest.\n\n\nThe catch (there is always a catch) is that we will never know with certainty if a sample is actually representative or not. In practice, we critically examine the method in which the sample was collected, and we use summaries of the sample to make educated decisions on whether to generalize the results. Better, however, is to employ methods of data collection that help to minimize the bias in the sample."
  },
  {
    "objectID": "02e-data.html#preferred-methods-of-sampling",
    "href": "02e-data.html#preferred-methods-of-sampling",
    "title": "6  Gathering the Evidence (Data Collection)",
    "section": "6.3 Preferred Methods of Sampling",
    "text": "6.3 Preferred Methods of Sampling\nNo method guarantees a perfectly representative sample; but, we can take measures to reduce or eliminate bias. A useful strategy is to employ randomization. This is summarized in our second Fundamental Idea.\n\n\n\n\n\n\nFundamental Idea II\n\n\n\nIf data is to be useful for making conclusions about the population, a process referred to as drawing inference, proper data collection is crucial. Randomization can play an important role ensuring a sample is representative and that inferential conclusions are appropriate.\n\n\nConsider the School Debt example (Example 6.1) again. Suppose instead of the data collection strategy described there, we had done the following:\n\nWe constructed a list of all graduating seniors from the institute. We placed the name of each student on an index card; then, we thoroughly shuffled the cards and chose the top 25 cards. For these 25 individuals, we recorded the amount of debt in student loans each carried.\n\nThis essentially describes using a lottery to select the sample. This popular method is known as taking a simple random sample. By conducting a lottery, we make it very unlikely that our sample consists of only students with a very small amount of student debt (as occurred when we used a convenience sample).\n\nDefinition 6.2 (Simple Random Sample) Often abbreviated SRS, this is a sample of size \\(n\\) such that every collection of size \\(n\\) is equally likely to be the resulting sample. This is equivalent to a lottery.\n\n\n\n\n\n\n\nNote\n\n\n\nIt is convention to use \\(n\\) to represent the sample size.\n\n\nThe primary benefit of a simple random sample is that it removes bias. More specifically, the process of simple random sampling is unbiased; that is, this process does not produce values which are consistently too high or low.\nThere are situations in which a simple random sample does not suffice. Again, consider our School Debt example. The Rose-Hulman student body is predominantly domestic, with only about 3% of the student body being international students. But, suppose we are interested in comparing the average debt carried between international and domestic students. It is very likely, by chance alone, that in a simple random sample of 25 students none will be international. Instead of a simple random sample, we might consider taking a sample of 13 domestic students and a sample of 12 international students; this is an example of a stratified random sample. This approach is useful when there is a natural grouping of interest within the population.\n\nDefinition 6.3 (Stratified Random Sample) A sample in which the population is first divided into groups, or strata, based on a characteristic of interest; a simple random sample is then taken within each group.\n\n\n\n\n\n\n\nWarning\n\n\n\nNote that a stratified random sample essentially results in a representative sample within each strata. However, the combined sample may not be representative of the population. If there is interest in using the sample in its entirety, instead of comparing the strata in some way, advanced statistical methodology is required. See texts on analyzing “complex survey design” for a more thorough discussion. Our text will not consider such cases.\n\n\nThere are countless sampling techniques used in practice. The two described above can be very useful starting points for developing a custom method suitable for a particular application. Their benefit stems from their use of randomization as it limits researcher influence on the composition of the sample and therefore minimizes bias.\nThis section is entitled “Preferred Methods” because while these methods are ideal, they are not always practical. Consider the Deepwater Horizon Case Study described in Chapter 4; conceptually, we can take a simple random sample of the volunteers for our study. However, as with any study involving human subjects, researchers would be required to obtain consent from each subject in the study. That is, any individual has the right to refuse to participate in the study. Therefore, it is unlikely that a simple random sample as described above could be obtained. While random selection is a nice tool, the goal is a sample which is representative of the population. While random sampling is helpful for accomplishing this, we may need to appeal to the composition of the sample itself to justify its use. Based on the characteristics of those willing to participate in the study, do we feel the study participants form a representative group of all volunteers? That is the essential question. This is often why studies report a table summarizing participant demographics such as age, gender, etc. It is also why it is extremely important for researchers to describe how observations were obtained so that readers may make the judgement for themselves whether the sample is representative."
  },
  {
    "objectID": "02e-data.html#footnotes",
    "href": "02e-data.html#footnotes",
    "title": "6  Gathering the Evidence (Data Collection)",
    "section": "",
    "text": "http://ticas.org/sites/default/files/pub_files/Debt_Facts_and_Sources.pdf↩︎"
  },
  {
    "objectID": "02f-summaries.html#characteristics-of-a-distribution-summarizing-a-single-variable",
    "href": "02f-summaries.html#characteristics-of-a-distribution-summarizing-a-single-variable",
    "title": "7  Presenting the Evidence (Summarizing Data)",
    "section": "7.1 Characteristics of a Distribution (Summarizing a Single Variable)",
    "text": "7.1 Characteristics of a Distribution (Summarizing a Single Variable)\nRemember that because of variability, the key to asking good questions is to not ask questions about individual values but to characterize the underlying distribution (see Definition 5.3). Therefore, characterizing the underlying distribution is also the key to a good visualization or numeric summary. For the Deepwater Horizon Case Study described in Chapter 4, the response (whether a volunteer experienced adverse respiratory symptoms) is categorical. As we stated previously, summarizing the distribution of a categorical variable reduces to showing the proportion of individual subjects that fall into each of the various groups defined by the categorical variable. Figure 7.1) displays a bar chart summarizing the rate of respiratory symptoms for volunteers cleaning wildlife.\n\n\n\n\n\nFigure 7.1: Frequency of adverse respiratory symptoms for volunteers cleaning wildlife following the Deepwater Horizon oil spill.\n\n\n\n\nIn general, it does not matter whether the frequency or the relative frequencies are reported; however, if the relative frequencies are plotted, some indication of the sample size should be provided with the figure, either as an annotation or within the caption. From the above graphic, we see that nearly 28% of volunteers assigned to wildlife experienced adverse respiratory symptoms; the graphic helps address our question, even if not definitively.\n\n\n\n\n\n\nNote\n\n\n\nWhen you are summarizing only categorical variables, a bar chart is sufficient. Statisticians tend to agree that bar charts are preferable to pie charts (see this whitepaper and this blog for further explanation).\n\n\nWhile a single type of graphic (bar charts) are helpful for looking at categorical data, summarizing the distribution of a numeric variable requires a bit more thought. Consider the following example.\n\nExample 7.1 (Paper Strength) While electronic records have become the predominant means of storing information, we do not yet live in a paperless society. Paper products are still used in a variety of applications ranging from printing reports and photography to packaging and bathroom tissue. In manufacturing paper for a particular application, the strength of the resulting paper product is a key characteristic.\nThere are several metrics for the strength of paper. A conventional metric for assessing the inherent (not dependent upon the physical characteristics, such as the weight of the paper, which might have an effect) strength of paper is the breaking length. This is the length of a paper strip, if suspended vertically from one end, that would break under its own weight. Typically reported in kilometers, the breaking length is computed from other common measurements. For more information on paper strength measurements and standards, see the following website: http://www.paperonweb.com\nA study was conducted at the University of Toronto to investigate the relationship between pulp fiber properties and the resulting paper properties (Lee 1992). The breaking length was obtained for each of the 62 paper specimens, the first 5 measurements of which are shown in Table 7.1. The complete dataset is available online at the following website: https://vincentarelbundock.github.io/Rdatasets/doc/robustbase/pulpfiber.html\nWhile there are several questions one might ask with the available data, here we are primarily interested in characterizing the breaking length of these paper specimens.\n\n\n\n\n\n\nTable 7.1: Breaking length (km) for first 5 specimens in the Paper Strength study.\n\n\nSpecimen\nBreaking Length\n\n\n\n\n1\n21.312\n\n\n2\n21.206\n\n\n3\n20.709\n\n\n4\n19.542\n\n\n5\n20.449\n\n\n\n\n\n\n\n\n\nFigure 7.2 presents the breaking length for all 62 paper specimens in the sample through a dot plot in which the breaking length for each observed specimen is represented on a number line using a single dot.\n\n\n\n\n\nFigure 7.2: Breaking Length (km) for 62 paper specimens.\n\n\n\n\nWith any graphic, we tend to be drawn to three components:\n\nwhere the values tend to be,\nhow tightly the values tend to be clustered there, and\nthe way the values tend to cluster.\n\nNotice that about half of the paper specimens in the sample had a breaking length longer than 21.26 km. Only about 25% of paper specimens had a breaking length less than 19.33 km. These are measures of location. In particular, these are known as percentiles, of which the median, first quartile and third quartile are commonly used examples.\n\nDefinition 7.1 (Percentile) The \\(k\\)-th percentile is the value \\(q\\) such that \\(k\\)% of the values in the distribution are less than or equal to \\(q\\). For example,\n\n25% of values in a distribution are less than or equal to the 25-th percentile (known as the “first quartile” and denoted \\(Q_1\\)).\n50% of values in a distribution are less than or equal to the 50-th percentile (known as the “median”).\n75% of values in a distribution are less than or equal to the 75-th percentile (known as the “third quartile” and denoted \\(Q_3\\)).\n\n\nThe average is also a common measure of location. The breaking length of a paper specimen is 21.72 km, on average. In this case, the average breaking length and median breaking length are very close; this need not be the case. The average is not describing the “center” of the data in the same way as the median; they capture different properties.\n\nDefinition 7.2 (Average) Also known as the “mean,” this measure of location represents the balance point for the distribution. If \\(x_i\\) represents the \\(i\\)-th value of the variable \\(x\\) in the sample, the sample mean is typically denoted by \\(\\bar{x}\\).\nFor a sample of size \\(n\\), it is computed by \\[\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i.\\]\nWhen referencing the average for a population, the mean is also called the “Expected Value,” and is often denoted by \\(\\mu\\).\n\nClearly, the breaking length is not equivalent for all paper specimens; that is, there is variability in the measurements. Measures of spread quantify the variability of values within a distribution. Common examples include the standard deviation (related to variance) and interquartile range. For the Paper Strength example, the breaking length varies with a standard deviation of 2.88 km; the interquartile range for the breaking length is 5.2 km.\nThe standard deviation is often reported more often than the variance since it is on the same scale as the original data; however, as we will see later, the variance is useful from a mathematical perspective for derivations. Neither of these values has a natural interpretation; instead, larger values of these measures simply indicate a higher degree of variability in the data.\n\nDefinition 7.3 (Variance) A measure of spread, this roughly captures the average distance values in the distribution are from the mean.\nFor a sample of size \\(n\\), it is computed by \\[s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} \\left(x_i - \\bar{x}\\right)^2\\]\nwhere \\(\\bar{x}\\) is the sample mean and \\(x_i\\) is the \\(i\\)-th value in the sample. The division by \\(n-1\\) instead of \\(n\\) removes bias in the statistic.\nThe symbol \\(\\sigma^2\\) is often used to denote the variance in the population.\n\n\nDefinition 7.4 (Standard Deviation) A measure of spread, this is the square root of the variance.\n\n\nDefinition 7.5 (Interquartile Range) Often abbreviated as IQR, this is the distance between the first and third quartiles. This measure of spread indicates the range over which the middle 50% of the data is spread.\n\n\n\n\n\n\n\nNote\n\n\n\nThe IQR is often incorrectly reported as the interval \\(\\left(Q_1, Q_3\\right)\\). The IQR is actually the width of this interval, not the interval itself.\n\n\nThe measures we have discussed so far are illustrated in Figure 7.3. While some authors suggest the summaries you choose to report depend on the shape of the distribution, we argue that it is best to report the values that align with the question of interest. It is the question that should be shaped by the beliefs about the underlying distribution.\n\n\n\n\n\nFigure 7.3: Illustration of measures of location and spread for a distribution of values.\n\n\n\n\nFinally, consider the shape of the distribution of breaking length we have observed. The breaking length tends to be clustered in two locations; we call this bimodal (each mode is a “hump” in the distribution). Other terms used to describe the shape of a distribution are symmetric and skewed. Symmetry refers to cutting a distribution in half (at the median) and the lower half being a mirror image of the upper half; skewed distributions are those which are not symmetric.\nObserve that the dot plot above gives us some idea of the location, spread, and shape of the distribution, in a way that the table of values could not. This makes it a useful graphic as it is characterizing the distribution of the sample we have observed. This is one of the four components of what we call the Distributional Quartet.\n\nDefinition 7.6 (Distribution of the Sample) The pattern of variability in the observed values of a variable.\n\nWhen the sample is not large, a dot plot is reasonable. Other common visualizations for a single numeric variable include:\n\njitter plot: similar to a dot plot, each value observed is represented by a dot; the dots are “jittered” (shifted randomly) in order to avoid over-plotting when many subjects share the same value of the response.\nbox plot: a visual depiction of five key percentiles; the plot includes the minimum, first quartile, median, third quartile, and maximum value observed. The quartiles are connected with a box, the median cuts the box into two components. Occasionally, outliers are denoted on the graphic.\nhistogram: can be thought of as a grouped dot plot in which subjects are “binned” into groups of similar values. The height of each bin represents the number of subjects falling into that bin.\ndensity plot: a smoothed histogram in which the y-axis has been standardized so that the area under the curve has value 1. The y-axis is not interpretable directly, but higher values along the y-axis indicate that the corresponding values on along the x-axis are more likely to occur.\n\n\nDefinition 7.7 (Outlier) An individual observation which is so extreme, relative to the rest of the observations in the sample, that it does not appear to conform to the same distribution.\n\nTo illustrate these graphics, the breaking length for the Paper Strength example is summarized using various methods in Figure 7.4. The latter three visualizations are more helpful when the dataset is very large and plotting the raw values actually hides the distribution. There is no right or wrong graphic; it is about choosing the graphic which addresses the question and adequately portrays the distribution.\n\n\n\n\n\nFigure 7.4: Four graphical summaries of the breaking length for the Paper Strength example.\n\n\n\n\nThe numeric summaries of a distribution are known as statistics. While parameters characterize a variable at the population level, statistics characterize a variable at the sample level.\n\nDefinition 7.8 (Statistic) Numeric quantity which summarizes the distribution of a variable within a sample.\n\nWhy would we compute numerical summaries in the sample if we are interested in the population? Remember the goal of this discipline is to use the sample to say something about the underlying population. As long as the sample is representative, the distribution of the sample should reflect the distribution of the population; therefore, summaries of the sample should be close to the analogous summaries of the population (statistics estimate their corresponding parameters). Now we see the real importance of having a representative sample; it allows us to say that what we observe in the sample is a good proxy for what is happening in the population.\n\nDefinition 7.9 (Distribution of the Population) The pattern of variability in values of a variable at the population level. Generally, this is impossible to know, but we might model it.\n\nStatistics being a proxy for the corresponding parameter implies the mean in the sample should approximate (estimate) the mean in the population; the standard deviation of the sample should estimate the standard deviation in the population; and, the shape of the sample should approximate the shape of the population, etc. The sample is acting as a representation in all possible ways of the population.\n\n\n\n\n\n\nBig Idea\n\n\n\nA representative sample reflects the population; therefore, we can use statistics as estimates of the population parameters.\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotation in any discipline is both important and somewhat arbitrary. We can choose any symbol we want to represent the sample mean. However, it is convention that we never use \\(\\bar{x}\\) to represent a parameter like the mean of the population. The symbol \\(\\bar{x}\\) (or \\(\\bar{y}\\), etc.) represents observed values being averaged together. Since the values are observed, we must be talking about the sample, and therefore \\(\\bar{x}\\) represents a statistic. A similar statement could be made for \\(s^2\\) (sample variance) compared to \\(\\sigma^2\\) (population variance).\nAgain, in reality, the symbols themselves are not important. The importance is on their representation. Statistics are observed while parameters are not."
  },
  {
    "objectID": "02f-summaries.html#summarizing-relationships",
    "href": "02f-summaries.html#summarizing-relationships",
    "title": "7  Presenting the Evidence (Summarizing Data)",
    "section": "7.2 Summarizing Relationships",
    "text": "7.2 Summarizing Relationships\nThe summaries discussed above are nice for examining a single variable. In general, however, research questions of interest typically involve the relationship between two or more variables. Most graphics are two-dimensional (though 3-dimensional graphics and even virtual reality are being utilized now); therefore, summarizing a rich set of relationships may require the use of both axes as well as color, shape, size, and even multiple plots in order to tell the right story. We will explore these various features in upcoming units of the text. Here, we focus on the need to tell a story that answers the question of interest instead of getting lost in making a graphic. Consider the following question from the Deepwater Horizon Case Study described in #sec-caseDeepwater:\n\nWhat is the increased risk of developing adverse respiratory symptoms for volunteers cleaning wildlife compared to those volunteers who do not have direct exposure to oil?\n\nConsider the graphic in Figure 7.5; this is not a useful graphic. While it compares the number of volunteers with symptoms in each group, we cannot adequately address the question because the research question involves comparing the rates for the two groups; that is, we are lacking a sense of how many volunteers in each group did not report symptoms.\n\n\n\n\n\nFigure 7.5: Illustration of a poor graphic; the graphic does not give us a sense of the rate within each group at which volunteers reported symptoms.\n\n\n\n\nInstead, Figure 7.6 compares the rates within each group. Note that the graphic is still reporting frequency along the y-axis; that was not the primary problem with Figure 7.5. However, by reporting frequencies for both those with respiratory symptoms and those without, we get a sense of the relative frequency with which respiratory symptoms occur.\n\n\n\n\n\nFigure 7.6: Comparison of the rate of adverse respiratory symptoms among volunteers assigned to different tasks.\n\n\n\n\nFrom the graphic, it becomes clear that within the sample a higher fraction of volunteers cleaning wildlife experienced adverse symptoms compared with those without oil exposure. In fact, volunteers cleaning wildlife were 1.79 times more likely to experience adverse respiratory symptoms.\nThe key to a good summary is understanding the question of interest and addressing this question through a useful characterization of the variability.\n\n\n\n\nLee, J. 1992. “Relationships Between Properties of Pulp-Fibre and Paper.”"
  },
  {
    "objectID": "03a-fundamentals.html",
    "href": "03a-fundamentals.html",
    "title": "Unit III: Fundamentals of Bayesian Inference",
    "section": "",
    "text": "Once we have data, we want to use it to say something about the underlying population. This is the process of “drawing inference.” There are two large paradigms in the statistical community for defining the framework under which inference occurs. This unit introduces the fundamental components of the Bayesian paradigm. We focus on the mechanics in scenarios for which the process is analytically tractable by hand. The remainder of the text merely illustrates these principles in more complex scenarios."
  },
  {
    "objectID": "03b-bayesrule.html#tenants-of-the-bayesian-approach-to-inference",
    "href": "03b-bayesrule.html#tenants-of-the-bayesian-approach-to-inference",
    "title": "8  Bayes Rule",
    "section": "8.1 Tenants of the Bayesian Approach to Inference",
    "text": "8.1 Tenants of the Bayesian Approach to Inference\nThe above results on their own may be interesting in a probability course. However, we are interested primarily in their application when we have observed a sample from a population which is not fully known. Before we delve into the mechanics, let’s pause to reflect on how we intend to apply these results.\nRecall that statistics is about using a sample to make inference on the population. Specifically, we will posit a model for the distribution of a response within the population; however, that model will be specified only up to some unknown parameters. There are two general statistical paradigms for performing inference, and these stem from two different questions we might ask:\n\nGiven a hypothesis about the parameters is true, how likely is the observed data?\nGiven the observed data, how likely is a particular hypothesis about the parameters?\n\nThe first question results in the classical Frequentist perspective (most statistical courses) and a frequentist interpretation of probability. The second results in the Bayesian perspective and a subjective interpretation of probability.\nPrior to collecting data, we might have some belief about the the unknown parameters that govern our model for the population. Then, we collect a sample from the population; since this data is representative of the population, it must contain information about those parameters. Therefore, we want to update our belief about the parameters in light of this data. That is the Bayesian process in a nutshell.\n\n\n\n\n\n\nTenants of the Bayesian Approach to Inference\n\n\n\nEvery analysis in this course is built on the following three tenants:\n\nThe Bayesian approach takes into account prior knowledge when making inference.\nThe Bayesian approach uses probability models to quantify uncertainty in the parameters.\nThe Bayesian approach updates our prior knowledge conditional on the observed data.\n\n\n\nThroughout, we will rely on a subjective view of probability. That is, probability characterizes how sure you are of something. So, it does not make sense to say “how likely is it to rain tomorrow?” There is no one probability that answers this question. Instead, we will always have (even if not explicitly stated) a “how likely do you believe…’’ element to our question. That is, we are always bringing in our personal (subjective) opinion. This can be very uncomfortable for some of us — the idea of there not being a single”right” answer. We will save this discussion for a future chapter."
  },
  {
    "objectID": "03c-modelingsamples.html#independent-and-identically-distributed",
    "href": "03c-modelingsamples.html#independent-and-identically-distributed",
    "title": "9  Modeling Samples",
    "section": "9.1 Independent and Identically Distributed",
    "text": "9.1 Independent and Identically Distributed\nThe above discussion, while accurate, is unrealistic in that it begins with a completely formed likelihood. In reality, we must posit models which correspond to the data generating process. Positing a model for the distribution of an individual observation (element of \\(\\mathbf{X}\\)) often means choosing from among well-known named probability models. Regardless of whether a named model is used or a custom model constructed, the process always involves examining the context to determine an appropriate structure — the shape and support — of the distribution. We then allow the parameters of this distribution to remain unknown. This is where we turn from probability to statistics — suddenly, our models are only partly known, and there are some aspects (the parameters governing the behavior of the model) which are unknown. We will use data to make some statements about these parameters to address questions of interest which are framed in terms of these parameters.\nInstead of trying to model the joint distribution of the observed data directly, we often model the variability in the individual observations. We then place additional conditions on the relationship between the observations in order to develop the joint distribution. One of the most popular conditions is that of independence.\n\nDefinition 9.4 (Independence) Random variables \\(X_1, X_2, \\dotsc, X_n\\) are said to be mutually independent (or just “independent”) if and only if\n\\[Pr\\left(X_1 \\in A_1, X_2 \\in A_2, \\dotsb, X_n \\in A_n\\right) = \\prod_{i=1}^{n} Pr\\left(X_i \\in A_i\\right),\\]\nwhere \\(A_1, A_2, \\dotsc, A_n\\) are arbitrary sets. Perhaps more helpful, \\(X_1, X_2, \\dotsc, X_n\\) are said to be mutually independent if and only if\n\\[f_{\\mathbf{X}}(\\mathbf{x}) = \\prod_{i=1}^{n} f_{X_i}\\left(x_i\\right).\\]\n\n\n\n\n\n\n\nNote\n\n\n\nFor those not familiar, \\(\\prod_{i=1}^n a_i\\) is the product operator. It is analogous to \\(\\sum_{i=1}^{n} a_i\\), but uses products instead of sums.\n\n\nEssentially, a random variable \\(X\\) is said to be independent of \\(Y\\) if the likelihood that \\(X\\) takes a particular value is the same regardless of the value \\(Y\\) takes.\nAssuming independence allows us to easily construct joint densities by taking the product of the marginal density for each observation. Independence is a powerful condition when constructing likelihoods. However, it cannot be blindly enforced; we should take caution when assuming independence. This requires considering the method in which the data was obtained to determine if it is reasonable that the value of one observation does not affect the likelihood of any other observation.\nIdeally, when we take a sample, each observation is representative of the same process. This is what allows us to use all observations in a sample in order to make inference — we believe that each observation is able to contribute information about the unknown parameter. Believing that each observation is representative of the same process is essentially assume that each corresponding random variable (prior to observing the data) has the same distribution.\n\nDefinition 9.5 (Identically Distributed) We say that random variables \\(X\\) and \\(Y\\) are identically distributed if \\(F_X(u) = F_Y(u)\\) for all \\(u\\). This is equivalent to saying the two random variables have the same density function \\(f\\).\n\n\n\n\n\n\n\nWarning\n\n\n\nLet \\(X\\) and \\(Y\\) be identically distributed random variables. This does not mean that \\(X = Y\\). “Identically distributed” says the two random variables have the same distribution, not the same value. As a result, they share the same mean, variance, etc.\n\n\nWhen the observations in our sample are both independent and identically distributed, we say we have a “random sample.”\n\nDefinition 9.6 (Random Sample) A random sample of size \\(n\\) refers to a collection of \\(n\\) random variables \\(X_1, X_2, \\dotsc, X_n\\) such that the random variables are mutually independent, and the distribution of each random variable is identical.\n\n\nExample 9.1 (Delivery by Cesarean Section (C-section)) It is sometimes necessary for babies to be delivered through a surgical procedure known as a Cesarean Section (C-section). As surgical procedures carry risk, a C-section is typically performed when a vaginal delivery would place the infant or mother in undue risk of complications. Suppose we are interested in characterizing the hospital experiences of mothers who have undergone a C-section at Union Hospital in Terre Haute, Indiana.\nFor this community health project, we would like to survey \\(n = 15\\) mothers who have undergone a C-section. Of course, not every delivery is a C-section; let \\(X_i\\) represent the number of vaginal deliveries that occur between the \\(i\\)-th C-section and the previous C-section we observe.\nSuppose we are willing to believe that (absent any additional information on the pregnancy) each patient in the labor and delivery ward has the same probability of undergoing a C-section; further, whether one patient undergoes a C-section is independent of any other patient undergoing a C-section. Develop a model for the likelihood of the data to be observed.\n\n\nSolution. We begin by thinking about the specific context. Note, for example, that \\(X_i\\) is a non-negative integer; that is, \\(X_i \\in \\{0, 1, 2, \\dotsc \\}\\). Let \\(\\theta\\) represent the probability that a patient undergoes a C-section (and therefore \\(1 - \\theta\\) represents the probability of a vaginal birth). Since we believe the method of delivery for one patient is independent of the method of delivery for all other patients, and that each probability of a delivery by C-section is the same for each patient, then it is reasonable to state that\n\\[Pr\\left(X_i = x\\right) = \\theta (1 - \\theta)^x \\qquad x = 0, 1, 2, \\dotsc.\\]\nThat is, \\(X_i\\) follows a Geometric distribution with parameter \\(\\theta\\). This distribution captures the idea that \\(x\\) vaginal deliveries occur (each with probability \\(1 - \\theta\\)) before we see the \\(i\\)-th C-section (which occurs with probability \\(\\theta\\)).\nFurther, since each birth is independent, we can consider \\(X_1, X_2, \\dotsc, X_n\\) to be a random sample. Letting \\(\\mathbf{X} = \\left(X_1, X_2, \\dotsc, X_n\\right)^\\top\\) be the random vector of observations, the likelihood is given by\n\\[\n\\begin{aligned}\n  f(\\mathbf{x} \\mid \\theta)\n    &= \\prod_{i=1}^{n} f_{X_i}\\left(x_i \\mid \\theta\\right) \\\\\n    &= \\prod_{i=1}^{n} \\theta (1 - \\theta)^{x_i} \\\\\n    &= \\theta^n (1 - \\theta)^{\\sum_{i=1}^{n} x_i} \\\\\n    &= \\theta^n (1 - \\theta)^{n\\bar{x}}.\n\\end{aligned}\n\\tag{9.1}\\]\nNote that line (1) makes use of the independence to say the likelihood is the product of the marginal density functions of each observation. Line (2) makes use of that each observation is identically distributed; this means that each observation has the same functional family for the density and is governed by the same parameter. This still allows \\(x_i\\) to differ from \\(x_j\\), but the distribution is the same. Line (3) brings the product through the expression, with the product of exponentials with the same base resulting in adding the exponents. Line (4) simplifies the expression; for notational simplicity, we prefer \\(n\\bar{x}\\) to \\(\\sum_{i=1}^{n} x_i\\), though the two are equivalent.\nWe also note that the likelihood expressly acknowledges the dependence on the parameter \\(\\theta\\) by using \\(f(\\mathbf{x} \\mid \\theta)\\) instead of just \\(f(\\mathbf{x})\\).\n\n\n\n\n\n\n\nNote\n\n\n\nIt is helpful to be in the habit acknowledging the dependence of the likelihood on the parameter.\n\n\n\n\n\n\n\n\nBig Idea\n\n\n\nBy placing conditions on how the data is generated, we are able to model the joint distribution of the responses. This is sometimes referred to as the likelihood of the unknown parameters; we also refer to it as the model for the data generating process, as it explains the variability in the observed data."
  },
  {
    "objectID": "03d-priors.html",
    "href": "03d-priors.html",
    "title": "10  Quantifying/Modeling Prior Information",
    "section": "",
    "text": "Data contributes information to, and therefore impacts, our beliefs. But, prior to beginning a study, we generally have some established beliefs — based on previous studies, expert opinions, personal experience, etc. The Bayesian framework explicitly incorporates these established prior beliefs in the analysis. The beliefs just need to be quantified.\n\n\n\n\n\n\nBig Idea\n\n\n\nThe Bayesian framework encodes any uncertainty through probability distributions.\n\n\nBefore we examine the technical aspects of quantifying the beliefs we have prior to the start of a study, we need to consider how this fits into the larger scope of performing inference. Recall that our primary aim is to make some statement about the population using a corresponding sample. Further, we have some model for the data generating process up to some unknown parameters (this was the focus of the previous chapter). When we collect data, it provides additional information about these unknown parameters. That is, the data impacts the beliefs we have about these unknown parameters. Similarly, any beliefs we have entering the study must relate to these unknown parameters.\nPrior to beginning the study, we generally have some notion about the parameters that govern the data generating process. What is a typical GPA for a college student? How much does a member of the mathematics faculty earn each year, on average? While we use data to inform these beliefs (topic of the next chapter), even without data available, we have some idea of where we think the answer lies. Bayesians encode these beliefs into probability distributions. The beliefs we have prior to seeing the data are described by a “prior” distribution, since the beliefs were those we had a priori.\n\nDefinition 10.1 (Prior Distribution) A distribution quantifying our beliefs about uncertainty in the parameter(s) of the underlying sampling distribution prior to observing any data. This is often denoted by \\(\\pi(\\boldsymbol{\\theta})\\) where \\(\\boldsymbol{\\theta}\\) is the parameter vector.\n\nThis relies on a subjective view of probability.\nAs prior beliefs are subjective, there is no “one” prior, but each individual may have a unique prior.\n\n\nConstructing a prior distribution is not all that different from constructing the likelihood. There are several aspects involved, but it is all about understanding the structure of the beliefs.\n\n\n\n\n\n\nTips for Constructing a Prior\n\n\n\nThe following considerations should be kept in mind when constructing the prior distribution.\n\nIdentify the unknown parameter(s). That is, on what unknown value(s) does the likelihood (model for the data generating process) depend?\nDescribe the support for the parameter(s).\nUse clear statements about our beliefs of the parameters to determine the hyperparameters.\n\n\n\n\nDefinition 10.2 (Hyperparameter) A constant term of a prior distribution that characterizes the family we are considering.\n\n\n\n\n\n\n\nNote\n\n\n\nIt is sometimes said that a hyperparameter is a “parameter” of the prior distribution. You want to distinguish between “parameters” (constant terms that characterize the likelihood), which are unknown, and hyperparameters (constant terms that characterize a prior), which are known values chosen such that the prior distribution reflects our prior beliefs.\n\n\n\nExample 10.1 (A Naive Classification of College Students) Rose-Hulman Institute of Technology (RHIT) and Indiana State University (ISU) are located in Terre Haute, IN. While both colleges cater to undergraduate students, they have different profiles. For the 2021-2022 academic year, Indiana State University reported having 5738 full-time undergraduate students, 3232 (56.3%) of which identified as female. For the same year, Rose-Hulman reported having 2058 full-time undergraduate students, 507 (24.6%) of which identified as female.\nSuppose an individual sees a group of 10 college students hanging out at a coffee shop in Terre Haute; they are interested in determining which college the students attend. If the students attend ISU, then we would expect 56.3% to identify as female; if the students attend RHIT, then we would expect 24.6% to identify as female. However, since the coffee shop is located in downtown Terre Haute (which is near the ISU campus), the individual believes there is a 60% chance the students are from ISU.\n\nNotice that in this example, no data has been collected — we have no information on how the students within the group identify. The belief about how likely the students are to attend ISU is stated prior to seeing any data, and this belief can therefore be used to form a prior distribution. Again, notice the use of “a” when describing the prior instead of “the.” While this prior will reflect the beliefs of this particular individual, if someone had a different set of beliefs, we would arrive at a different prior.\nLet \\(Y\\) represent the number of students who identify as female. Then, \\(Y \\sim Bin(10, \\theta)\\), where \\(\\theta\\) is the probability that a student identifies as female. Notice we are modeling the data that we have not yet collected; that is, this represents the probability model for the likelihood. This likelihood depends on the unknown parameter \\(\\theta\\), which represents the probability a randomly selected student identifies as female. This is the first step in constructing a prior — constructing the likelihood and identifying any unknown parameters.\nNow, we describe the support for \\(\\theta\\). Ordinarily, we might think that \\(\\theta\\) could be any value between 0 and 1 since it represents a probability. However, notice that the context we have here suggests there are really only two possible values: either \\(\\theta = 0.563\\), representing the gender diversity of ISU students; or \\(\\theta = 0.246\\) representing the gender diversity of RHIT students. Therefore, the support of \\(\\theta\\) in this particular context is the set \\(\\{0.563, 0.246\\}\\). Since the support is countable, we will need a discrete distribution for \\(\\theta\\).\nWe are now ready to write a distribution that captures the individual’s beliefs prior to observing the data. In this example, the individual is 60% sure the students are from ISU; we write this as\n\\[\nPr(\\theta = u) = \\begin{cases}\n  0.4 & u = 0.246 \\\\\n  0.6 & u = 0.563. \\end{cases}\n\\tag{10.1}\\]\nThis says the individual is 60% sure that \\(\\theta\\) takes the value 0.563, and they are 40% sure that \\(\\theta\\) takes the value 0.246; the 0.6 is the hyperparameter that governs this distribution. It was chosen to correspond with the prior beliefs stated by the individual.\nThis is a completely acceptable way of writing the prior distribution. However, as we will later see, the prior distribution is much easier to work with when written in a compact form instead of piecewise notation. For example, we can rewrite Equation 10.1 as\n\\[\\pi(\\theta) = 0.4\\delta(\\theta - 0.246) + 0.6\\delta(\\theta - 0.563) \\tag{10.2}\\]\nwhere \\(\\delta(x)\\) is the Dirac delta function.\n\nDefinition 10.3 (Dirac Delta Function) The Dirac delta function is the function (not in a rigorous sense) \\(\\delta\\) such that\n\\[\\int_{-\\infty}^{\\infty} \\delta(x) dx = 1\\]\nand\n\\[\\int_{-\\infty}^{\\infty} f(x) \\delta(x) dx = f(0)\\]\nfor any real-valued function \\(f\\).\nThe Dirac delta function allows us to describe a discrete distribution, which places mass at a single point, as a continuous function on the real line.\n\nThe above example offers a rather simplistic view of constructing a prior. In practice, nearly every problem will involve some numerical computation at some point. Rarely, perhaps never, are we simply provided with a complete prior distribution and asked to perform an analysis. Generally, we must convert statements from researchers into some type of distribution.\n\nExample 10.2 (C-Section Deliveries Continued) Example 9.1 introduced a study, a component of which includes estimating the probability of a mother undergoing a C-section delivery at a particular hospital.\nWhile we do not know the probability of a C-section, we do have some external information (even before collecting data). Specifically, the March of Dimes has reported that in 2021, 30.4% of live births in Indiana were C-section deliveries. Suppose we have the following beliefs:\n\nOn average, the rate of C-sections at Union Hospital equals the rate of C-sections in the state of Indiana.\nWe feel fairly confident (90% sure) the rate of C-sections at Union Hospital is between 20% and 40%.\n\nDevelop a suitable prior distribution which captures these beliefs.\n\n\nSolution. As is typical, the prior beliefs that have been provided to us are limited; that is, they do not come pre-packaged in the form of a prior distribution. So, we must develop a prior distribution that aligns with these beliefs. Let’s begin by converting the above beliefs into statements about the unknown parameter.\nThe first belief specifies the average value of the parameter; specifically,\n\\[E(\\theta) = 0.304.\\]\nThe second belief conveys information about where the parameter is located; specifically,\n\\[Pr(0.2 &lt; \\theta &lt; 0.4) = \\int_{0.2}^{0.4} \\pi(\\theta) d\\theta = 0.9.\\]\nNotice the use of the subjective interpretation of probability in capturing this belief. Unfortunately, these two statements alone do not define a unique distribution; this is extremely common as discipline experts do not typically think in probability distributions. Therefore, there is no one unique prior distribution (even for this set of beliefs); instead, we must make some decisions.\nNotice that the unknown parameter \\(\\theta\\) is a probability; therefore, we know that \\(\\pi(\\theta)\\) must have a support on the interval \\((0, 1)\\) since those are the only possible values for \\(\\theta\\). Without further guidance, it seems reasonable to select a common distributional family that shares this support; we suggest the Beta distribution. Therefore, we suggest that \\(\\theta \\sim Beta(a, b)\\). We now must select the values of the hyperparameters \\(a\\) and \\(b\\) so that the prior distribution captures the above statements. That is, we want to choose the hyperparameters to satisfy the following system of equations:\n\\[\n\\begin{aligned}\n  0.304 &= E(\\theta) = \\frac{a}{a+b} \\\\\n  0.90 &= \\int_{0.2}^{0.4} \\frac{\\Gamma(a + b)}{\\Gamma(a)\\Gamma(b)} \\theta^{a-1} (1 - \\theta)^{b-1} d\\theta.\n\\end{aligned}\n\\]\nAs we have two equations and two unknowns, this system can be solved (numerically). Solving this system results in \\(a = 17\\) and \\(b = 39\\) (approximately). Note that the choice of hyperparameters need not carry a lot of precision; these values get us extremely close to the prior beliefs. Therefore, we propose representing our prior beliefs with the prior distribution\n\\[\\pi(\\theta) = \\frac{\\Gamma(17 + 39)}{\\Gamma(17)\\Gamma(39)} \\theta^{17-1} (1 - \\theta)^{39-1} \\tag{10.3}\\]\nor equivalently \\(\\theta \\sim Beta(17, 39)\\).\n\n\n\n\n\n\n\nBig Idea\n\n\n\nA prior distribution quantifies the uncertainty we have about a parameter prior to observing data."
  },
  {
    "objectID": "03e-posteriors.html",
    "href": "03e-posteriors.html",
    "title": "11  Updating Prior Beliefs (Posterior Distributions)",
    "section": "",
    "text": "The previous chapter addressed the construction of a prior distribution, a distribution which captures the uncertainty we have in the unknown parameters governing the data generating process prior to observing any data. Once we observe data, however, the data should update our beliefs about the parameters. Through an application of Bayes’ Theorem, we derive the distribution of the parameters after observing the data, incorporating our prior beliefs. This is known as the posterior distribution.\n\nDefinition 11.1 (Posterior Distribution) A distribution quantifying our beliefs about the uncertainty in the parameter(s) of the underlying sampling distribution after observing data. This is often denoted by \\(\\pi(\\boldsymbol{\\theta} \\mid \\mathbf{y})\\) where \\(\\boldsymbol{\\theta}\\) is the parameter vector and \\(\\mathbf{y}\\) the observe data.\nGiven the likelihood \\(f(\\mathbf{y} \\mid \\boldsymbol{\\theta})\\) and a prior distribution on the parameters \\(\\pi(\\boldsymbol{\\theta})\\), the posterior distribution is computed using Bayes Theorem:\n\\[\\pi(\\boldsymbol{\\theta} \\mid \\mathbf{y}) = \\frac{f(\\mathbf{y} \\mid \\boldsymbol{\\theta}) \\pi(\\theta)}{\\int f(\\mathbf{y} \\mid \\boldsymbol{\\theta}) \\pi(\\boldsymbol{\\theta}) d\\boldsymbol{\\theta}}.\\]\n\nA posterior distribution is the conditional distribution of the parameters given the observed data. This allows us to make statements like “given the data, how likely is it that the parameter is between \\(a\\) and \\(b\\).” Just as a prior distribution depends on a subjective interpretation of probability, so too does a posterior distribution. With a posterior distribution, we have a way of quantifying our uncertainty in the parameters given the observed data!\n\n\n\n\n\n\nNote\n\n\n\nRecall that there is no “one” prior distribution but instead a different prior distribution for each set of prior beliefs. Similarly, there is no “one” posterior distribution. When we say “the” posterior, we are referring to the posterior distribution corresponding to the chosen prior distribution and the data observed.\n\n\n\nExample 11.1 (Naive Classification of College Students, Cont.) Consider Example 10.1 introduced in the previous chapter. Suppose that out of the 10 students, 3 identify as female.\nGiven this data, how sure is the individual that the college students are from ISU? How has the data observed impacted the individual’s prior beliefs?\n\nRecall that we had previously said that the likelihood could be modeled as a Binomial distribution. Specifically, letting \\(Y\\) represent the number of college students in the group that identify as female, then \\(Y \\sim Bin(10, \\theta)\\). That is,\n\\[\nf(y \\mid \\theta) = \\binom{n}{y} \\theta^y (1 - \\theta)^{n-y}.\n\\tag{11.1}\\]\nFurther, based on our prior beliefs, we defined a prior distribution in Equation 10.2:\n\\[\\pi(\\theta) = 0.4\\delta(\\theta - 0.246) + 0.6\\delta(\\theta - 0.563).\\]\nFor this likelihood and prior distribution, applying Bayes Theorem provides the corresponding posterior distribution. Specifically,\n\\[\\pi(\\theta \\mid y) = \\frac{\\binom{10}{3} \\theta^{3} (1 - \\theta)^{10 - 3} \\left[0.4\\delta(\\theta - 0.246) + 0.6\\delta(\\theta - 0.563)\\right]}{\\int_{0}^{1} \\binom{10}{3} \\theta^{3} (1 - \\theta)^{10 - 3} \\left[0.4\\delta(\\theta - 0.246) + 0.6\\delta(\\theta - 0.563)\\right] d\\theta}. \\tag{11.2}\\]\nEquation 11.2 is accurate, but it is not extremely useful in its current form; in particular, the form is daunting and makes it difficult to interpret directly. We can begin simplifying the expression by first simplifying the denominator. Note that\n\\[\n\\begin{aligned}\n  \\text{denom}\n    &= \\int_{0}^{1} \\binom{10}{3} \\theta^{3} (1 - \\theta)^{10 - 3} (0.4) \\delta(\\theta - 0.246) \\\\\n    &\\qquad + \\int_{0}^{1} \\binom{10}{3} \\theta^{3} (1 - \\theta)^{10 - 3} (0.6) \\delta(\\theta - 0.563) d\\theta \\\\\n    &= \\binom{10}{3} (0.246)^{3} (0.754)^{7} (0.4) + \\binom{10}{3} (0.563)^{3} (0.437)^{7} (0.6).\n\\end{aligned}\n\\]\nNotice that this denominator does not depend on the parameter (as the parameter was integrated out). The denominator is function only of the observed data.\n\n\n\n\n\n\nImportant\n\n\n\nOnce the data is observed, the denominator in the posterior distribution is a constant.\n\n\nWe now use this computed denominator to simplify Equation 11.2. Plugging in, we have\n\\[\n\\begin{aligned}\n  \\pi(\\theta \\mid y)\n    &= \\frac{\\binom{10}{3} \\theta^{3} (1 - \\theta)^{10 - 3} \\left[0.4\\delta(\\theta - 0.246) + 0.6\\delta(\\theta - 0.563)\\right]}{\\binom{10}{3} (0.246)^{3} (0.754)^{7} (0.4) + \\binom{10}{3} (0.563)^{3} (0.437)^{7} (0.6)} \\\\\n    &= \\theta^{3} (1 - \\theta)^{10 - 3} \\left[\\frac{\\delta(\\theta - 0.246)}{(0.246)^3 (0.754)^7 + (0.563)^3 (0.437)^7 (3/2)} \\right. \\\\\n    &\\qquad + \\left.\\frac{\\delta(\\theta - 0.563)}{(0.246)^3 (0.754)^7 (2/3) + (0.563)^3 (0.437)^7}\\right],\\\\\n\\end{aligned}\n\\]\nwhich simplifies to\n\\[\\pi(\\theta \\mid y) = (0.7169)\\delta(\\theta - 0.246) + (0.2831)\\delta(\\theta - 0.563). \\tag{11.3}\\]\nGiven the data, the individual can be 71.69% sure the students attend RHIT. Notice that the data has reversed the individual’s prior beliefs. Where they were 60% sure the students were from ISU, once they observed the data, they are now more than 70% sure the students are from RHIT. The data observed (3 out of 10 students identifying as female) could easily have come from either school; that is, it is entirely possible that we could sample 10 students at random from ISU and 3 identify as female. However, such a sample is more likely to occur if we sample our 10 students from the RHIT student body. Therefore, the individual’s belief about where the students attend school was updated based on the data.\nThis example illustrates how Bayes Theorem can be used to update our beliefs given observed data. However, there are some additional observations that are worth noting. First, note that the support of the posterior matches the support of the prior.\n\n\n\n\n\n\nNote\n\n\n\nIf the support of the likelihood does not depend on the parameter, then the support of the posterior matches the support of the chosen prior.\nIf the support of the likelihood depends on the parameter, the data will further refine the support of the posterior.\n\n\nIf you go into a problem wholeheartedly believing something is not possible, then no amount of data will convince you otherwise; think of this as a core belief that is unshakable. That is, any parameter value that is excluded by the prior distribution will be excluded in the posterior distribution automatically. Data can only convince those who are open to believing something different!\nSecond, notice the hardest computational aspect of the above example was computing the integral in the denominator and then carrying the algebra through in order to determine a simplified form of the posterior distribution. While we could rely on a computer algebra system in order to perform these computations in simple settings, relying on these tools tends to fail in more complex problems encountered in practice. Moving forward, we will want a way of overcoming the integral in the denominator, especially in cases when the parameter vector grows to be high-dimensional. To begin emphasizing the need to find alternatives, consider the following observation: the denominator in the computation of the posterior is constant with respect to the parameter. Careful consideration of this observation allows us to move through computations more quickly.\n\n\n\n\n\n\nApplying Bayes Theorem in Practice:\n\n\n\nThe denominator in Bayes rule exists to ensure the distribution integrates to 1; it is just a scaling constant. That is,\n\\[\\pi(\\boldsymbol{\\theta} \\mid \\mathbf{y}) \\propto f(\\mathbf{y} \\mid \\boldsymbol{\\theta}) \\pi(\\boldsymbol{\\theta}).\\]\nThis recognition allows us to quickly compute the kernel of the posterior, which in many cases is sufficient for identifying the posterior distribution.\n\n\nFinally, we emphasize that the posterior distribution does not tell you the value of the unknown parameter — a parameter is unknown and will always remain so! The posterior distribution only tells you the beliefs you have about that parameter given the data you have observed and your prior beliefs.\n\nExample 11.2 (C-section Deliveries Continued) Example 9.1 introduced a study, a component of which includes estimating the probability of a mother undergoing a C-section delivery at a particular hospital.\nUsing the likelihood developed in Example 9.1 and the prior developed in Example 10.2, derive the form of the posterior distribution given a sample of data \\(X_1, X_2, \\dotsc, X_n\\). Then, suppose we observed the following data, how does it update our beliefs?\n\n\n\n\n\nTable 11.1: Hypothetical data representing the number of vaginal deliveries between consecutive C-sections.\n\n\n3\n1\n0\n0\n0\n\n\n2\n5\n6\n9\n0\n\n\n5\n1\n0\n1\n0\n\n\n\n\n\n\n\n\n\n\n\nSolution. We first develop a general solution before substituting in the observed data. Recall that the likelihood (Equation 9.1) was given by\n\\[f(\\mathbf{x} \\mid \\theta) = \\theta^n (1 - \\theta)^{n \\bar{x}},\\]\nand the prior (Equation 10.3) was given by\n\\[\\pi(\\theta) = \\frac{\\Gamma(a+b)}{\\Gamma(a) \\Gamma(b)} \\theta^{a-1} (1 - \\theta)^{b-1},\\]\nwhere we have written the prior in its general form (not with the specific choices of the hyperparameter). Applying Bayes Theorem, we know that the posterior is proportional to the product of the likelihood and prior; that is,\n\\[\n\\begin{aligned}\n  \\pi(\\theta \\mid \\mathbf{x})\n    &\\propto f(\\mathbf{x} \\mid \\theta) \\pi(\\theta) \\\\\n    &= \\theta^n (1 - \\theta)^{n \\bar{x}} \\frac{\\Gamma(a + b)}{\\Gamma(a) \\Gamma(b)} \\theta^{a-1} (1 - \\theta)^{b - 1} \\\\\n    &\\propto \\theta^{n + a - 1} (1 - \\theta)^{n\\bar{x} + b - 1}.\n\\end{aligned}\n\\]\nObserve that since we are simply trying to determine what the posterior is proportional to, we can drop any scaling constants (with respect to the parameter); doing this in line (3) allows us to drop the gamma terms, simplifying the expression greatly. In fact, we now note that our posterior distribution is proportional to the form \\(\\theta^{\\text{something} - 1} (1 - \\theta)^{\\text{something else} - 1}\\), which we recognize as the kernel of a Beta distribution. That is, the appropriate scaling term to ensure that the posterior integrates to 1 (and is therefore a valid density function) is\n\\[\\frac{\\Gamma(n + a + n\\bar{x} + b)}{\\Gamma(n + a)\\Gamma(n\\bar{x} + b)},\\]\ngiving a posterior distribution of\n\\[\\pi(\\theta \\mid \\mathbf{x}) = \\frac{\\Gamma(n + a + n\\bar{x} + b)}{\\Gamma(n + a)\\Gamma(n\\bar{x} + b)} \\theta^{n + a - 1} (1 - \\theta)^{n\\bar{x} + b - 1}, \\tag{11.4}\\]\nor \\(\\theta \\mid \\mathbf{x} \\sim Beta(n + a, n\\bar{x} + b)\\). Of course, \\(a\\) and \\(b\\) are known values (chosen in the derivation of the prior), and once we observe the data, \\(n\\), \\(\\bar{x}\\) are also known. Therefore, the posterior distribution is fully specified. Specifically, substituting in these known values given the data observed, we have that \\(\\theta \\mid \\mathbf{x} \\sim Beta(32, 72)\\).\n\nFigure 11.1 compares the prior and posterior densities given the data in Example 11.2. Notice the two distributions are similar. Both have the same support (the interval \\((0, 1)\\)), and both tend to have a mode (peak) at roughly the same location. However, the posterior distribution has less variability (notice most of its mass is condensed around a tighter interval). This suggests that the data has increased our confidence in the value of the unknown parameter. However, notice that we did not “solve” for the value of \\(\\theta\\); in fact, the posterior distribution highlights that we are not certain about the value of \\(\\theta\\). Instead, the posterior is simply telling us how likely we feel the parameter is within any particular interval given the observed data.\nFor example, since\n\\[\\int_{0.2}^{0.4} \\pi(\\theta \\mid \\mathbf{x}) d\\theta = \\int_{0.2}^{0.4} \\frac{\\Gamma(32 + 72)}{\\Gamma(32)\\Gamma(72)} \\theta^{32 - 1} (1 - \\theta)^{72 - 1} d\\theta = 0.971,\\]\ngiven the data observed, we are now 97.1% sure that the rate of C-sections at the hospital is between 20% and 40%; this is an increase from what we believed prior to observing the data.\n\n\n\n\n\nFigure 11.1: Comparison of the prior distribution and posterior distribution for the C-section Deliveries example given the observed data."
  },
  {
    "objectID": "03f-point-estimation.html",
    "href": "03f-point-estimation.html",
    "title": "12  Point Estimation",
    "section": "",
    "text": "Everything we could ever want to know about a parameter, given the data we have observed, is contained in the posterior distribution. For those very comfortable with probability theory, we may not shy away from having an entire distribution presented to us as a way of summarizing the information available about the parameter. However, we know there are ways of summarizing a distribution, and often practitioners prefer to be presented with a summary of the posterior distribution. In this chapter, we examine methods for estimating the parameter of interest using the posterior distribution.\n\n\n\n\n\n\nBig Idea\n\n\n\nEstimating the parameter of interest is typically done by summarizing the location of the posterior distribution.\n\n\nA course in probability introduces us to the idea of using the mean, the median, and/or the mode in order summarize the location of a distribution. We can apply the same techniques to summarize the location of the posterior distribution. The three common point estimates for a parameter in the Bayesian framework are the posterior mean, the posterior median, and posterior mode.\n\nDefinition 12.1 (Posterior Mean) The posterior mean is the average value of the parameter, given the data:\n\\[E\\left[\\boldsymbol{\\theta} \\mid \\mathbf{y}\\right] = \\int \\boldsymbol{\\theta} \\pi(\\theta \\mid \\mathbf{y}) d\\boldsymbol{\\theta}.\\]\n\nWe note that as the dimension of the parameter vector increases, this could be a very difficult integral to compute. We will address this issue in the next unit. For now, we will typically work with known distributions and therefore known expressions for the posterior mean.\n\nDefinition 12.2 (Posterior Median) We are 50% sure, given the data, the parameter falls below the posterior median. Formally, the posterior median is the value \\(q\\) such that\n\\[0.5 = \\int_{-\\infty}^{q} \\pi(\\theta \\mid \\mathbf{y}) d\\theta.\\]\n\nWhile closed-form solutions may exist for the posterior mean, even with known distributions the posterior median must often be computed numerically. This is not problematic; we are simply acknowledging that in statistics, numerical solutions are common and are not viewed as inferior.\n\nDefinition 12.3 (Posterior Mode) We think of the posterior mode as the most likely value of the parameter, given the data. If the posterior distribution is continuous, the posterior mode is the value of the parameter that maximizes the posterior distribution. Formally, the posterior mode is given by\n\\[\\arg \\max_{\\theta} \\pi(\\theta \\mid \\mathbf{y}).\\]\n\n\n\n\n\n\n\nNote\n\n\n\nThe posterior mode only makes sense as an estimate if the posterior distribution is unimodal.\n\n\nOne might ask which of the three estimates is “best.” It depends. The mean and median may not be representative of a “typical” value. The mean is more sensitive to extreme values; the median tends to be more stable. However, many software packages default to reporting the posterior mean, making it a popular choice out of simplicity. Again, regardless of which value we choose to report, we should not neglect that we have access to the entire posterior distribution; therefore, we are not limited by a single estimate but can provide a much richer summary of the posterior distribution.\n\n\n\n\n\n\nWarning\n\n\n\nIt is common for those first learning the Bayesian framework to confuse the parameter being estimated with the method of estimation used. We can use the posterior mode to estimate the mean response. We can use the posterior mean to estimate the variance of the response. The method of estimation (posterior mean, posterior median, or posterior mode) is not linked to the parameter (mean response, variance of the response, etc.).\n\n\nWe close this chapter by considering two examples. First, consider Example 11.1. Since the support of the posterior includes only two values (0.246 and 0.563), the posterior mean would necessarily take a value not in the support. The posterior median suffers from the same limitation. The posterior mode is 0.246, since it is the more likely value, given the data and the individual’s prior beliefs. Note, however, that we lose information by only reporting the point estimate; we have a much richer conclusion when we report the entire posterior distribution: we are 71.69% sure the students are from RHIT and 28.31% sure the students are from ISU.\n\nExample 12.1 (C-section Deliveries Continued) Example 9.1 introduced a study, a component of which includes estimating the probability of a mother undergoing a C-section delivery at a particular hospital.\nExample 11.2 found the posterior distribution to be\n\\[\\theta \\mid \\mathbf{x} \\sim Beta\\left(n + a, n\\bar{x} + b\\right)\\]\nwhere \\(a = 17, b = 39, n = 15\\) and \\(n\\bar{x} = 33\\) given the observed data. Estimate the rate of C-sections at the hospital given the observed data.\n\n\nSolution. While it may seem obvious, our estimate is based on the observed data; different data would lead to a different estimate. Hidden in that statement is that our estimate is also based (at least partially) on our prior beliefs; different prior beliefs may also lead to a different estimate.\nSince the distributional family of the posterior is well-studied (a Beta distribution), we can make use of established properties in computing our point estimate. In particular, we immediately have that\n\\[\n\\begin{aligned}\n  E\\left(\\theta \\mid \\mathbf{x}\\right)\n    &= \\frac{n + a}{n + a + n\\bar{x} + b} = 0.308 \\\\\n  \\text{Posterior Mode}\n    &= \\frac{n + a - 1}{n + a + n\\bar{x} + b - 2} = 0.304.\n\\end{aligned}\n\\]\nWhile we must compute it numerically, the posterior median is also readily available and is given by 0.306. All three estimates are extremely similar. Given the data, we estimate the C-section rate at the hospital to be just over 30% (very near the rate in the state of Indiana)."
  },
  {
    "objectID": "03g-interval-estimation.html",
    "href": "03g-interval-estimation.html",
    "title": "13  Interval Estimation",
    "section": "",
    "text": "The previous chapter considered a single point estimate for the parameter of interest. However, this ignores the fact that there is variability in these estimates. The distribution itself tells us the parameter is more likely to fall in some regions than others. In response, we consider providing a range of plausible values for the parameter and quantifying our belief that the parameter falls in that range. This is the contrast between point and interval estimation.\n\nDefinition 13.1 (Point Estimation) Point estimation is the process of estimating a parameter with a single statistic. This is like trying to hit an infinitesimally small target with a dart.\n\n\nDefinition 13.2 (Interval Estimation) Interval estimation is the process of estimating a parameter with a range of values. This is like trying to capture a target with a ring.\n\nRegardless of which method we use, both are estimates, and both depend on the posterior distribution. That is, both are statements about the parameter given the observed data and our prior beliefs. As there were various techniques for constructing a point estimate, there are various techniques for an interval estimate; the most common of these is the credible interval.\n\nDefinition 13.3 (Credible Interval) A \\(100c\\)% credible interval is an interval \\((a, b)\\) such that\n\\[Pr(a \\leq \\theta \\leq b \\mid \\mathbf{y}) = \\int_{a}^{b} \\pi(\\theta \\mid \\mathbf{y})d\\theta = c.\\]\n\n\n\n\n\n\n\nWarning\n\n\n\nFor those who have had a previous statistics course taught from the classical Frequentist perspective, this seems to mirror a confidence interval, but the interpretation is completely different. Since probability is used to quantify subjective beliefs, notice that the credible interval allows us to say that we are \\(100c\\)% sure the parameter falls in this range, given the data.\nSince we are working from a subjective interpretation of probability, we do not need to appeal to repeated sampling (like a Frequentist would). In fact, since a parameter is a fixed, unknown quantity, any probability statement is illogical from a Frequentist perspective. However, from the Bayesian perspective, the posterior quantifies our uncertainty about the parameter, and therefore the credible interval is simply summarizing this uncertainty. We can now say, based on the data observed (however much or little we have), we are \\(100c\\)% sure the parameter falls in this interval.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere is no one unique credible interval for a parameter.\n\n\nSince there are infinitely many regions which contain \\(100c\\)% of the posterior distribution, there are infinitely many credible intervals we could provide. In order to provide some level of continuity between applications, we tend to gravitate to one of two types of intervals which have nice properties.\n\nDefinition 13.4 (Equal-Tailed Credible Interval) The equal-tailed credible interval, which is probably the most commonly used in practice, chooses endpoints such that\n\\[Pr(\\theta &lt; a \\mid \\mathbf{y}) = \\frac{1-c}{2} = Pr(\\theta &gt; b \\mid \\mathbf{y}).\\]\n\nAs the name implies, an equal-tailed credible interval places the same probability in each tail; we are taking the middle \\(100c\\)% of the posterior distribution.\nAn equal-tailed interval is easy, but it may not always be the most intuitive interval. Figure 13.1 compares two potential 90% credible intervals for a hypothetical posterior distribution. Observe that the equal-tailed interval removes the bottom 5% of the distribution; while this band is narrow, it represents values which correspond to the highest posterior density values. It seems intuitive that we would want to choose the narrowest credible interval which still retains the same area under the curve, as illustrated in the second panel of Figure 13.1.\n\n\n\n\n\nFigure 13.1: Comparison of two 90% credible intervals for a hypothetical posterior distribution.\n\n\n\n\n\nDefinition 13.5 (Highest Density Interval) The highest density interval, often called an HDI or HPD (for highest posterior density), chooses the endpoints such that the interval is as short as possible.\nWhen the density is unimodal, this can be accomplished by choosing the endpoints \\(a\\) and \\(b\\) such that\n\\[\\pi(\\theta \\mid \\mathbf{y}) \\mid_{\\theta = a} = \\pi(\\theta \\mid \\mathbf{y}) \\mid_{\\theta = b}\\]\nand\n\\[\\int_{a}^{b} \\pi(\\theta \\mid \\mathbf{y} d\\theta = c.\\]\n\n\n\n\n\n\n\nNote\n\n\n\nIf the posterior distribution is multimodal, then the highest density interval is actually a region as it will likely involve two disjoint intervals.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nMost software that computes an HDI assumes the poterior distribution is unimodal.\n\n\nSuppose we have a \\(100c\\)% credible interval \\((a, b)\\) for some parameter \\(\\theta\\), but we are interested in a transformation of the parameter \\(\\eta = g(\\theta)\\). We can develop a \\(100c\\)% credible interval for \\(\\eta\\) by applying the same transformation to each endpoint of the interval for \\(\\theta\\). That is, \\(\\left(g(a), g(b)\\right)\\) will be a \\(100c\\)% credible interval for the parameter \\(\\eta\\) given the data.\nWhile we can guarantee that \\(\\left(g(a), g(b)\\right)\\) is a \\(100c\\)% credible interval, it will in general not be the HDI for \\(\\eta\\), even if \\((a, b)\\) is the HDI for \\(\\theta\\).\n\nExample 13.1 (C-section Deliveries Continued) Example 9.1 introduced a study, a component of which includes estimating the probability of a mother undergoing a C-section delivery at a particular hospital.\nExample 11.2 found the posterior distribution to be\n\\[\\theta \\mid \\mathbf{x} \\sim Beta\\left(n + a, n\\bar{x} + b\\right)\\]\nwhere \\(a = 17, b = 39, n = 15\\) and \\(n\\bar{x} = 33\\) given the observed data. Estimate the rate of C-sections at the hospital given the observed data.\n\n\nSolution. Example 12.1 developed point estimates of the unknown parameter given the observed data (and prior beliefs). Here, we consider an interval estimate. To compute an equal-tailed interval, we must choose the values of \\(s\\) and \\(t\\) such that\n\\[\n\\begin{aligned}\n  0.025 &= \\int_{0}^{s} \\frac{\\Gamma(32 + 72)}{\\Gamma(32) \\Gamma(72)} \\theta^{32 - 1} (1 - \\theta)^{72 - 1} d\\theta \\\\\n  0.025 &= \\int_{t}^{1} \\frac{\\Gamma(32 + 72)}{\\Gamma(32) \\Gamma(72)} \\theta^{32 - 1} (1 - \\theta)^{72 - 1} d\\theta. \\\\\n\\end{aligned}\n\\]\nSolving this system numerical gives the interval \\((0.223, 0.399)\\). We are 95% sure, given the data, the rate of C-sections at the hospital is between 22.3% and 39.9%. Given that the posterior distribution is unimodal and nearly symmetric, we would expect the HDI to be very similar to the equal-tailed interval."
  },
  {
    "objectID": "03h-prediction.html#derivation-of-the-posterior-predictive",
    "href": "03h-prediction.html#derivation-of-the-posterior-predictive",
    "title": "14  Prediction",
    "section": "14.1 Derivation of the Posterior Predictive",
    "text": "14.1 Derivation of the Posterior Predictive\nLet \\(\\mathbf{Y}^*\\) denote a collection of \\(m\\) future (or new) observations not yet observed. This is distinguished from the collection of \\(n\\) observations we have already made \\(\\mathbf{Y}\\). We impose the following two conditions/assumptions on the data generating process:\n\nGiven the value of the parameter, the likelihood of \\(\\mathbf{Y}^*\\) has the same form as the likelihood of of the observed data \\(\\mathbf{Y}\\).\nGiven the value of the parameter, the observed data \\(\\mathbf{Y}\\) is independent of the new observations \\(\\mathbf{Y}^*\\).\n\nThe first condition essentially states the data generated under one process should only be used to predict data generated from the same process. Intuitively, when we collect data, it can only inform us about the process from which it was generated. Therefore, our future observations are always related in some way to the likelihood, as that models the data generating process of interest.\nThe second condition extends the concept of independence presented in a typical probability course. This is conditional independence.\n\nDefinition 14.3 (Conditional Independence) Two random variables \\(X\\) and \\(Y\\) are said to be independent, conditional on (or “given”) \\(Z\\) if, and only if,\n\\[f_{(X,Y) \\mid Z} (x, y \\mid z) = f_{X \\mid Z}(x \\mid z) f_{Y \\mid Z}(y \\mid z).\\]\n\nConditional independence is common in statistical theory. Two random quantities are somehow related, but given an additional piece of information become independent. That is, all the information about the relationship between \\(X\\) and \\(Y\\) is contained in the random variable \\(Z\\).\nReturning to the stated condition, we are saying that the only thing the new and old observations have in common is the data generating process; once we know the quantities that govern this process (the parameters), then we can gain no further knowledge about the new observations from the old observations. That is, if someone told you what the parameters were, there would be no need to collect data — you would know everything possible for predicting a future observation. So, the data observed is only useful in that it informs our beliefs about the unknown parameters.\n\n\n\n\n\n\nBig Idea\n\n\n\nThe data observed informs our beliefs about the parameters in the data generating process. It is only through what the data tells us about the parameters that the data is useful in predicting a future observation.\n\n\nWe are now prepared to derive the posterior predictive distribution. Recall that a marginal distribution can be constructed by integrating over the other elements of a joint distribution. For example,\n\\[f\\left(\\mathbf{y}^*\\right) = \\int f\\left(\\mathbf{y}^*, \\theta\\right) d\\theta.\\]\nHere, we have considered the joint distribution of the new data \\(\\mathbf{Y}^*\\) and the parameter \\(\\theta\\), then integrated over \\(\\theta\\). This strategy works even when we are carrying a conditional term through. That is, we have that\n\\[\\pi\\left(\\mathbf{y}^* \\mid \\mathbf{y}\\right) = \\int f\\left(\\mathbf{y}^*, \\theta \\mid \\mathbf{y}\\right) d\\theta. \\tag{14.1}\\]\nHere, we have considered the joint distribution of the new data \\(\\mathbf{Y}^*\\) and the parameter \\(\\theta\\) (conditional on the observed data), then integrated over \\(\\theta\\). This statement would have been true for any choice of random variable, but using the parameter allows us to make use of the information we have collected through the observed data.\nWe now recall that any joint distribution can be written as the product of a conditional distribution and a marginal distribution; this is true even if we are already conditioning on another random variable. This gives\n\\[f(\\mathbf{y}^*, \\theta \\mid \\mathbf{y}) = f(\\mathbf{y}^* \\mid \\mathbf{y}, \\theta) \\pi(\\theta \\mid \\mathbf{y}). \\tag{14.2}\\]\nSubstituting Equation 14.2 into Equation 14.1, we now have that the posterior predictive distribution is given by\n\\[\\pi\\left(\\mathbf{y}^* \\mid \\mathbf{y}\\right) = \\int f(\\mathbf{y}^* \\mid \\mathbf{y}, \\theta) \\pi(\\theta \\mid \\mathbf{y}) d\\theta. \\tag{14.3}\\]\nWe now make use of conditional independence. We now consider the term \\(f(\\mathbf{y}^* \\mid \\mathbf{y}, \\theta)\\) inside the integral. By the definition of a conditional density, we have\n\\[f(\\mathbf{y}^* \\mid \\mathbf{y}, \\theta) = \\frac{f(\\mathbf{y}^*, \\mathbf{y} \\mid \\theta)}{f(\\mathbf{y} \\mid \\theta)}.\\]\nHowever, if we are willing to assume that \\(\\mathbf{Y}^*\\) is independent of \\(\\mathbf{Y}\\) given \\(\\theta\\), then the numerator becomes the product of \\(f(\\mathbf{y}^* \\mid \\theta)\\) and \\(f(\\mathbf{y} \\mid \\theta)\\), meaning we have that \\(f(\\mathbf{y}^* \\mid \\mathbf{y}, \\theta) = f(\\mathbf{y}^* \\mid \\theta)\\) under conditional independence. Substituting in this expression into Equation 14.3 gives the posterior predictive distribution in Definition 14.2."
  },
  {
    "objectID": "03h-prediction.html#summary",
    "href": "03h-prediction.html#summary",
    "title": "14  Prediction",
    "section": "14.2 Summary",
    "text": "14.2 Summary\nOnce you have the posterior predictive distribution, you have everything there is to know about future observations given the data observed. Further, the posterior predictive distribution can be summarized just like any other distribution. Summarizing the location (mean, median, mode) would result in point estimates for future observations. Alternatively, we can construct interval estimates by defining a range for which the future observations would fall with some known probability.\n\n\n\n\n\n\nWarning\n\n\n\nKeep in mind that we have switched our focus. We are now focused on a possible data point, not a parameter. Therefore, the support of the posterior predictive need not be the same as the support of the posterior distribution.\n\n\n\nExample 14.1 (C-section Deliveries Continued) Example 9.1 introduced a study, a component of which includes estimating the probability of a mother undergoing a C-section delivery at a particular hospital.\nExample 11.2 found the posterior distribution to be\n\\[\\theta \\mid \\mathbf{x} \\sim Beta\\left(n + a, n\\bar{x} + b\\right)\\]\nwhere \\(a = 17, b = 39, n = 15\\) and \\(n\\bar{x} = 33\\) given the observed data. Suppose we are interested in adding a 16-th patient to our survey; predict the number of vaginal deliveries we should expect before we observe a C-section.\n\n\nSolution. We are interested in predicting a new response given the observed data. Following the discussion in the derivation of the posterior predictive distribution, it makes sense that we would assume the density of this new response follows the same distribution as the observed data. In particular, if \\(Y\\) represents the future observation, then\n\\[f(y \\mid \\theta) = \\theta (1 - \\theta)^y\\]\nsince each observed value (previously denoted by \\(X_i\\)) was modeled as a random variate from a Geometric distribution. Applying Definition 14.2, we have that the posterior predictive distribution is given by\n\\[\n\\begin{aligned}\n  \\pi(y \\mid \\mathbf{x})\n    &= \\int f\\left(y \\mid \\theta\\right) \\pi(\\theta \\mid \\mathbf{x}) d\\theta \\\\\n    &= \\int \\theta (1 - \\theta)^y \\frac{\\Gamma(n + a + n\\bar{x} + b)}{\\Gamma(n + a) \\Gamma(n\\bar{x} + b)} \\theta^{n + a - 1} (1 - \\theta)^{n\\bar{x} + b - 1} d\\theta \\\\\n    &= \\frac{\\Gamma(n + a + n\\bar{x} + b)}{\\Gamma(n + a) \\Gamma(n\\bar{x} + b)} \\int \\theta^{n + a + 1 - 1} (1 - \\theta)^{n\\bar{x} + b + y - 1} d\\theta.\n\\end{aligned}\n\\]\nNotice that the integrand is the kernel of a Beta distribution; therefore, we can multiply and divide by the appropriate scaling terms. This gives\n\\[\n\\begin{aligned}\n  \\pi(y \\mid \\mathbf{x})\n    &= \\frac{\\Gamma(n + a + n\\bar{x} + b)}{\\Gamma(n + a) \\Gamma(n\\bar{x} + b)} \\int \\theta^{n + a + 1 - 1} (1 - \\theta)^{n\\bar{x} + b + y - 1} d\\theta \\\\\n    &= \\frac{\\Gamma(n + a + n\\bar{x} + b)}{\\Gamma(n + a) \\Gamma(n\\bar{x} + b)} \\frac{\\Gamma(n + a + 1) \\Gamma(n\\bar{x} + b + y)}{\\Gamma(n + a + n\\bar{x} + b + 1 + y)} \\\\\n    &\\qquad \\cdot \\int \\frac{\\Gamma(n + a + n\\bar{x} + b + 1 + y)}{\\Gamma(n + a + 1) \\Gamma(n\\bar{x} + b + y)} \\theta^{n + a + 1 - 1} (1 - \\theta)^{n\\bar{x} + b + y - 1} d\\theta \\\\\n    &= \\frac{\\Gamma(n + a + n\\bar{x} + b)}{\\Gamma(n + a) \\Gamma(n\\bar{x} + b)} \\frac{\\Gamma(n + a + 1) \\Gamma(n\\bar{x} + b + y)}{\\Gamma(n + a + n\\bar{x} + b + 1 + y)}.\n\\end{aligned}\n\\]\nWe are not meant to recognize this distribution. However, we can work with it. We must keep in mind the warning, however, given just prior to this example — the support of this distribution is non-negative integers, not the interval \\((0, 1)\\).\nAgain, prediction is not about saying how many vaginal births we will see before the next C-section; it is really about quantifying our uncertainty in the various possibilities. For example, given the data observed, we are 30.8% sure that the very next birth will be a C-section, since\n\\[Pr(Y = 1 \\mid \\mathbf{x}) = \\frac{\\Gamma(32 + 72)}{\\Gamma(32) \\Gamma(72)} \\frac{\\Gamma(32 + 1) \\Gamma(72 + 0)}{\\Gamma(32 + 72 + 1 + 0)} = 0.308.\\]\nSimilarly, we are 88.3% sure that we will not experience more than 5 vaginal births before the next C-section, since\n\\[Pr(Y \\leq 5 \\mid \\mathbf{x}) \\sum_{u=0}^{5} Pr(Y = u \\mid \\mathbf{x}) = 1 - \\sum_{u=0}^{5} \\frac{\\Gamma(32 + 72)}{\\Gamma(32) \\Gamma(72)} \\frac{\\Gamma(32 + 1) \\Gamma(72 + u)}{\\Gamma(32 + 72 + 1 + u)} = 0.883.\\]\nHowever, if we would like to provide a single point estimate instead of probabilities of specific responses, we might report that, given the data, on average, we expect to see 2.32 vaginal deliveries before the next C-section since\n\\[E\\left(Y \\mid \\mathbf{x}\\right) = \\sum_{u=0}^{\\infty} u Pr(Y = u \\mid \\mathbf{x}) = 2.32.\\]"
  },
  {
    "objectID": "03i-hypothesis-testing.html#point-null-hypotheses",
    "href": "03i-hypothesis-testing.html#point-null-hypotheses",
    "title": "15  Hypothesis Testing",
    "section": "15.1 Point-Null Hypotheses",
    "text": "15.1 Point-Null Hypotheses\nTechnically speaking, hypothesis testing in the Bayesian framework requires little; the posterior distribution allows us to quantify our uncertainty about parameters (or corresponding hypotheses) given the data. However, there is a common scenario which has a potential pitfall worth discussion. Consider testing the following hypotheses:\n\\[H_0: \\theta = \\theta_0 \\qquad \\text{vs.} \\qquad H_1: \\theta \\neq \\theta_0\\]\nwhere here \\(\\Theta_0 = \\{\\theta_0\\}\\) is a singleton set. This is known as a “point-null hypothesis,” and it poses a problem for many prior distributions. To illustrate, consider computing the prior odds in favor of \\(H_1\\):\n\\[\\frac{Pr\\left(\\theta \\neq \\theta_0\\right)}{Pr\\left(\\theta = \\theta_0\\right)} = \\frac{\\int_{\\theta \\neq \\theta_0}^{} \\pi(\\theta) d\\theta}{\\int_{\\theta = \\theta_0}^{} \\pi(\\theta) d\\theta}. \\tag{15.1}\\]\nFor any continuous prior distribution, the numerator in Equation 15.1 will be 1 and the denominator will be 0! For any continuous distribution, the probability the random variable takes a specific value is 0; therefore, a continuous prior distribution is incompatible with a point-null hypothesis. The continuous prior distribution communicates you are infinitely more likely to believe the parameter takes any value other than \\(\\theta_0\\). There is a misalignment of beliefs; if you are truly interested in testing a point-null hypothesis, you must believe the null hypothesis has some probability of describing reality. Therefore, the prior distribution must incorporate the belief that \\[Pr\\left(H_0\\right) &gt; 0\\].\n\n\n\n\n\n\nBig Idea\n\n\n\nWe can only test a hypothesis if, a priori, we believe there is some chance the hypothesis is true. If we go into a study believing something is impossible, no amount of data will convince us otherwise.\n\n\nOne way of incorporating our prior beliefs about a point-null hypothesis is specifying a mixture distribution.\n\nDefinition 15.3 (Mixture Distribution) Let \\(X\\) be a random variable and \\(f(x)\\) and \\(g(x)\\) be valid density functions defined on a common support. Then,\n\\[h(x) = wf(x) + (1 - w) g(x),\\]\nwhere \\(0 &lt; w &lt; 1\\), is known as a mixture distribution.\n\nAppropriately applied, a mixture distribution allows us to place mass on the value of \\(\\theta_0\\) in a point-null hypothesis and spread out the remaining probability along the support.\n\n\n\n\n\n\nMixture Prior for Point-Null Hypotheses\n\n\n\nLet \\(\\theta\\) be a parameter which has support \\(\\Theta\\), and consider testing the hypotheses\n\\[H_0: \\theta = \\theta_0 \\qquad \\text{vs.} \\qquad H_1: \\theta \\neq \\theta_0\\]\nfor some \\(\\theta_0 \\in \\Theta\\). Suppose, a priori, we believe \\(Pr\\left(\\theta = \\theta_0\\right) = u\\) for some \\(0 &lt; u &lt; 1\\). Then,\n\\[\\pi(\\theta) = u \\delta\\left(\\theta - \\theta_0\\right) + (1 - u) \\pi_0(\\theta)\\]\nis a suitable family of prior distributions, where \\(\\pi_0(\\theta)\\) is any continuous distribution on the support \\(\\Theta\\).\n\n\n\n\n\n\n\n\nNote\n\n\n\nA mixture prior with a point mass is not a continuous distribution, nor is it a discrete distribution.\n\n\n\nExample 15.1 (C-section Deliveries Continued) Example 9.1 introduced a study, a component of which includes estimating the probability of a mother undergoing a C-section delivery at a particular hospital.\nSuppose the hospital is interested in testing\n\\[H_0: \\theta = 0.304 \\qquad \\text{vs.} \\qquad H_1: \\theta \\neq 0.304.\\]\nThe null hypothesis represents the C-section rate at the hospital being equivalent to the rate across the state of Indiana. Suppose we believe, prior to observing any data, either hypothesis is equally likely. Combining this belief with those expressed in Example 10.2, a reasonable prior distribution has the form\n\\[\\pi(\\theta) = 0.5 \\delta(\\theta - 0.304) + 0.5 \\frac{\\Gamma(a + b)}{\\Gamma(a)\\Gamma(b)}\\theta^{a-1}(1-\\theta)^{b-1},\\]\nwhere \\(a = 10.3\\) and \\(b = 23.6\\). Note that the mass at \\(\\theta_0 = 0.304\\) led to different values of hyperparameters in \\(\\pi_0(\\theta)\\) compared to Example 10.2. The resulting posterior will have the form\n\\[\\pi(\\theta \\mid \\mathbf{x}) = w \\delta(\\theta - 0.304) + (1 - w) \\frac{\\Gamma(a + b + n + n\\bar{x})}{\\Gamma(a + n)\\Gamma(b + n\\bar{x})} \\theta^{a + n - 1} (1 - \\theta)^{b + n\\bar{x} - 1},\\]\nwhere\n\\[w = \\frac{(0.5)(0.304)^n (1 - 0.304)^{n\\bar{x}}}{(0.5)(0.304)^n(1-0.304)^{n\\bar{x}} + (0.5) \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}\\frac{\\Gamma(a + n)\\Gamma(b + n\\bar{x})}{\\Gamma(a + b + n + n\\bar{x})}},\\]\nthe sample size \\(n\\) and sample mean \\(\\bar{x}\\) are given by the data, and the hyperparameters \\(a\\) and \\(b\\) were defined above.\nUsing this posterior distribution, describe our belief about the hypotheses.\n\n\nSolution. Notice that the form of the posterior distribution directly encodes our belief about the null hypothesis given the data. Specifically,\n\\[Pr\\left(\\theta = \\theta_0 \\mid \\mathbf{x}\\right) = w = 0.6098.\\]\nThe posterior odds in favor of the null hypothesis is\n\\[\\frac{Pr\\left(\\theta = \\theta_0 \\mid \\mathbf{x}\\right)}{Pr\\left(\\theta \\neq \\theta_0 \\mid \\mathbf{x}\\right)} = 1.56,\\]\nand the Bayes Factor is the same (since the prior odds were 1). The data strengthened our belief in the null hypothesis, but not by much."
  },
  {
    "objectID": "03i-hypothesis-testing.html#model-comparison",
    "href": "03i-hypothesis-testing.html#model-comparison",
    "title": "15  Hypothesis Testing",
    "section": "15.2 Model Comparison",
    "text": "15.2 Model Comparison\nHypothesis testing is a special case of model comparison, where in the Bayesian framework the “model” consists of both the likelihood and the prior.\n\n\n\n\n\n\nBig Idea\n\n\n\nA Bayesian model consists of the choice for the likelihood as well as the choice for the prior.\n\n\nConsider the Naive Classification of College Students (Example 10.1); we can reframe the problem as a choice between two models:\n\\[\n\\begin{aligned}\n  \\text{Model 0}:& \\quad \\pi(\\theta) = \\delta(\\theta - 0.246) \\\\\n    &\\quad f(y \\mid \\theta) = \\binom{n}{\\theta} \\theta^y (1-\\theta)^{n-y} \\\\\n  \\text{Model 1}:& \\quad \\pi(\\theta) = \\delta(\\theta - 0.563) \\\\\n    &\\quad f(y \\mid \\theta) = \\binom{n}{\\theta} \\theta^y (1-\\theta)^{n-y},\n\\end{aligned}\n\\]\nwhere we believed, a priori, that \\(Pr(\\text{Model 1}) = 0.4\\) and \\(Pr(\\text{Model 2}) = 0.6\\). In this case, the models differed only in their choice of prior (which here simplifies further to which value of the parameter to select). This simplification allowed us to choose between these two models by working only with the posterior distribution.\nWe would like to generalize this process to allow the model to alter both the likelihood and the prior distribution, and for us to place some prior probability on the entirety of the model. We are then interested in using the data to quantify the evidence for each model. To do this, we essentially consider the model \\(\\mathcal{M}\\) to be a parameter. This is known as a hierarchical model because the priors on the parameter are conditional on the model, and then a further prior is placed on the model itself (it is a multi-level model).\n\n\n\n\n\n\nModel Comparison\n\n\n\nLet \\(\\mathcal{M}_j\\) represent the \\(j\\)-th potential model for a data generating process. Reflect the likelihood and prior as a function of the model. For example, write\n\\[\n\\begin{aligned}\n  \\text{Model 0}:& \\quad f_0(\\mathbf{y} \\mid \\theta_0, \\mathcal{M}_0) \\\\\n    & \\quad \\pi_0(\\theta_0 \\mid \\mathcal{M}_0) \\\\\n    & \\quad \\pi(\\mathcal{M}_0) = Pr(\\mathcal{M}_0) \\\\\n  \\text{Model 1}:& \\quad f_1(\\mathbf{y} \\mid \\theta_1, \\mathcal{M}_1) \\\\\n    & \\quad \\pi_1(\\theta_1 \\mid \\mathcal{M}_1) \\\\\n    & \\quad \\pi(\\mathcal{M}_1) = Pr(\\mathcal{M}_1). \\\\\n\\end{aligned}\n\\]\nNotice we (potentially) allow the form of the likelihood, the parameters governing that likelihood, and the form of the prior to differ for each model. Our prior beliefs about the parameter are captured within each prior, but we also have prior beliefs about the model itself — how likely each model is. We are interested in determining \\(Pr(\\mathcal{M}_j \\mid \\mathbf{Y})\\) for each \\(j\\).\n\n\nAn iterated application of Bayes Theorem allows us to write the likelihood of each model, given the data, as\n\\[\n\\begin{aligned}\n  Pr\\left(\\mathcal{M}_j \\mid \\mathbf{Y}\\right)\n    &= \\frac{f_j(\\mathbf{y} \\mid \\mathcal{M}_j) Pr(\\mathcal{M}_j)}{\\sum_{j} f_j(\\mathbf{y} \\mid \\mathcal{M}_j) Pr(\\mathcal{M}_j)} \\\\\n  f_j(\\mathbf{y} \\mid \\mathcal{M}_j)\n    &= \\int f_j(\\mathbf{y} \\mid \\theta_j, \\mathcal{M}_j) \\pi_j(\\theta_j \\mid \\mathcal{M}_j) d\\theta_j.\n\\end{aligned}\n\\]\n\nDefinition 15.4 (Evidence for a Model) Under the Model Comparison framework defined above, the evidence for model \\(\\mathcal{M}_j\\) is defined as\n\\[f_j(\\mathbf{y} \\mid \\mathcal{M}_j) = \\int f_j(\\mathbf{y} \\mid \\theta_j, \\mathcal{M}_j) \\pi_j(\\theta_j \\mid \\mathcal{M}_j) d\\theta_j.\\]\n\nThe evidence for a model is a number; since it is a function only of observed data, once we observe the data, the evidence is a constant. We can also think of the evidence as the prior predictive distribution under a particular model evaluated at the observed data.\n\n\n\n\n\n\nWarning\n\n\n\nIt may seem strange to use the prior predictive distribution when defining the evidence instead of the posterior predictive, but keep in mind that within model comparison, the model itself is the parameter of interest. Changing either the likelihood or the prior will impact the evidence.\n\n\nThe evidence can also be used to compute the Bayes Factor for one model over another.\n\nDefinition 15.5 (Bayes Factor for Model Comparison) The Bayes Factor, in favor of Model 1, is\n\\[\n\\begin{aligned}\n  BF_{1} &= \\left(\\frac{Pr(\\mathcal{M}_1 \\mid \\mathbf{y})}{Pr(\\mathcal{M}_0 \\mid \\mathbf{y})}\\right)\\left(\\frac{Pr(\\mathcal{M}_0)}{Pr(\\mathcal{M}_1)}\\right) \\\\\n    &= \\left(\\frac{f_1(\\mathbf{y} \\mid \\mathcal{M}_1) Pr(\\mathcal{M}_1)}{f_0(\\mathbf{y} \\mid \\mathcal{M}_0) Pr(\\mathcal{M}_0)}\\right)\\left(\\frac{Pr(\\mathcal{M}_0)}{Pr(\\mathcal{M}_1)}\\right) \\\\\n    &= \\frac{f_1(\\mathbf{y} \\mid \\mathcal{M}_1)}{f_0(\\mathbf{y} \\mid \\mathcal{M}_0)}.\n\\end{aligned}\n\\]\nThat is, the Bayes Factor is a ratio of the evidence for each model.\n\nModel comparison simply extends our framework by the inclusion of another parameter. That parameter, the model itself, just happens to be discrete. Our goal is to use our machinery to quantify the uncertainty in each model given the data observed."
  },
  {
    "objectID": "03j-constructing-priors.html#elicitation-from-experts",
    "href": "03j-constructing-priors.html#elicitation-from-experts",
    "title": "16  Constructing Prior Distributions",
    "section": "16.1 Elicitation from Experts",
    "text": "16.1 Elicitation from Experts\nIdeally, the prior distribution would not be arbitrary but guided by experts. Example 10.2, for example, illustrated the use of statements from experts to form a parametric approximation to the prior information. We elicited information about the uncertainty in order to determine values for the hyperparameters — those values that determine the specific shape of the prior distribution.\nFor Example 10.2, the prior distribution chosen was a conjugate prior.\n\nDefinition 16.1 (Conjugate Prior) A prior distribution chosen such that the posterior distribution belongs to the same family as the prior distribution, with the (hyper)parameters that govern the family updated based on the observed data.\n\nIn Example 10.2, we chose a Beta distribution to represent the prior, and we found in Example 11.2, the posterior distribution also belonged to the Beta family. The choice to use a conjugate prior was often done historically in order to simplify computation in an era where computing power was limited. In the era of higher-speed computing, this is no longer necessary.\nOne argument for the use of conjugate priors is that the form is invariant to the data; that is, the data is restricted in what it can say about the unknown parameter. The data can update our beliefs, but it cannot update the family which encodes those beliefs.\nWhile we will not go into details here, it is almost always possible to construct a conjugate prior. And, if chosen carefully, that prior can approximate nearly any prior information given.\n\n\n\n\n\n\nNote\n\n\n\nThe posterior distribution is always a combination of the prior distribution and the likelihood. Conjugate priors make that very clear. As the sample gets large, the prior distribution is swamped by the data; and, as the sample size increases, Bayesian inference agrees with Frequentist inference.\n\n\nAgain, Example 10.2 illustrated combining the expert opinions provided with a parametric family in order to construct a prior distribution. When we are unable to determine a suitable parametric approximation for the prior beliefs provided, a “histogram approach” is possible.\n\nDefinition 16.2 (Histogram Approach to Constructing a Prior) Using expert information, attach probability to various intervals for the parameter. Specifically,\n\nDefine \\(m\\) intervals \\(\\left(\\theta_{j-1}, \\theta_j\\right)\\) for \\(j = 1, 2, \\dotsc, m\\) that partition the parameter space; define \\(\\theta_0\\) as the lower bound of the support for the parameter, and define \\(\\theta_m\\) as the upper bound of the support for the parameter.\nEliciting expert opinions, assign probability \\(\\pi_j\\) to each interval: \\(\\pi_j = Pr\\left(\\theta_{j-1} &lt; \\theta &lt; \\theta_j\\right)\\) for each \\(j = 1, 2, \\dotsc, m\\).\nSet the prior \\(\\pi(\\theta)\\) to be the piecewise distribution over this interval where \\(\\sum_{j=1}^{m} \\pi_j = 1\\).\n\n\nThere have been critiques of eliciting information from experts. Estimates given may be biased, due to the current availability of data on which the experts are making their informed decisions. We tend to be overconfident in our opinions or go with our initial reaction instead of allowing our beliefs to be updated. We also tend to want to create the prior only after observing the data, despite the fact that the prior should capture our beliefs prior to observing the data.\nThe experts may not actually represent a reasonable sample to capture widespread prior belief. How does one determine who is expertly qualified to speak on a particular topic? How do you rank levels of expertise?\nWe mention these critiques because more important than the choices we make is that those choices be clearly documented. It is okay to construct work that others critique; that is how science develops. No study is perfect, and being able to identify and own the limitations of our study and analysis is critical to the development of knowledge."
  },
  {
    "objectID": "03j-constructing-priors.html#mixture-priors",
    "href": "03j-constructing-priors.html#mixture-priors",
    "title": "16  Constructing Prior Distributions",
    "section": "16.2 Mixture Priors",
    "text": "16.2 Mixture Priors\nChapter 15 introduced the idea of a mixture distribution (Definition 15.3) for constructing a prior. While we discussed its use for a particular case, mixture distributions have wider applicability. Suppose we would like to work with a parametric approximation, but we cannot find a parametric family which captures the structure suggested by the prior information. In these cases, combining multiple distributions may be appropriate.\nAs an example, suppose we have a parameter on the support \\((0, 1)\\). For such a parameter, it is natural to consider the Beta distribution for a prior. However, suppose our prior beliefs suggest a multimodal distribution similar to that in Figure 16.1; it is impossible to choose hyperparameters for a Beta distribution that would result in such a prior. Instead, we could achieve such a prior by mixing two Beta distributions.\n\n\n\n\n\nFigure 16.1: Illustration of a mixture prior for a parameter on the interval (0, 1).\n\n\n\n\n\nDefinition 16.3 (General Mixture Distribution) Let \\(\\theta\\) be a parameter with support \\(\\Theta\\), and let \\(\\pi_k(\\theta)\\) be a valid distribution on the support, for \\(k = 1, 2, \\dotsc, K\\). Then,\n\\[\\pi(\\theta) = \\sum_{k=1}^{K} w_k \\pi_k(\\theta)\\]\nis a valid prior distribution provided \\(\\sum_{k=1}^{K} w_k = 1\\).\n\nIt turns out that if each component of the mixture distribution \\(\\pi_k(\\theta)\\) is a member of the conjugate family, the entire prior will be conjugate (a weighted average of distributions). This was illustrated in Example 15.1.\nNothing requires that the individual components of a mixture distribution be of the same family. For example, we might choose to mix a Normal distribution with a t-distribution in order to capture the presence of some outliers.\n\n\n\n\n\n\nNote\n\n\n\nWhile we have described the use of a mixture distribution for the prior distribution, nothing prevents the use from using a mixture distribution for the likelihood.\n\n\nIt has been shown that any distribution can be approximated by some mixture distribution. That is, if we choose \\(K\\) to be large enough, we can approximate any distributional shape with a mixture distribution."
  },
  {
    "objectID": "03j-constructing-priors.html#chains",
    "href": "03j-constructing-priors.html#chains",
    "title": "16  Constructing Prior Distributions",
    "section": "16.3 Chains",
    "text": "16.3 Chains\nWithin this unit, we have developed the fundamental concepts of Bayesian inference in a general setting. We have avoided a litany of examples and instead opted to illustrate the concepts with a single unifying example throughout the text (Example 9.1). In both this example and the exposition in the text, we have acted as though there is a single parameter \\(\\theta\\) governing the likelihood. Many interesting questions, however, involve models for the data that depend upon multiple parameters. These types of problems often necessitate the need for numerical solutions, which we address in the next unit. Here, we simply discuss a common tool for constructing priors over multiple parameters.\nWhen \\(\\theta\\) is a parameter vector, then \\(\\pi(\\theta)\\) is actually a joint density across all parameters. Therefore, one key decision that must be made is whether, a priori, we believe these parameters to be independent of one another.\n\nExample 16.1 (Heights of Children) During early development, children are regularly benchmarked against national growth charts. One such chart traces a child’s height as they grow. However, these charts were developed using the entire population of “healthy” children. Suppose I am interested in developing a growth chart for children with Hispanic parents, as I believe they tend to be a bit shorter, on average. It is typical to model heights using a Normal distribution, which has two unknown parameters (which govern the location and spread of the distribution).\nWe consider developing a likelihood and prior for this process.\n\nThe likelihood for the above example is readily available if we are willing to assume a random sample of \\(n\\) children (of the same age) born to Hispanic parents:\n\\[\n\\begin{aligned}\n  f(\\mathbf{y} \\mid \\mu, \\tau)\n    &= \\prod_{i=1}^{n} \\frac{\\tau^{1/2}}{\\sqrt{2\\pi}} e^{-\\tau/2 (y_i - \\mu)^2} \\\\\n    &= \\frac{\\tau^{n/2}}{(2\\pi)^{n/2}} e^{-(\\tau/2)\\sum_{i=1}^{n}(y_i - \\mu)^2}\n\\end{aligned}\n\\]\nwhere we have defined the likelihood in terms of the mean \\(\\mu\\) and the precision \\(\\tau\\), which is the inverse of the variance. The likelihood was simplified by assuming the height of one child is independent of the height of any other child. Suppose we are further willing to believe the parameters are independent of one another; then, it is reasonable to propose the prior distributions independently. Choosing a Normal prior for \\(\\mu\\) and a Gamma prior for \\(\\tau\\), we could then propose\n\\[\n\\begin{aligned}\n  \\pi(\\mu) &= \\frac{\\sqrt{b}}{\\sqrt{2\\pi}} e^{-b/2 (\\mu - a)^2} \\\\\n  \\pi(\\tau) &= \\frac{s^r}{\\Gamma(r)} \\tau^{r - 1} e^{-s\\tau} \\\\\n  \\Rightarrow \\pi(\\mu, \\tau) &= \\pi(\\mu) \\pi(\\tau).\n\\end{aligned}\n\\]\nThe joint prior across the parameters is easy to specify because of the independence assumption. Of course, nothing requires we assume the parameters are independent of one another; this was a modeling assumption. A different set of beliefs would lead to a different structure for the prior. For example, the prior given by\n\\[\n\\begin{aligned}\n  \\pi(\\tau) &= \\frac{s^2}{\\Gamma(r)} \\tau^{r - 1} e^{-s\\tau} \\\\\n  \\pi(\\mu \\mid \\tau) &= \\frac{\\sqrt{\\tau}}{\\sqrt{2\\pi}} e^{-\\tau/2 (\\mu - a)^2} \\\\\n  \\Rightarrow \\pi(\\mu, \\tau) &= \\pi(\\mu \\mid \\tau) \\pi(\\tau)\n\\end{aligned}\n\\]\nsuggests a Gamma prior for \\(\\tau\\) and a Normal prior for \\(\\mu\\) conditional on the value of \\(\\tau\\). This hierarchical structure allows the mean \\(\\mu\\) to depend on the precision \\(\\tau\\). The joint distribution of the parameters (prior to seeing the data) is then the product of the marginal distribution of \\(\\tau\\) and the conditional distribution of \\(\\mu \\mid \\tau\\).\nThis process of defining a prior in stages, each stage conditioning on parameters for which a prior distribution is specified, is known as “chaining.”\nRegardless of whether we form a prior through assuming independence or through chaining, the prior predictive distribution (the denominator in Bayes Theorem) will have the form\n\\[\\int \\int f(\\mathbf{y} \\mid \\mu, \\tau) \\pi(\\mu, \\tau) d\\mu d\\tau.\\]\nThe more parameters we have, the more complex the integration in the denominator; this is what motivates the computational methods we examine in the next unit."
  },
  {
    "objectID": "03j-constructing-priors.html#non-informative-priors",
    "href": "03j-constructing-priors.html#non-informative-priors",
    "title": "16  Constructing Prior Distributions",
    "section": "16.4 Non-Informative Priors",
    "text": "16.4 Non-Informative Priors\nEach of the above sections assumes that we have prior information that needs to be encoded into a distribution; we now consider the case when we have very little (or no) prior information. In such a setting, we must determine how we encode “ignorance.”\n\nDefinition 16.4 (Laplace Prior) The Laplace prior, also known as a “flat” prior, considers the form\n\\[\\pi(\\theta) = 1 \\qquad \\forall \\theta \\in \\Theta.\\]\n\n\n\n\n\n\n\nWarning\n\n\n\nFor any unbounded support \\(\\Theta\\), the Laplace prior will be improper; that is,\n\\[\\int \\pi(\\theta) d\\theta = \\infty.\\]\nIn such settings the Laplace prior is not actually a valid density function. This seems like it is breaking all the rules, and to some degree it is, but it is still commonly used.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe Bayes Factor should never be computed when you have an improper prior as the prior odds are not defined since there is no valid probability of each hypothesis a priori.\n\n\nThe Laplace prior is a common default prior when no (or little) prior information is available. However, a flat prior cannot represent total ignorance. When the parameter space is unbounded, notice that a flat prior essentially says that no matter how large of a value \\(q\\) you imagine, \\(Pr\\left(\\theta &gt; q\\right) = \\infty\\) — that is, there is always infinite probability that \\(\\theta\\) is larger than you can imagine.\n\n\n\n\n\n\nBig Idea\n\n\n\nFlat priors are chosen because they are easily overwhelmed by the observed data.\n\n\nThe benefit of a flat prior is that the posterior distribution is proportional to the likelihood. The idea here is to make the Bayesian framework dependent solely on the data, similar to a Frequentist approach (though the two are still not guaranteed to give the same results).\nFlat priors are a subset of a larger class of priors that continues to be an active area of research; this larger class of priors is determined solely by the form of the likelihood.\n\nDefinition 16.5 (Noninformative Prior) A prior distribution which is derived solely based on the form of the likelihood.\n\nA noninformative prior seems like a compromise between Bayesians and those who dislike the subjective nature of a prior distribution. So, why is this not the standard? On the one hand, true Bayesians argue that we should make use of prior information; we should not seek to make use of only the data available in that single study. This allows the information from one study to become the prior information for a follow-up study instead of beginning from scratch. Second, there is a potential pitfall when using noninformative priors when they are improper — it is possible for the posterior distribution to be improper (which is a nice way of saying it is not a distribution at all)! If the posterior distribution is improper, it cannot yield any valid inference on the parameters.\n\n\n\n\n\n\nWarning\n\n\n\nValid inference cannot be made when the posterior distribution is improper.\n\n\nAn improper prior can lead to an improper posterior; however, a proper prior will always lead to a proper posterior. The danger is that software which automates Bayesian analyses currently have no way of checking if a posterior is proper; so, this must be done manually. As computing the posterior can be difficult (the entire reason for the next unit), this often involves bounding the integral in some way — a job for true mathematicians.\nFear around improper priors often leads to what are known as vague priors. This is taking a parametric family and choosing the hyperparameters to result in a massive variance so that the prior distribution, while proper, appears flat over the parameter space. The idea here is to allow the data to easily overwhelm any prior information.\n\n\n\n\n\n\nBig Idea\n\n\n\nNoninformative priors try to make it easy for the likelihood to dominate the prior distribution in the computation of the posterior distribution."
  },
  {
    "objectID": "04a-computation.html",
    "href": "04a-computation.html",
    "title": "Unit IV: Numerical Approaches to Bayesian Computations",
    "section": "",
    "text": "The previous unit discussed the fundamental components of the Bayesian paradigm. We began by specifying the likelihood, a model describing the data generating process as a function of unknown parameters, and a prior distribution, which captures our beliefs about the unknown parameters prior to observing data. Using Bayes Theorem, we were able to characterize our beliefs about the unknown parameter after observing the data in the posterior distribution. We know that the posterior distribution is proportional to the product of the likelihood and the prior; that is,\n\\[\\pi(\\bs{\\theta} \\mid \\bm{x}) \\propto f(\\bm{x} \\mid \\bs{\\theta}) \\pi(\\bs{\\theta}).\\]\nHowever, to determine the exact form of the posterior distribution, we need to determine the scaling constant given by\n\\[\\frac{1}{\\int f(\\bm{x} \\mid \\bs{\\theta}) \\pi(\\bs{\\theta}) d\\bs{\\theta}}.\\]\nUnfortunately, this integral can be intractable, especially as the dimension of \\(\\bs{\\theta}\\) grows. In this unit, we consider numerical approaches for common Bayesian quantities, such as point and interval estimation. With these techniques, we can address more intricate problems."
  },
  {
    "objectID": "04b-mc-integration.html#law-of-large-numbers",
    "href": "04b-mc-integration.html#law-of-large-numbers",
    "title": "17  Monte Carlo Integration",
    "section": "17.1 Law of Large Numbers",
    "text": "17.1 Law of Large Numbers\nLet \\(X_1, X_2,\\dotsc, X_n\\) be independent and identically distributed random variables with density \\(f(x)\\). Consider a real valued function \\(g\\). Then, for any \\(\\epsilon &gt; 0\\) we have that\n\\[Pr\\left(\\lvert\\frac{1}{n}\\sum_{i=1}^n g\\left(X_i\\right) - E\\left[g(X)\\right]\\rvert &gt; \\epsilon\\right) \\rightarrow 0\\]\nas \\(n \\rightarrow \\infty\\), assuming \\(E\\left[g(X)\\right]\\) exists."
  },
  {
    "objectID": "04c-mcmc.html#hamiltonian-monte-carlo",
    "href": "04c-mcmc.html#hamiltonian-monte-carlo",
    "title": "18  Markov Chain Monte Carlo (MCMC)",
    "section": "18.1 Hamiltonian Monte Carlo",
    "text": "18.1 Hamiltonian Monte Carlo\nThe trick to MCMC methods is to choose a transition kernel that is efficient and can handle the myriad of situations encountered in practice. The Metropolis Algorithm, while simplistic, is not very efficient, and can be quite difficult to implement when the dimension of the parameter vector increases. A commonly implemented alternative is known as the Gibbs sampler. This is implemented in the popular Bayesian software packages BUGS (Bayesian inference Using Gibbs Sampling) and JAGS (Just Another Gibbs Sampler). BUGS is a standalone software package while JAGS is implemented in other computing languages (like R and Python). These software packages provide a myriad of algorithms based on the Gibbs sampler which address hierarchical models in a nice way. However, in some complex models, these algorithms can be inefficient or fail to produce variates from the posterior. Stan implements a Hamiltonian Monte Carlo (HMC) algorithm which can succeed in these situations. While the details of the algorithm are beyond the scope of this text, we discuss the ways in which HMC improves upon the Metropolis Algorithm discussed above.\nThe Metropolis Algorithm can be summarized in the following two statements:\n\nChoose the candidate point using a symmetric proposal distribution centered on the current point.\nFavor points with a larger corresponding posterior density, moving to candidate points with lower posterior density probabilistically.\n\nThe key distinction between the Metropolis Algorithm and HMC is to allow the proposal distribution to be dependent upon our current location.\n\n\n\n\n\n\nBig Idea\n\n\n\nHMC uses proposal distributions which favor moving toward the posterior mode.\n\n\nThe idea is illustrated in Figure 18.1, created by John Kruschke (Kruschke 2015), which shows how proposals are generated for two different initial values. Note, the end result in this graphic is not a sample from the posterior but the distribution of potential next steps.\n\n\n\nFigure 18.1: Examples of Hamiltonian Monte Carlo proposal distributions. Two columns show two different current parameter values, marked by a large dot. The first row shows the posterior distribution. The second row shows the potential energy, with a random impulse given to the dot. The third row shows trajectories, which are the theta value (x-axis) as a function of time (y-axis marked Steps*Eps). The fourth row shows histograms of the proposals.\n\n\nTo illustrate how this works, consider two different current positions within a posterior distribution. The Metropolis Algorithm would simply say to generate proposals which are symmetric about the current position. HMC generates proposals that are closer to the posterior mode (as evidenced by the bottom part of the figure where the majority of proposals are near the mode). In order to determine where to move from the current position, the HMC algorithm considers the potential of the position, defined through the negative log-density. The potential gives an idea of how far we might want to travel (the potential of the position to change).\n\nDefinition 18.4 (Potential) The potential of a value \\(\\theta\\) is the negative logarithm of the posterior evaluated at \\(\\theta\\). In practice, we need only know the potential up to a constant. That is, it suffices to define the potential as\n\\[\\text{Potential}(\\theta) = -\\log\\left[f(\\mathbf{y} \\mid \\theta) \\pi(\\theta)\\right].\\]\n\nWhile we have described the potential as a value, since it exists for all \\(\\theta\\) in the support, we can think of the potential as a function (row 2 of Figure 18.1). Now, imagine the current position is a ball on the potential; the proposed position is determined by flicking the ball randomly. This random “flick” is done by selecting a random variable from a Standard Normal distribution, which determines both the magnitude and direction of the flick (negative values move the ball to the left, and positive values move the ball to the right). We then watch the ball roll around for a while. Wherever the ball stops is the proposed position. This is illustrated in the third row of graphics in Figure 18.1 that show how the ball moves over time to the proposed position.\n\n\n\n\n\n\nNote\n\n\n\nThe sum of potential and kinetic energy is known as the Hamiltonian (hence the name of this procedure). The total energy should be conserved at each point in the algorithm.\n\n\nAs the ball rolls around on the potential, it will naturally be drawn to lower points on this surface. That is, candidate points will tend to be drawn from regions with lower potential, corresponding to regions with a higher posterior density. Notice that when the current position is near the posterior mode, the potential positions are nearly symmetric about the current location as in the Metropolis Algorithm. But, if the current position is far from the posterior mode, the potential positions are drawn from regions closer to the posterior mode and the potential positions are far from the current position.\n\n\n\n\n\n\nBig Idea\n\n\n\nThe HMC algorithm generates proposals which tend to have lower potential.\n\n\nWe emphasize that these are just candidate positions. Once a candidate position is identified, we must decide whether to move there or remain in the current position, just as we do in the Metropolis Algorithm.\n\n\n\n\n\n\nDecision Rule for HMC:\n\n\n\nGenerate \\(U \\sim Unif(0,1)\\) and \\(A\\left(\\theta^*, \\theta^{(k-1)}\\right)\\) where\n\\[A\\left(\\theta^*, \\theta^{(k-1)}\\right) = \\frac{f\\left(\\mathbf{y} \\mid \\theta^*\\right) \\pi\\left(\\theta^*\\right) \\omega\\left(\\theta^*\\right)}{f\\left(\\mathbf{y} \\mid \\theta^{(k-1)}\\right) \\pi\\left(\\theta^{(k-1)}\\right) \\omega\\left(\\theta^{(k-1)}\\right)}\\]\nand \\(\\omega(\\cdot)\\) is the momentum. If \\(U \\leq A\\left(\\theta^*, \\theta^{(k-1)}\\right)\\), then we move to the new position; otherwise, we remain in the same position.\n\n\nThe momentum can be thought of as how much speed the ball has when you reach the candidate position (remember, we stop the ball not when it comes to rest but after some fixed amount of time). Recall that we apply a random momentum to the current location of the ball. The aspect we want to emphasize here is that the decision rule is quite similar to the Metropolis Algorithm.\nWe have described this process as letting the “ball roll around” for some fixed set of time. In practice, we emulate this by taking some predefined number of steps of a certain size based on the gradient (much like numeric function minimization). Both the step size and number of steps require some tuning. The step size is tuned to balance how far away from the current position we move and the degree of approximation. If we take small steps, we approximate the curve quite nicely, but we do not get anywhere. If we take large steps, we move away from our current position, but the approximation suffers. The total duration (the number of steps taken) is tuned to ensure we do not overshoot or make a u-turn. If we let the ball roll for too long, it could overshoot the posterior mode by a large degree; or, we may end up stopping the ball when it has rolled back to where it started. Figure 18.2, created by John Kruschke (Kruschke 2015), illustrates the impact of allowing the “time” (number of steps and length of step size) to be too large; notice the difference in the distribution of candidate points in Figure 18.2 compared to Figure 18.1.\n\n\n\nFigure 18.2: Examples of Hamiltonian Monte Carlo proposal distributions for two different current parameter values, marked by the large dots, in the two columns. For this figure, a large range of random trajectory lengths (Steps*Eps) is sampled.\n\n\nIn addition to these tuning parameters, we must determine the standard deviation of the symmetric distribution used to apply the momentum to the current position. This choice needs to balance variety with accuracy. Too small of a standard deviation (like nudging the ball) means it will not roll far from where it started, and every candidate is essentially the same (leading to a higher likelihood of acceptance/rejection). Too large of a standard deviation, and a high degree of candidates will be rejected. Figure 18.3, created by John Kruschke (Kruschke 2015), illustrates the impact of standard deviation in the proposal distribution; notice the difference in the distribution of candidate points between the two columns in Figure 18.3.\n\n\n\nFigure 18.3: Examples of a Hamiltonian Monte Carlo proposal distributions for two different variances of the initial random momentum, indicated in the second row.\n\n\nProper tuning ensures that the algorithm is efficient and a majority of the variates are useful in representing the posterior. These are handled internally by the software, but it is important to have an understanding of what is happening in the background.\nWith MCMC methods, we can address a multitude of more complex problems. We do note the one limitation of Stan is that it does not currently support discrete parameters directly. This is because the HMC algorithm needs a smooth function in order to compute the gradient. Not supporting discrete parameters is not as limiting as it might seem, but it does prohibit automatic model comparison within Stan and eliminates the ability to put a point mass in the prior distribution.\n\n\n\n\n\n\nWarning\n\n\n\nWhile you could write custom implementations of the HMC (or any MCMC) algorithm, software like Stan does the hard work for you. However, in order to make use of those tools, you must specify the Bayesian model (the likelihood and the prior) in addition to providing the data. This can require your learning a new “probability language” (as opposed to a computing language) for specifying such models. Some software packages have pre-built functions/interfaces for commonly specified models allowing you to get started more quickly.\n\n\n\n\n\n\nKruschke, John K. 2015. Doing Bayesian Data Analysis: A Tutorial with r, JAGS, and Stan. 2nd ed. Elsevier."
  },
  {
    "objectID": "04d-mcmc-assessment.html",
    "href": "04d-mcmc-assessment.html",
    "title": "19  Assessing MCMC Samples",
    "section": "",
    "text": "The reliability of any statistical analysis depends on the quality of the data obtained; as a result, any good analysis requires that we give some thought to the data we have obtained. Similarly, we must consider the derivation of our prior distribution and the reasonableness of our model for the likelihood. When our analysis also includes the use of an MCMC algorithm, we should, at a minimum, also investigate that certain assumptions about the resulting sample are reasonable before proceeding. This chapter briefly discusses some checks that are done on the posterior sample to determine its suitability for answering questions.\nThere are essentially four considerations when examining the output of any MCMC algorithm.\n\n\n\n\n\n\nAssessment of an MCMC Algorithm\n\n\n\nBefore using a sample from an MCMC algorithm, the following should be considered:\n\nThe posterior distribution is proper.\nThe resulting Markov Chains converged.\nSensitivity of the algorithm to starting values.\nThe correlation between generated variates is negligible.\n\n\n\nSoftware which implements MCMC algorithms typically generate output for assessing the reliability of the resulting sample. In this chapter, we focus on navigating this output for assessment.\nAn improper posterior distribution cannot provide valid inference. Unfortunately, an MCMC algorithm will generate a sequence of values, even if the target distribution is improper. If the combination of the likelihood and prior specified would result in an improper posterior, the resulting sample from the MCMC algorithm is useless. In general, software is unable to determine if the target distribution is improper; therefore, it is up to the analyst to analytically determine if the posterior distribution is proper. The simplest way to ensure that we have a proper posterior distribution is to use a proper prior distribution.\n\n\n\n\n\n\nEnsuring a Proper Posterior\n\n\n\nIf we use a proper prior, we are guaranteed a proper posterior.\n\n\n\nExample 19.1 (C-section Deliveries Continued) Example 9.1 introduced a study, a component of which includes estimating the probability of a mother undergoing a C-section delivery at a particular hospital.\nWhile MCMC methods are not required for this example, as Example 11.2 illustrates, it is possible to use and MCMC algorithm to address this question. Before even writing MCMC code, explain why we can be confident that the posterior is proper in this case.\n\n\nSolution. Our prior information was represented using a Beta distribution, which is a proper distribution. Therefore, we know the posterior will also be proper.\n\nRecall that we “seed” an MCMC algorithm with an initial value. Only after a large number of iterations is the algorithm generated values from the stationary distribution — the distribution to which the process essentially converges. Therefore, we must ensure we have allowed the algorithm to run long enough that the process has converged to the stationary distribution — that the variates generate behave as if they are drawn from the posterior distribution. A trace plot showing the value of the generated variates at each step of the algorithm can be used to visually assess convergence.\nYou are essentially looking for whether the chain eventually settles in a particular region of the parameter space; this should not be confused with the chain reaching a specific value. As we are looking for a stationary distribution, we expect the variates generated to bounce around (according to the stationary distribution); however, if the chain has some signal to it (trending in location or spread), that is unexpected. It should eventually look like noise around some central point.\nOften, we eliminate the early part of a chain, the discarded portion known as the “warm-up” (or “burn-in”) period. That is, we might remove the first 1000 variates from the resulting Markov chain because we expect the chain is still moving toward the stationary distribution during this time period. The variates after the burn-in period behave more like a sample from the stationary distribution and are retained for analysis.\n\n\n\n\n\n\nGraphically Assessing Convergence of MCMC Chains\n\n\n\nCreate a plot of the values generated (after eliminated the values from the burn-in period) against the order in which they were generated. This is known as a “trace plot.” If the chain has converged, the plot should not have any trends in the location or spread over time.\n\n\n\nExample 19.2 (C-section Deliveries Continued) Example 9.1 introduced a study, a component of which includes estimating the probability of a mother undergoing a C-section delivery at a particular hospital.\nWe used Stan to implement a Hamiltonian Monte Carlo to obtain a sample from the posterior distribution. We generated three chains (each seeded with a different initial value); each chain had 5000 iterations, with the first 2000 representing a burn-in. This provided a sample of size 9000 from the posterior distribution after combining the three chains.\nTable 19.1 summarizes the sample generated by the MCMC algorithm. Figure 19.1 is a trace plot for each of the three chains. Comment on the convergence of each chain.\n\n\n\n\n\n\nTable 19.1: Summary of results from MCMC algorithm used to estimate the rate of C-sections at a hospital. The model was fit with Stan; 3 chains were generated, each with 5000 iterations and a burn-in of 2000 for a total of 9000 post-burn-in variates. The 95% credible interval reported is an equal-tailed interval.\n\n\nPosterior Mean\nPosterior Median\n95% Credible Interval\nESS\nShrink Ratio\n\n\n\n\n0.307\n0.306\n(0.224, 0.397)\n3495.544\n1.001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 19.1: Traceplot from an example estimating the rate of C-sections at a hospital. The burn-in of 2000 iterates is removed from the graphic.\n\n\n\n\n\nSolution. We do not notice any trends in the location or spread of any of the three chains after the burn-in period. Each of the chains seems to bounce around the value 0.3, and the spread stays relatively constant (values generated tend to be between 0.2 and 0.4).\nAs there are no trends in the location or spread, the samples generated by the algorithm are consistent with what we would expect if they had reached the stationary distribution.\n\nIn theory, the stationary distribution is the posterior distribution, we just need to run the algorithm long enough to get there. Practically, however, the distribution to which the algorithm converges could depend on the value used to seed the process. If that happens, then any results based on the sample are potentially biased. Therefore, we want to determine if the MCMC algorithm is sensitive to the chosen starting (initial) value. To do so, we seed the algorithm with multiple starting values, resulting in multiple chains. It is generally sufficient to consider three chains. Overlaying the trace plot from each chain allows us to assess whether the chains “mix” well. If the various chains are distinct, this suggests that the stationary distribution suggested by the algorithm varies according to the starting value, which means a stationary distribution was not really obtained.\nIn addition to the visual check, we can compute the “shrink factor.” Also known as the “potential scale reduction factor,” this is the ratio of the between-chain variability to the within-chain variability. If the chains are well mixed, this ratio should be near 1. If the ratio gets much larger than 1.1, it indicates a serious problem with the mixing.\n\n\n\n\n\n\nAssessing Sensitivity of MCMC Algorithm to Starting Value\n\n\n\nSeed the algorithm with three initial values. If the trace plots of the resulting chains are distinct — occupy different aspects of the parameter space — then your results are sensitive to the starting value. If the chains mix well, the chains are then combined to form a single sample for estimation.\nAlternatively, a shrink factor above 1.1 indicates sensitivity to the starting value.\n\n\n\nExample 19.3 (C-section Deliveries Continued) Example 9.1 introduced a study, a component of which includes estimating the probability of a mother undergoing a C-section delivery at a particular hospital.\nRevisiting Example 19.2 above, assess the sensitivity of the algorithm to the initial value.\n\n\nSolution. Notice that in Figure 19.1, the three chains overlap completely; in fact, it is difficult to distinguish one chain from another. This suggests the chains are mixing well as they occupy the same part of the parameter space.\nReported in Table 19.1, the shrink factor was estimated to be 1, which is consistent with our observations in the graphic above. There is no evidence the algorithm is sensitive to the initial value, and it seems reasonable to combine the variates from the three chains.\n\nRecall that we expect our Markov chain to result in correlated variates. As a result, each variate does not contain as much unique information as we may believe. The “effective sample size” gives a crude measure of how much independent information there is within the chain. For example, we may generate 5000 iterates, but if the variates are highly dependent, the effective sample size may suggest we act as if only 100 iterates were generated.\n\nDefinition 19.1 (Effective Sample Size) The effective sample size (ESS) is given by\n\\[ESS = \\frac{N}{1 + 2\\sum_{k=1}^{\\infty} ACF(k)}\\]\nwhere ACF is the auto-correlation function of degree \\(k\\).\n\n\n\n\n\n\n\nWarning\n\n\n\nWhen discussing MCMC procedures, it is common to use “sample size” to actually refer to the number of variates generated during the MCMC algorithm. We should not confuse the number of variates generated from the posterior with the number of observations in our data set.\n\n\nIf you want to measure something that is toward the center of the distribution (mean/median), the ESS need not be large. But, if you want to compute a tail probability (such as for a credible interval), you need a much larger ESS. Some texts recommend near 10000 variates from the posterior in order to reliably compute a highest posterior density interval, for example.\n\n\n\n\n\n\nAssessing Independence of Variates\n\n\n\nThe effective sample size (ESS) takes into account the correlation between the variates and gives you an indication of how precise your results are. A small ESS suggests high correlation between the variates and indicates that your results are less reliable.\n\n\n\nExample 19.4 (C-section Deliveries Continued) Example 9.1 introduced a study, a component of which includes estimating the probability of a mother undergoing a C-section delivery at a particular hospital.\nRevisiting Example 19.2, assess the independence of the variates.\n\n\nSolution. We see that while we generated 9000 (post burn-in) variates, the effective sample size is estimated to be just under 3500. This suggests that there is a relatively strong correlation between the observations; only every third variate generated is independent. However, the effective sample size still remains fairly large; so, we are confident in both our point estimates and any credible intervals generated.\n\nWhile these four checks are somewhat universal, there is an additional check that can be helpful when using a Hamiltonian Monte Carlo (HMC) algorithm. Recall that the total energy should be conserved during the HMC procedure. A divergent transition occurs when the simulated Hamiltonian departs from the true value (as measured at the initial point). Divergent transitions (after warm-up) indicate the results will be biased. A “pairs plot” allows us to visualize when this occurs. If the amount of error (divergence) is larger than the median, it can often be fixed by increasing the target acceptance rate. If not, then this indicates that the posterior may be very difficult to sample from for this algorithm.\n\n\n\n\n\n\nAssessing Divergent Transitions\n\n\n\nFor the HMC algorithm only, a pairs plot allows us to determine if the amount of divergence is larger than expected. If there are issues, try increasing the target acceptance rate.\n\n\nWith regard to Example 19.2, no divergent iterations were noted among the 9000 variates generated.\nPosterior checks of the MCMC samples are necessary in order to prevent making conclusions that are unreasonable. We have only discussed how to identify problems. Fixing the problems is often dependent upon the specific application. There are several subtleties with each model that take time to learn in order to understand where the algorithm might get hung-up. The fix is often a clever reparameterization of the likelihood or prior in order to help the algorithm. With enough computation, we can overcome many of the problems faced."
  },
  {
    "objectID": "05a-comparing-groups.html",
    "href": "05a-comparing-groups.html",
    "title": "Unit V: Hierarchical Models for Comparing Groups",
    "section": "",
    "text": "Unit III introduced the fundamentals of the Bayesian framework. While the text introduced these ideas in a general setting, the running example through that unit considered modeling a single response variable. In this unit, we consider comparing a response across groups. While the framework remains the same, we discuss additional considerations to be made in these settings."
  },
  {
    "objectID": "05b-study-design.html#two-types-of-studies",
    "href": "05b-study-design.html#two-types-of-studies",
    "title": "20  Elements of Good Study Design",
    "section": "20.1 Two Types of Studies",
    "text": "20.1 Two Types of Studies\nIn order to illustrate how study design can impact the results, consider the following example.\n\nExample 20.1 (Kangaroo Care) At birth, infants have low levels of Vitamin K, a vitamin needed in order to form blood clots. Though rare, without the ability for her blood to clot, an infant could develop a serious bleed. In order to prevent this, the American Academy of Pediatrics recommends that all infants be given a Vitamin K shot shortly after birth in order to raise Vitamin K levels. As with any shot, there is typically discomfort to the infant, which can be very discomforting to new parents.\nKangaroo Care is a method of holding a baby which emphasizes skin-to-skin contact. The child, who is dressed only in a diaper, is placed upright on the parent’s bare chest; a light blanket is draped over the child. Suppose we are interested in determining if utilizing the method while giving the child a Vitamin K shot reduces the discomfort in the infant, as measured by the total amount of time the child cries following the shot.\nWithin this context, contrast the following two potential study designs:\n\nWe allow the attending nurse to determine whether Kangaroo Care is initiated prior to giving the Vitamin K shot. Following the shot, we record the total time (in seconds) the child cries.\n\nWe flip a coin. If it comes up heads, the nurse should have the parents implement Kangaroo Care prior to giving the Vitamin K shot; if it comes up tails, the nurse should give the Vitamin K shot without implementing Kangaroo Care. Following the shot, we record the total time (in seconds) the child cries.\n\nNote, in both study designs (A) and (B), we only consider term births which have no complications to avoid situations that might alter the timing of the Vitamin K shot or the ability to implement Kangaroo Care.\n\nNote that there are some similarities in the two study designs:\n\nThe underlying population is the same for both designs: infants born at term with no complications.\nThere are two groups being compared in both designs: the “Kangaroo Care” group and the “no Kangaroo Care” group.\nThe response (variable of interest) is the same in both designs: the time (in seconds) the infant cries.\nThere is action taken by the researcher in both designs: a Vitamin K shot is given to the child.\n\nThere is one prominent difference between the two study designs:\n\nFor design (A), the choice of Kangaroo Care is left up to the nurse (self-selected); for design (B), the choice of Kangaroo is assigned to the nurse by the researcher, and this selection is made at random.\n\nDesign (A) is an example of an observational study; design (B) is a an example of a controlled experiment.\n\nDefinition 20.1 (Observational Study) A study in which each subject “self-selects” into one of groups being compared in the study. The phrase “self-selects” is used very loosely here and can include studies for which the groups are defined by an inherent characteristic or are chosen haphazardly.\n\n\nDefinition 20.2 (Controlled Experiment) A study in which each subject is randomly assigned to one of the groups being compared in the study.\n\nIt is common to think that anytime the environment is “controlled” by the researcher that a controlled experiment is taking place, but the defining characteristic is the random assignment to groups (sometimes referred to as the factor under study or treatment groups). In the example above, both study designs involved a controlled setting (the delivery room of a hospital) in which trained staff (the nurse) deliver the shot. However, only design (B) is a controlled experiment because the researchers randomly determined which treatment the infant would receive.\nTo understand the impact of random allocation, suppose that we had conducted a study using design (A); further, the results of the study suggest that those infants who were given a shot while using Kangaroo Care cried for a shorter time period, on average. Can we conclude that it was the Kangaroo Care that led to the shorter crying time? Maybe. Consider the following two potential explanations for the resulting data:\n\nKangaroo Care is very effective; as a result, those children who are given Kangaroo Care cry for less time, on average, following the Vitamin K shot.\n\nIt turns out that those nurses who choose to implement Kangaroo Care (remember, they have a choice under design (A) whether they implement the method) are also the nurses with a gentler bedside manner. Therefore, these nurses tend to be very gentle when giving the Vitamin K shot whereas the nurses who choose not to implement Kangaroo Care tend to jab the needle when giving the shot. The reduced crying time is not a result of the Kangaroo Care but the manner in which the shot was given.\n\nThe problem is that we are unable to determine which of the explanations is correct under study design (A). Given the data we have collected, we are unable to tease out the effect of the Kangaroo Care from that of the nurse’s bedside manner. As a result, we are able to say we observed an association between the use of Kangaroo Care and reduced crying time, but we are unable to conclude that Kangaroo Care caused a reduction in the crying time (that is, the reduced crying time may be due to something else, like the bedside manner of the nurse). In this hypothetical scenario, the nurse’s bedside manner is called a confounder.\n\nDefinition 20.3 (Confounding) When the effect of a variable on the response is mis-represented due to the presence of a third, potentially unobserved, variable known as a confounder.\n\n\n\n\n\n\n\nNote\n\n\n\nWhile both result in estimates we may not trust, confounding is not equivalent to a biased sample.\n\n\nConfounders can mask the relationship between the factor under study and the response. Did you know there is a documented association between ice cream sales and the risk of shark attacks? As ice cream sales increase, the risk of a shark attack also tends to increase. This does not mean that if a small city in the Midwest increases its ice cream sales that the citizens are at higher risk of being attacked by a shark. As Figure 20.1 illustrates, there is a confounder — temperature. As the temperatures increase, people tend to buy more ice cream; as the temperature increases, people tend to go to the beach, thereby increasing the risk of a shark attack. The two variables, ice cream sales and shark attacks, appear to be related as a result of the confounder of temperature.\n\n\n\n\n\nFigure 20.1: Illustration of a confounding variable. The confounder, related to both the response and the factor of interest (or treatment) can make it appear as though there is a causal relationship when none exists.\n\n\n\n\n\n\n\n\n\n\nBig Idea\n\n\n\nConfounders are variables that influence both the factor of interest and the response.\n\n\nObservational studies are subject to confounding; thus, controlled experiments are often considered the gold standard in research because they allow us to infer cause-and-effect relationships from the data. Controlled experiments allow for causal interpretations because the random allocation to the levels of the factor removes the impact of confounders. Let’s return to the hypothetical Vitamin-K study in Example 20.1. Suppose there are nurses with a gentle bedside manner and those who are a little less gentle. If we randomly determine which infants receive Kangaroo Care, then for every gentle nurse who is told to implement Kangaroo Care while giving the shot, there tends to be a gentle nurse who is told to not implement Kangaroo Care. Similarly, for every less-gentle nurse who is told to implement Kangaroo Care while giving a shot, there tends to be a less-gentle nurse who is told to not implement Kangaroo Care. This is illustrated in Figure 20.2. For an observational study, the treatment groups can be unbalanced; for example, Figure 20.2 illustrates a case in which there is a higher fraction (11/12 compared to 1/4) of friendly nurses in the Kangaroo Care group compared to the No Kangaroo Care group. For the controlled experiment however, the treatment groups tend to be balanced with respect to this confounder; there is approximately the same fraction of friendly nurses in both groups. Random assignment is the great equalizer. It tends to result in groups which are similar in all respects; therefore, since we have eliminated all other differences between the groups (other than the treatment they receive), any differences we observe between the groups must be due to the grouping and not an underlying confounding variable.\n\n\n\n\n\nFigure 20.2: Illustration of the impact of random assignment in study design. For the observational study, the treatment groups are unbalanced. For the controlled experiment, the treatment groups are balanced.\n\n\n\n\n\n\n\n\n\n\nBig Idea\n\n\n\nRandomly assigning subjects to groups balances the groups with respect to any confounders; that is, the groups being compared are similar. Therefore, any differences between the two groups can be attributed to the grouping itself, leading to cause-and-effect conclusions.\n\n\nWhile controlled experiments are a fantastic study design, we should not discount the use of observational studies. Consider the Deepwater Horizon Case Study described in Chapter 4; suppose we are interested in the following question:\n\nIs there evidence that volunteers who are directly exposed to oil have an increased risk of developing adverse respiratory symptoms compared to those who are not directly exposed to oil?\n\nThe response is whether a volunteer develops adverse respiratory symptoms; the factor of interest is whether the volunteer has direct exposure to oil. We could conduct a controlled experiment by randomly determining which volunteers are assigned to wildlife clean up and which are assigned to administrative tasks, for example. However, it may be that volunteer tasks need to be determined by skillset or by greatest need at the time the person volunteers. It may not be feasible to randomly assign volunteers to specific positions. Or, it could be that the data was obtained after the fact; that is, the data is not the result of a planned study in which case random assignment is not possible because volunteers self-selected into positions in the past. If random assignment is not possible, it does not mean the data is useless. But, it does mean we will need to be sure we acknowledge, and potentially address, the potential confounding when performing the analysis and discussing the results.\nThe big idea is that in order to make causal conclusions, we must be able to state that the groups being compared are balanced with respect to any potential confounders; random assignment is one technique for accomplishing this."
  },
  {
    "objectID": "05b-study-design.html#aspects-of-a-well-designed-study",
    "href": "05b-study-design.html#aspects-of-a-well-designed-study",
    "title": "20  Elements of Good Study Design",
    "section": "20.2 Aspects of a Well-Designed Study",
    "text": "20.2 Aspects of a Well-Designed Study\nWhile controlled experiments are a useful tool, there are many aspects to consider when designing a study. Generally speaking, there are three components to a well-designed study: replication, randomization, and reduction of extraneous noise.\n\n\n\n\n\n\nWarning\n\n\n\nA study is not poor just because it lacks one of these elements. That is, a study can provide meaningful insights even if it did not make use of each of these elements; every study is unique and should be designed to address the research objective. These elements are simply helpful in creating study designs.\n\n\nVariability is inherit in any process. We know there is variability in the population; not every subject will respond exactly the same to each treatment. Therefore, our questions do not seek to answer statements about individuals but about general trends in the population. In order to establish these general trends, we must allow that subject-to-subject variability be present within the study itself. This is accomplished through replication, obtaining data on multiple subjects from each group. Each subject’s response would be expected to be similar, with variability within the group due to the inherent variability in the data generating process.\n\nDefinition 20.4 (Replication) Replication results from taking measurements on different units (or subjects), for which you expect the results to be similar. That is, any variability across the units is due to natural variability within the population.\n\n\n\n\n\n\n\nWarning\n\n\n\nThe term “replication” is also used in the context of discussing whether the results of a study are replicable. While our use of the term is about replicating a measurement process within a study, this does not downplay the importance of replicating an entire study.\n\n\nWhen we talk about gathering “more data,” we typically mean obtaining a larger number of replicates. Ideally, replicates will be obtained through random selection from the underlying population to ensure they are representative. The subjects are then randomly allocated to a particular level of the factor under study (randomly allocated to a group) when performing a controlled experiment. This random allocation breaks the link between the factor and any potential confounders, allowing for causal interpretations. However, random allocation preserves any link between the factor and the response, if a link exists. These are the two aspects of randomization.\n\nDefinition 20.5 (Randomization) Randomization can refer to random selection or random allocation. Random selection refers to the use of a random mechanism (e.g., a simple random sample, Definition 6.2, or a stratified random sample, Definition 6.3) to select units from the population. Random selection minimizes bias.\nRandom allocation refers to the use of a random mechanism when assigning units to a specific treatment group in a controlled experiment (Definition 20.2). Random allocation eliminates confounding and permits causal interpretations.\n\n\n\n\n\n\n\nNote\n\n\n\nWhile those new to study design can typically describe random selection and random allocation, they often confuse their purpose. Random selection is to ensure the sample is representative. Random allocation balances the groups with respect to confounders.\n\n\nIt is tempting to manually adjust the treatment groups to achieve what the researcher views as balance between the groups. This temptation should be avoided as balancing one feature of the subjects may lead to an imbalance in other features. Remember, random allocation leads to balance. Of course, random allocation does not guarantee any particular sample is perfectly balanced; however, any differences are due to chance alone. As the sample size increases, these differences due to chance are minimized.\nEven with random allocation providing balance between the groups, there will still be variability within each group. The more variability present, the more difficult it is to detect a signal — to discern a difference in the mean response across groups. The study will more easily detect the signal if the groups are similar. This leads to the third component of a well-designed study — the reduction of noise.\n\nDefinition 20.6 (Reduction of Noise) Reducing extraneous sources of variability can be accomplished by fixing extraneous variables or blocking (Definition 20.7). These actions reduce the number of differences between the units under study.\n\n\n\n\n\n\n\nTension between Lab Settings and Reality\n\n\n\nScientists and engineers are trained to control unwanted sources of variability (or sources of error in the data generating process). This creates a tension between what is observed in the study (under “lab” settings) and what is observed in practice (in “real-world” settings). This tension always exists, and the proper balance depends on the goals of the researchers.\n\n\nFixing the value of extraneous variables can reduce variability in a study. For example, in Example 20.1, we might choose to conduct the study at a single hospital. This choice impacts the value of an extraneous variable. It is likely each hospital has its own training process and protocols. The choice to only conduct the study at a single hospital reduces the “noise” in how infants respond due to different nurse behavior that reflects hospital training/protocol. However, note that this decision also potentially limits the scope of the study. It may not longer be appropriate to apply these results to all hospitals.\nAn additional tool for reducing noise is blocking, in which observations which are dependent on one another because of a shared characteristic are grouped together.\n\nDefinition 20.7 (Blocking) Blocking is a way of minimizing the variability contributed by an inherent characteristic that results in dependent observations. In some cases, the blocks are the unit of observation which is sampled from a larger population, and multiple observations are taken on each unit. In other cases, the blocks are formed by grouping the units of observations according to an inherent characteristic; in these cases that shared characteristic can be thought of having a value that was sampled from a larger population.\nIn both cases, the observed blocks can be thought of as a random sample; within each block, we have multiple observations, and the observations from the same block are more similar than observations from different blocks.\n\n\nExample 20.2 (Overseeding Golf Greens) Golf is a major pastime, especially in southern states. Each winter, the putting greens need to be overseeded with grasses that will thrive in cooler weather. This overseeding can affect how the ball rolls along the green. Dudeck and Peeacock (1981) reports on an experiment that involved comparing the ball roll for greens seeded with one of five varieties of rye grass. Ball roll was measured by the mean distance (in meters) that five balls traveled on the green. In order to induce a constant initial velocity, each ball was rolled down an inclined plane.\nBecause the distance a ball rolls is influenced by the slope of the green, 20 greens were placed into four groups in such a way that the five greens in the same group had a similar slope. Then, within each of these four groups, each of the five greens was randomly assigned to be overseeded with one of the five types of Rye grass. The average ball roll was recorded for each of the 20 greens.\n\nThe data from Example 20.2 are shown in Table 20.1.\n\n\n\n\n\nTable 20.1: Data from the Overseeding Golf Greens example.\n\n\nRye Grass Variety\nSlope of Green Grouping\nMean Distance Traveled (m)\n\n\n\n\nA\n1\n2.764\n\n\nB\n1\n2.568\n\n\nC\n1\n2.506\n\n\nD\n1\n2.612\n\n\nE\n1\n2.238\n\n\nA\n2\n3.043\n\n\nB\n2\n2.977\n\n\nC\n2\n2.533\n\n\nD\n2\n2.675\n\n\nE\n2\n2.616\n\n\nA\n3\n2.600\n\n\nB\n3\n2.183\n\n\nC\n3\n2.334\n\n\nD\n3\n2.164\n\n\nE\n3\n2.127\n\n\nA\n4\n3.049\n\n\nB\n4\n3.028\n\n\nC\n4\n2.895\n\n\nD\n4\n2.724\n\n\nE\n4\n2.697\n\n\n\n\n\n\n\n\n\nIt would have been easy to simply assign 4 greens to each of the Rye grass varieties; the random allocation would have balanced any confounders across the five varieties. However, an additional layer was added to the design in order to control some of that additional variability. In particular, greens with similar slopes were grouped together; then, the random allocation to Rye grass varieties happened within the grouped greens. The blocks in this study are the “slope groups.” Each block represents greens with a different slope. Certainly, there are more than 4 potential slopes that a green might have; yet, we observed 4 such groups in our study. We can think of these 4 observed slopes as a sample of all slopes that might exist on a putting green.\nWithin each block, we have five units of observations; these five units were randomized to the five treatment groups (the five Rye grass varieties). Notice the random allocation strategy ensures that each variety appears exactly once within each slope grouping. This study design will allow us to compare the impact of the Rye grass variety while minimizing the extraneous variability due to the slope of the green, which is a nuisance characteristic. To see how we capitalize on blocking in the analysis, we refer you to Unit IV of the text.\n\n\n\n\n\n\nNote\n\n\n\nBlocking is often a way of gaining additional power when limited resources require your study to have a small sample size.\n\n\nAn extreme case of blocking occurs when you repeatedly measure the response on the same subject under different treatment conditions. For example, a pre-test/post-test study is an example of a study which incorporates blocking. In this case, the blocks are the individual subjects, the unit of observation. The response is then observed on the subject both prior to the intervention (the “test”) and following the intervention. The rationale here is to use every subject as his or her own “control.” This reduces extraneous noise because the two treatment groups (the pre-test group and the post-test group) are identical.\n\n\n\n\n\n\nBig Idea\n\n\n\nA block is a secondary grouping variable present during the data collection that records a nuisance characteristic. While it reduces extraneous noise in the sample, the block must be accounted for appropriately during the analysis of the data (as described in Chapter 22)."
  },
  {
    "objectID": "05b-study-design.html#collecting-observational-data",
    "href": "05b-study-design.html#collecting-observational-data",
    "title": "20  Elements of Good Study Design",
    "section": "20.3 Collecting Observational Data",
    "text": "20.3 Collecting Observational Data\nAn inability to conduct a controlled experiment does not mean we neglect study design. Random sampling is still helpful in ensuring that the data is representative of the population. And, even if random sampling is not feasible, we should still aim to minimize bias and have a sample that is representative of our population. Similarly, ensuring there are a sufficient number of replications to capture the variability within the data is an important aspect of conducting an observational study. When collecting observational data, one of the most important steps is constructing a list of potential confounders and then collecting data on these variables as well. This will allow us to account for these confounders in our analysis (see Chapter 24); we cannot model what we do not collect. Finally, observational studies may still permit the blocking of subjects and accounting for this additional variability in our analysis.\n\n\n\n\nDudeck, A E, and C H Peeacock. 1981. “Effects of Several Overseeded Ryegrasses on Turf Quality, Traffic Tolerance and Ball Roll.” In Proceedings of the Fourth International Turfgrass Research Conference, edited by R W Sheard, 75–81."
  },
  {
    "objectID": "05c-independent-groups.html#bridge-sampling",
    "href": "05c-independent-groups.html#bridge-sampling",
    "title": "21  Considerations when Comparing Independent Groups",
    "section": "21.1 Bridge Sampling",
    "text": "21.1 Bridge Sampling\nIn Chapter 15 we introduced the idea of model comparison. This is especially useful when comparing groups. Consider a simple case in which we have two groups. Specifically, suppose\n\\[\n\\begin{aligned}\n  Y_{1,i} &\\stackrel{IID}{\\sim} f(y \\mid \\theta_1) \\\\\n  Y_{2,i} &\\stackrel{IID}{\\sim} f(y \\mid \\theta_2) \\\\\n  Y_{1,i} &\\perp\\negthickspace\\negmedspace\\perp Y_{2,j} \\quad \\forall i, j\n\\end{aligned}\n\\]\nwhere \\(\\perp\\negthickspace\\negmedspace\\perp\\) refers to independence. In this example, the data generating process for each group is distinguished only by a potentially different value of the parameter. It would be natural in such settings to consider the hypotheses\n\\[H_0: \\theta_1 = \\theta_2 \\qquad \\text{vs.} \\qquad H_1: \\theta_1 \\neq \\theta_2.\\]\nIf we place a continuous prior on the parameters, then the probability of \\(H_0\\) must be 0. One way of addressing this problem is to consider a clever prior which places mass along the line equating the two parameters. A more natural solution, however, is to think of this as a model comparison problem:\n\\[\n\\begin{aligned}\n  \\mathcal{M}_0:& \\quad Y_{j, i} \\stackrel{IID}{\\sim} f(y \\mid \\theta) \\\\\n    & \\quad \\theta \\sim \\pi_0(\\theta) \\\\\n  \\mathcal{M}_1:& \\quad Y_{j, i} \\stackrel{Ind}{\\sim} f(y \\mid \\theta_j) \\\\\n    & \\quad \\theta_j \\stackrel{Ind}{\\sim} \\pi_j(\\theta_j)\n\\end{aligned}\n\\]\nfor \\(j = 1, 2\\). Notice that under model \\(\\mathcal{M}_0\\), there is only a single parameter, capturing the null hypothesis \\(\\theta_1 = \\theta_2\\). And, model \\(\\mathcal{M}_1\\) allows the flexibility for two parameters, capturing the alternative hypothesis. We would therefore be interested in computing a Bayes Factor (see Definition 15.2) comparing these two models. The problem is, the Bayes Factor depends on the evidence\n\\[f(\\mathbf{y} \\mid \\mathcal{M}_j) = \\int f(\\mathbf{y} \\mid \\boldsymbol{\\theta}, \\mathcal{M}_j) \\pi(\\boldsymbol{\\theta} \\mid \\mathcal{M}_j) d\\boldsymbol{\\theta}.\\]\nUnfortunately, this integral is quite difficult to estimate. Using the prior distribution and performing MC Integration techniques does not typically result in reliable estimation; this results from the prior distribution often being too vague. Instead, the evidence is estimated from the posterior distribution using bridge sampling.\n\nDefinition 21.1 (Bridge Sampling) The bridge sampling estimator of the marginal likelihood \\(m(\\mathbf{y})\\) is given by\n\\[\n\\begin{aligned}\n  m(\\mathbf{y})\n    &= \\int f(\\mathbf{y} \\mid \\boldsymbol{\\theta}) \\pi(\\boldsymbol{\\theta}) d\\boldsymbol{\\theta} \\\\\n    &= \\frac{E_g\\left[h(\\boldsymbol{\\theta}) f(\\mathbf{y} \\mid \\boldsymbol{\\theta}) \\pi(\\boldsymbol{\\theta})\\right]}{E_{\\pi}\\left[h(\\boldsymbol{\\theta}) g(\\boldsymbol{\\theta}) \\right]} \\\\\n    &\\approx \\frac{m^{-1}\\sum_{j=1}^{m} h\\left(\\tilde{\\boldsymbol{\\theta}}_j\\right) f\\left(\\mathbf{y} \\mid \\tilde{\\boldsymbol{\\theta}}_j\\right) \\pi\\left(\\tilde{\\boldsymbol{\\theta}}_j\\right)}{m^{-1}\\sum_{i=1}^{m} h\\left(\\boldsymbol{\\theta}^*_j\\right) g\\left(\\boldsymbol{\\theta}^*_j\\right)}\n\\end{aligned}\n\\]\nwhere \\(h(\\boldsymbol{\\theta})\\) is called the bridge function and \\(g(\\boldsymbol{\\theta})\\) is the proposal distribution. Here, \\(\\tilde{\\boldsymbol{\\theta}}\\) denotes a random variate from the proposal distribution and \\(\\boldsymbol{\\theta}^*\\) a random variate from the posterior; \\(E_g\\) denotes taking an expectation with respect to the proposal distribution and \\(E_\\pi\\) denotes taking an expectation with respect to the posterior distribution.\n\n\n\n\n\n\n\nNote\n\n\n\nWhile not required, it is typical to use a Normal distribution for the proposal distribution \\(g(\\boldsymbol{\\theta})\\).\n\n\nBridge sampling is available in some software packages (such as R, for example). While the details of bridge sampling are beyond the scope of this text, we do want to note that the definition comes from making the following observation:\n\\[\n\\begin{aligned}\n  1 &= \\frac{\\int f(\\mathbf{y} \\mid \\boldsymbol{\\theta}) \\pi(\\boldsymbol{\\theta}) g(\\boldsymbol{\\theta}) h(\\boldsymbol{\\theta})d\\boldsymbol{\\theta}}{\\int f(\\mathbf{y} \\mid \\boldsymbol{\\theta}) \\pi(\\boldsymbol{\\theta}) g(\\boldsymbol{\\theta}) h(\\boldsymbol{\\theta})d\\boldsymbol{\\theta}} \\\\\n  \\Rightarrow m(\\mathbf{y}) &= m(\\mathbf{y}) \\frac{\\int f(\\mathbf{y} \\mid \\boldsymbol{\\theta}) \\pi(\\boldsymbol{\\theta}) g(\\boldsymbol{\\theta}) h(\\boldsymbol{\\theta})d\\boldsymbol{\\theta}}{\\int f(\\mathbf{y} \\mid \\boldsymbol{\\theta}) \\pi(\\boldsymbol{\\theta}) g(\\boldsymbol{\\theta}) h(\\boldsymbol{\\theta})d\\boldsymbol{\\theta}}.\n\\end{aligned}\n\\]\nWe are able to multiply both sides by the prior predictive distribution because it will be non-negative on its support. Next, recognize that the integral is with respect to \\(\\boldsymbol{\\theta}\\); therefore, we can move the marginal distribution inside the integral in the denominator to get\n\\[\n\\begin{aligned}\n  m(\\mathbf{y}) &= \\frac{\\int f(\\mathbf{y} \\mid \\boldsymbol{\\theta}) \\pi(\\boldsymbol{\\theta}) h(\\boldsymbol{\\theta}) g(\\boldsymbol{\\theta})d\\boldsymbol{\\theta}}{\\int \\frac{f(\\mathbf{y} \\mid \\boldsymbol{\\theta}) \\pi(\\boldsymbol{\\theta})}{m(\\mathbf{y})} h(\\boldsymbol{\\theta}) g(\\boldsymbol{\\theta})d\\boldsymbol{\\theta}} \\\\\n    &= \\frac{\\int f(\\mathbf{y} \\mid \\boldsymbol{\\theta}) \\pi(\\boldsymbol{\\theta}) h(\\boldsymbol{\\theta}) g(\\boldsymbol{\\theta})d\\boldsymbol{\\theta}}{\\int \\pi(\\boldsymbol{\\theta} \\mid \\mathbf{y}) h(\\boldsymbol{\\theta}) g(\\boldsymbol{\\theta})d\\boldsymbol{\\theta}},\n\\end{aligned}\n\\]\nwhere the last equality makes use of the definition of the posterior distribution from Bayes Theorem. Now, the top integral can be viewed in terms of an expectation over a random variable \\(\\boldsymbol{\\theta} \\sim g(\\boldsymbol{\\theta})\\), and the denominator can be viewed as an expectation over a random variable which follows the posterior distribution.\n\n\n\n\n\n\nBig Idea\n\n\n\nBridge sampling is an efficient algorithm for estimating the evidence of a model, allowing for computation of the Bayes Factor."
  },
  {
    "objectID": "05d-dependent-groups.html",
    "href": "05d-dependent-groups.html",
    "title": "22  Considerations when Comparing Related Groups",
    "section": "",
    "text": "The previous chapter was a natural extension of the framework we had developed in earlier portions of the text. Assuming each group is independent, we were essentially able to model each group separately; the likelihood was then the product of the likelihoods from each group, and the prior was the product of the priors from each group. This independence will actually carry through into the posterior distribution. That is, the independence allows us to model the groups separately and then combine afterwards. However, independence between the groups is not always reasonable.\nRecall that one of the aspects of a good study design is comparative groups — treatment groups which are similar except for the treatment being applied. The benefit of this is that it reduces extraneous variability. We also saw that blocking (Definition 20.7) is a useful strategy for reducing extraneous variability that groups together observations which share some inherent characteristic. A very common example of blocking is a pre/post test. In such settings, participants are given a baseline assessment. Then, each participant is exposed to some form of treatment; following this, participants take an another assessment. Interest is typically on quantifying the change from baseline.\nIn this case, the two groups (pre-treatment and post-treatment) are not only similar, they are identical! Intuitively, this is a good design because we are allowing every individual to act as their own control. We have eliminated all other external sources of variability allowing us to focus on the treatment of interest. Here, the individual participants act as the blocks. In such cases, we believe the variability among observations between blocks is greater than the variability among observations within a block. Unfortunately, this causes a relationship among the observations, meaning it is no longer reasonable to assume the observations are independent of one another.\n\n\n\n\n\n\nBig Idea\n\n\n\nWhen subjects can be blocked (or pooled) into similar groups, independence is no longer reasonable.\n\n\nWhen we believe there are clusters of related observations, we lean on the hierarchical nature of the data generating process, and this allows us to construct the likelihood.\n\nExample 22.1 (Final Exams) Common final exams are typical for multiple sections of the same course at a university. For example, there may be four instructors, each teaching two sections of Calculus; all eight sections will receive the same final exam. Suppose each exam is graded out of 100 points and we are interested in modeling the exam score for students taking the exam.\n\nWe might start off by assuming a common distribution for all students. That is,\n\\[\n\\begin{aligned}\n  Y_i \\mid \\boldsymbol{\\theta} &\\stackrel{IID}{\\sim} f(y \\mid \\boldsymbol{\\theta}) \\\\\n  \\theta &\\sim \\pi(\\theta).\n\\end{aligned}\n\\]\nwhere \\(Y_i\\) is the exam score for the \\(i\\)-th subject. However, this model is imposing a particular assumption — there are no differences among instructors that would impact the scores of the students. Essentially, every instructor delivers the same content in the same way suggesting the students might has well have been taught by the same instructor. While this is a nice ideal, it is typically not the case. Autonomy in the classroom means that different instructors approach the material differently — emphasizing different topics and presenting the material in different ways. As a result, it is possible that students who share an instructor are more likely to perform well on the same types of problems and also more likely to make the same types of mistakes (compared with students who do not share the same instructor). The additional variability introduced by the differences across instructors is being ignored in this model.\nWe might try to correct for this by assuming the instructors are completely independent of one another, following the approach of the previous chapter. This would say\n\\[\n\\begin{aligned}\n  Y_{j, i} \\mid \\boldsymbol{\\theta}_j &\\stackrel{IID}{\\sim} f(y \\mid \\boldsymbol{\\theta}_j) \\\\\n  Y_{r, s} &\\perp\\negthickspace\\negmedspace\\perp Y_{t, u} \\quad \\forall r, s, t, u \\\\\n  \\boldsymbol{\\theta}_j &\\stackrel{Ind}{\\sim} \\pi_j(\\boldsymbol{\\theta}_j).\n\\end{aligned}\n\\]\nHowever, this model has very limited utility in practice. If we wanted to predict the exam score of a student taking calculus, our model would only be applicable if they were taking it with one of these four instructors! Given another student, we would need to know which of these instructors they were taking the course with in order to know which of the four parameters to lean on in predicting their score. That is, this model inherently compares instructors; but, we do not actually care about comparing Instructor 1 and Instructor 2. Further, the next time the course is offered, there are likely to be four completely new instructors, and we want our model to be accommodate this. Perhaps more importantly, we do not actually think the instructors are completely unrelated (they are teaching the same major content after all…at least, we would hope so). So, this extreme perspective also seems to ignore the structure in the problem.\n\n\n\n\n\n\nBig Idea\n\n\n\nHow we model the relationship between groups, if one exists at all, must be based on the context of the problem.\n\n\nIt seems natural to think of these four instructors as a representative sample from a population of all potential instructors. That is, we are adding a layer to the data generating process. First, instructors are chosen to teach the course; then, students are placed with an instructor, impacting their performance on the final exam. The way these instructors impact the data generating process is through the formation of the parameters \\(\\boldsymbol{\\theta}_j\\). Therefore, we incorporate this into the model. Specifically, consider\n\\[\n\\begin{aligned}\n  Y_{j, i} \\mid \\boldsymbol{\\theta}_j &\\stackrel{IID}{\\sim} f(y \\mid \\boldsymbol{\\theta}_j) \\\\\n  \\boldsymbol{\\theta}_j \\mid \\boldsymbol{\\eta} &\\stackrel{IID}{\\sim} \\pi(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\eta}) \\\\\n  \\boldsymbol{\\eta} &\\sim \\pi(\\boldsymbol{\\eta}).\n\\end{aligned}\n\\]\nThis model has three layers:\n\nLayer 1: Describes how students within an instructor perform; the parameters are unique to each instructor but shared across students with the same instructor.\nLayer 2: Describes the variability across instructors; treating the instructors as a random sample of all instructors, we allow the parameters to move across instructors according to some overall model.\nLayer 3: Describes our prior beliefs about the shared parameters for the instructor-level model.\n\nThis hierarchical model bridges the gap between ignoring the blocks and treating the blocks as independent groups. This allows us to pool information similar across blocks while allowing unique properties to exist for each block as well.\n\n\n\n\n\n\nHierarchical Approach to Addressing Dependence\n\n\n\nWhen the dependence in data is the result of a hierarchical data generating process, we can incorporate that additional variability by modeling the hierarchical structure directly. At a minimum, this has the following three layers:\n\nLayer 1: Describes how observations within a block behave; with parameters unique to each block.\nLayer 2: Describes how the parameters move across blocks; this views the blocks as a random sample.\nLayer 3: Describes our prior beliefs on the common parameters for the model in Layer 2.\n\n\n\nIt is natural to question the difference between a block and a factor (or the grouping comparison that we considered in the previous chapter). The difference is primarily in how we address them in the modeling. If we are interested in making group-to-group comparisons, and if the groups would remain the same when repeating the study, the variable should be treated as a factor of interest. If the groups primarily capture an additional source of variability and can be viewed as a random sample from some larger population, the variable should be treated as a block.\nOf course, we can combine the two ideas in a single study. Suppose further that we were interested in comparing students majoring in mathematics and those not majoring in mathematics. The first layer of the model must still describe what happens within a block (or within an instructor in our example). The assumptions you are willing to make determine how complex this becomes. For example, assuming that the difference between mathematics majors and non-mathematics majors is similar regardless of the instructor implies that mathematics majors share some common parameter across instructors. However, allowing the difference to vary across instructors implies that there is no common parameter. This is addressed within the first two layers of the model. If the effect is held constant across the blocks, a single set of “global” parameters enters into Layer 1 for which priors are described in Layer 3. If the effect is allowed to vary, then the way in which the parameters vary is defined in Layer 2. To broaden our model above, we might have something of the form\n\\[\n\\begin{aligned}\n  Y_{j, i} \\mid \\boldsymbol{\\theta}_j, \\boldsymbol{\\phi} &\\stackrel{IID}{\\sim} f(y \\mid \\boldsymbol{\\theta}_j, \\boldsymbol{\\phi}) \\\\\n  \\boldsymbol{\\theta}_j \\mid \\boldsymbol{\\eta} &\\stackrel{IID}{\\sim} \\pi(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\eta}) \\\\\n  \\boldsymbol{\\eta} &\\sim \\pi(\\boldsymbol{\\eta}) \\\\\n  \\boldsymbol{\\phi} &\\sim \\pi(\\boldsymbol{\\phi}).\n\\end{aligned}\n\\]\nIn this formulation, \\(\\boldsymbol{\\phi}\\) represents parameters which are constant across the blocks; so, they are separated out from \\(\\boldsymbol{\\theta}_j\\) as they do not vary across blocks. We place a prior directly on these parameters in Layer 3.\nNote that we have been intentionally vague about the modeling structure by leaving everything in terms of \\(f\\) instead of defining a specific model. The form of the model will change depending on the data generating process. Further, which parameters in that family may be altered. For example, suppose we considered a Normal distribution for the data generating process; we could allow the mean response to vary across groups, the variability to vary across groups, both the mean response and variability to vary across groups, or hold both the mean response and variability to be similar across groups. This is true for both the blocks as well as the treatment groups! That is, our modeling should be specific to the question at hand and the data generating process being considered."
  },
  {
    "objectID": "06a-regression.html",
    "href": "06a-regression.html",
    "title": "Unit VI: Introduction to Regression Modeling",
    "section": "",
    "text": "The heart of this text (Unit III) introduced the fundamentals of the Bayesian framework for performing inference. We then allowed the distribution of the response to vary across a finite number of groups by allowing the unknown parameters that govern the likelihood to potentially differ for each group (Unit IV). We now consider those unknown parameters that govern the distribution of the response to depend on one or more predictors through some functional form.\nThis unit provides a brief overview of considerations for such “regression” models. We take a very general approach, considering both quantitative and categorical response variables, quantitative and categorical predictors, and various functional forms for how these predictors impact the parameters governing the likelihood."
  },
  {
    "objectID": "06b-linear-regression.html#developing-a-model",
    "href": "06b-linear-regression.html#developing-a-model",
    "title": "23  Regression Models for a Quantitative Response",
    "section": "23.1 Developing a Model",
    "text": "23.1 Developing a Model\nIn many introductory statistics courses, statistical models are typically introduced in the following generic form:\n\\[\\text{Response} = \\text{Signal} + \\text{Noise}.\\]\nThe “response” is the variable we would like to explain or predict. The “signal” represents the part of the data generating process we can explain; it is the deterministic portion of the model and is a function of the predictor(s). The “noise” represents the stochastic portion capturing the variability observed beyond what can be explained by the deterministic portion alone. The common starting point for such a model is the simple linear regression model:\n\\[(\\text{Response})_i = \\beta_0 + \\beta_1 (\\text{Predictor})_i + \\varepsilon_i.\\]\nThis model views the deterministic portion of the model as a line. Notice that any two subjects with the same value of the predictor would have the same value for the deterministic portion of the model; however, the two subjects will not necessarily have the same response as the noise \\(\\varepsilon_i\\) can differ from one subject to the next. Of course, this model is too vague to be helpful; so, we place additional constraints on the stochastic portion. In particular, we place conditions on the distribution of \\(\\varepsilon_i\\). For example, we might assume that \\(E\\left(\\varepsilon_i\\right) = 0\\) or that the \\(\\varepsilon_i\\)’s are identically distributed.\nNotice that this approach, while reasonable, does not align with the approach we have taken thus far in the course. Instead of thinking of the response as a signal plus noise and then placing conditions on the distribution of the noise, we have considered fully specifying the distributional form of the data generating process. That is, we specified the likelihood \\(f(\\mathbf{y} \\mid \\boldsymbol{\\theta})\\). This approach is preferred in the Bayesian perspective (and in the classical perspective in our opinion) because it generalizes more easily and provides a unifying framework for inference. It is within this context of fully specifying the likelihood that we consider developing regression models.\n\nDefinition 23.1 (Regression) A regression model is one for which the parameter(s) governing the data generating process depends on one or more predictors. “Parametric” regression models do this through specifying a functional form for the dependence of the parameter(s) on the predictor(s).\n\nTo illustrate the scope of Definition 23.1, let’s consider several potential models we might consider in the Bayesian framework. Consider\n\\[(\\text{Response})_i \\mid \\beta_0, \\beta_1, \\sigma^2 \\stackrel{\\text{Ind}}{\\sim}N\\left(\\beta_0 + \\beta_1 (\\text{Predictor})_i, \\sigma^2\\right). \\tag{23.1}\\]\nNotice that Equation 23.1 allows the mean response to depend on the predictor through the form\n\\[\\beta_0 + \\beta_1 (\\text{Predictor})_i.\\]\nThat is, instead of a single mean response \\(\\mu\\), the mean response is determined only after first specifying the value of the predictor. As the mean response differs for each subject, depending on their value of the predictor, the responses are not identically distributed. However, we have retained the assumption that the responses are independent of one another (given the unknown parameters). Further, it is only the mean response that is impacted by the predictor. The variance of the response is not impacted but remains constant across all observations. This model also fully specifies the distributional form of the response — it is a Normal distribution.\nIn contrast to Equation 23.1, consider\n\\[(\\text{Response})_i \\mid \\alpha, \\gamma_0, \\gamma_1 \\stackrel{\\text{Ind}}{\\sim}Gamma\\left(\\alpha (\\text{Predictor 1})_i^2, \\gamma_0e^{\\gamma_1 (\\text{Predictor 2})_i}\\right), \\tag{23.2}\\]\nwhich says the responses follow a Gamma distribution where the shape parameter depends on “Predictor 1” through a quadratic relationship; and, the rate parameter depends on “Predictor 2” through an exponential function. Again, while the responses are not identically distributed, they remain independent. Both Equation 23.1 and Equation 23.2 extend common distributional models by allowing the parameters to depend on predictors. The model that is appropriate should be driven by the research objectives and discipline expertise.\nTo get a sense of how model construction is related to the research objectives, let’s return to Equation 23.1. Notice that by replacing the mean with a functional form, we have introduced new parameters into the distribution: \\(\\beta_0\\) and \\(\\beta_1\\). These parameters govern the mean response. As a result, their interpretation is tied to the mean response:\n\n\\(\\beta_0\\) represents the mean response when the value of the predictor is 0.\n\\(\\beta_1\\) represents the change in the mean response when the predictor is increased by 1 unit.\n\nNotice that the interpretation of the parameters could be related to specific scientific questions. If we are interested in how changes in the predictor relate with changes in the response, we are interested in \\(\\beta_1\\).\nThe interpretation of \\(\\alpha\\) in Equation 23.2 is not as clear. It represents the shape parameter when the first predictor takes a value of 1; since the shape parameter is not directly interpretable in terms of the response (like the mean or variance is), this interpretation is less satisfying. But, we know that \\(\\frac{\\alpha}{\\gamma_0}\\) would represent the average response when the first predictor takes a value of 1 and the second predictor takes a value of 0. That is, through a combination of the parameters, we have some sense of the mean response.\nOf course, there are infinitely many other models we could specify. The distributional family could vary (Normal, Gamma, Beta, a mixture, etc.); the functional form could vary (linear, exponential, sinusoidal, etc.); the parameters impacted could vary (mean, shape, rate, etc.). What is common to each of these potential models is that we specify the distribution of the response allowing key aspects of the distribution to vary as functions of the predictor(s).\nSpecifying the distribution of the response is only a portion of the model under the Bayesian framework. Returning to the distributional family specified in Equation 23.1, the assumption of independence allows us to easily construct the likelihood, given by\n\\[\n\\begin{aligned}\n  f\\left(\\mathbf{Response} \\mid \\beta_0, \\beta_1, \\sigma^2\\right)\n    &= \\prod_{i=1}^{n} f\\left((\\text{Response})_i \\mid \\beta_0, \\beta_1, \\sigma^2\\right) \\\\\n    &= \\left(2\\pi \\sigma^2\\right)^{-n/2} \\exp\\left\\{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} \\left((\\text{Response})_i - \\beta_0 - \\beta_1 (\\text{Predictor})_i\\right)^2\\right\\}.\n\\end{aligned}\n\\]\nWe present this to show that the likelihood can quickly become difficult to work with as the complexity of the distributional model grows."
  },
  {
    "objectID": "06b-linear-regression.html#simple-extensions",
    "href": "06b-linear-regression.html#simple-extensions",
    "title": "23  Regression Models for a Quantitative Response",
    "section": "23.2 Simple Extensions",
    "text": "23.2 Simple Extensions\nAs previously stated, the distributional model in Equation 23.1 represents a common model for introducing regression. However, there are two simple extensions that are worth considering. First, we consider the inclusion of multiple predictors. For example, given \\(p\\) predictors, we might posit that\n\\[(\\text{Response})_i \\mid \\boldsymbol{\\beta}, \\sigma^2 \\stackrel{\\text{Ind}}{\\sim}N\\left(\\beta_0 + \\sum_{j=1}^{p} \\beta_j (\\text{Predictor } j)_i, \\sigma^2\\right) \\tag{23.3}\\]\nfor \\(j = 1, 2, \\dots, p\\), giving a likelihood of\n\\[f\\left(\\mathbf{Response} \\mid \\boldsymbol{\\beta}, \\sigma^2\\right) = \\left(2 \\pi \\sigma^2\\right)^{-n/2} \\exp\\left\\{- \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} \\left((\\text{Response})_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j (\\text{Predictor } j)_i\\right)^2\\right\\}.\\]\nThis model allows several predictors to impact the mean response.\n\n\n\n\n\n\nNote\n\n\n\nWhile it is common to have the predictors enter the model such that the mean response is linear in the parameters, this is not a requirement. We could easily allow the predictors to impact the mean response through some nonlinear function.\n\n\nSecond, nothing requires that the response follow a Normal distribution. For example, we might posit that\n\\[(\\text{Response})_i = \\beta_0 + \\beta_1 (\\text{Predictor})_i + \\varepsilon_i,\\]\nwhere \\(\\varepsilon_i \\stackrel{\\text{IID}}{\\sim}t_{\\nu}\\). While we have specified this in a “signal plus noise” form, notice that we are simply saying the response can be modeled by a t-distribution which has been shifted to be centered on \\(\\beta_0 + \\beta_1 (\\text{Predictor})_i\\).\nBoth of these extensions are complex from a classical perspective, requiring different machinery to be able to conduct inference. However, from the Bayesian perspective, we have specified a different model, but the process for performing inference remains exactly the same: specify a distribution to capture the prior information (see Chapter 25) and then compute the posterior distribution (typically using MCMC methods)."
  },
  {
    "objectID": "06b-linear-regression.html#fixed-vs.-random-predictors",
    "href": "06b-linear-regression.html#fixed-vs.-random-predictors",
    "title": "23  Regression Models for a Quantitative Response",
    "section": "23.3 Fixed vs. Random Predictors",
    "text": "23.3 Fixed vs. Random Predictors\nYou may have noticed that Equation 23.1 only specified the distribution of the response variable as a function of the predictor; it omitted the distribution of the predictor itself. For a designed experiment in which the values of all predictors are determined in advance by the researchers, the predictors are constants. As such, the notation of Equation 23.1 is appropriate. However, in many situations, the predictors are not fixed in advanced but observed during the data collection; that is, the predictors can also be viewed as observed values of random variables which vary across individuals in the population. Consider the Rehabilitation example (Example 23.1) above; the age of each patient is unknown prior to the study. We expect the age to vary across individuals who have undergone knee replacement; therefore, age has a distribution across the population that should be modeled.\nIt is common in a regression analysis to condition on the predictors when making inference. Consider Equation 23.1; if we believe the predictor is not determined in advance, but we condition on the value of the predictor, then our model would be expressed as\n\\[(\\text{Response})_i \\mid (\\text{Predictor})_i, \\beta_0, \\beta_1, \\sigma^2 \\stackrel{\\text{Ind}}{\\sim}N\\left(\\beta_0 + \\beta_1 (\\text{Predictor})_i, \\sigma^2\\right). \\tag{23.4}\\]\nThe likelihood for this model is equivalent to what we had before; in fact, nothing about our analysis changes. However, conceptually, we are saying that our model applies only after knowing the value of the predictor. As a result, the model does not specify the distribution of the predictor, and all interpretations assume we have access to the predictor prior to making a statement about the response.\nWhile it is common to condition on the predictors when making inference, it is not a requirement. Including the distribution of the predictors is simply a modeling exercise when developing the likelihood. Since Equation 23.4 already specifies the conditional distribution of the response given the predictor, adding a statement about the marginal distribution of the predictor leads to a hierarchical model that fully specifies the distribution of all observed variables. For example, we might consider the model\n\\[\n\\begin{aligned}\n  (\\text{Response})_i \\mid (\\text{Predictor})_i, \\beta_0, \\beta_1, \\sigma^2\n    &\\stackrel{\\text{Ind}}{\\sim}N\\left(\\beta_0 + \\beta_1 (\\text{Predictor})_i, \\sigma^2\\right) \\\\\n  (\\text{Predictor}_i \\mid \\gamma, \\eta^2 &\\stackrel{\\text{IID}}{\\sim}N\\left(\\gamma, \\eta^2\\right).\n\\end{aligned}\n\\tag{23.5}\\]\nWhile Equation 23.5 has the same predictor as in Equation 23.1; however, it fully specifies the distribution of all observed variables. The likelihood of the observed data is then\n\\[\n\\begin{aligned}\n  f\\left(\\mathbf{Data} \\mid \\beta_0, \\beta_1, \\sigma^2, \\gamma, \\eta^2\\right)\n    &= \\prod_{i=1}^{n} f\\left((\\text{Response})_i \\mid (\\text{Predictor})_i, \\beta_0, \\beta_1, \\sigma^2\\right) g\\left((\\text{Predictor})_i \\mid \\gamma, \\eta^2\\right) \\\\\n    &= \\left(2\\pi \\sigma^2\\right)^{-n/2} \\exp\\left\\{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} \\left((\\text{Response})_i - \\beta_0 - \\beta_1 (\\text{Predictor})_i\\right)^2\\right\\} \\\\\n    &\\qquad \\cdot \\left(2\\pi \\eta^2\\right)^{-n/2} \\exp\\left\\{-\\frac{1}{2\\eta^2} \\sum_{i=1}^{n}\\left((\\text{Predictor})_i - \\gamma\\right)^2\\right\\}.\n\\end{aligned}\n\\]\nWhile clearly a more complex model, the benefit is that we are able to simultaneously predict the value of the predictor and response for a future observation.\n\n\n\n\n\n\nNote\n\n\n\nWhile it is common to model the response conditional on the predictors and not specify the distribution of the predictors, it is possible to fully specify the likelihood of all observed variables, if desired."
  },
  {
    "objectID": "06b-linear-regression.html#interpreting-the-predictors",
    "href": "06b-linear-regression.html#interpreting-the-predictors",
    "title": "23  Regression Models for a Quantitative Response",
    "section": "23.4 Interpreting the Predictors",
    "text": "23.4 Interpreting the Predictors\nAs we have already stated, the range of potential models is infinitely large; as a result, there is no one interpretation we can provide for a parameter in the model. However, it is common that the predictors in a regression model govern the mean response. For example, Equation 23.3 considers multiple predictors, but each impacts the mean response while assuming the variance \\(\\sigma^2\\) is constant across the population. In this model, \\(\\beta_j\\) describe the relationship between the response and the \\(j\\)-th predictor. Specifically,\n\n\\(\\beta_j\\) is the change in the average response given a 1-unit increase in the \\(j\\)-th predictor, holding all other predictors fixed.\n\nWe can also provide an interpretation for the intercept \\(\\beta_0\\):\n\n\\(\\beta_0\\) is the average value of the response when all predictors take the value 0.\n\nWhile not governing the mean response, we should not ignore the interpretation of \\(\\sigma^2\\):\n\n\\(\\sigma^2\\) is the variability in the response for a fixed set of predictors.\n\nThese interpretations seem straight forward but are hiding a lot of complexity. Working backward, notice the conditional/cross-sectional nature of the interpretation of \\(\\sigma^2\\). It is not the overall variability in the response; it is the variability of the response for any fixed set of values for the predictors. That is, the model is specifying the distribution of the response for a specified level of the predictors.\n\n\n\n\n\n\nWarning\n\n\n\nRecall that a marginal distribution and a conditional distribution of a random variable are distinct distributions. The distributional model of the response in a regression setting is conditional on the values of the predictors, and the form of the marginal distribution of the response need not have the same form.\n\n\nNext, we notice that the interpretation of \\(\\beta_0\\) may not always make sense in context. For example, suppose our response is the weight of individuals (in pounds) and our predictor is their height (in inches). It does not make sense to talk about an individual with a height of 0 inches. This is the result of extrapolation.\n\nDefinition 23.2 (Extrapolation) Extrapolation occurs when we use a model to predict outside of the region for which data is available.\n\nThe danger with extrapolation is that without scientific justification, we have no reason to believe the model we have observed in one region of the support will continue to hold for all regions of the support. For example, it is possible the relationship between the mean response and the predictor is linear on the interval \\((a, b)\\), but it becomes quadratic outside of this interval. If we fit the model on the interval \\((a, b)\\) and then predict outside this range, our predictions will be biased. This is what can lead to senseless interpretation of \\(\\beta_0\\). We should always be cautious of extrapolation.\nFinally, and we cannot emphasize the benefit of this enough, the interpretation of \\(\\beta_j\\) measures the effect of the \\(j\\)-th predictor holding the value of other predictors in the model fixed. This means that in a regression model, we are naturally isolating the effect of the predictor. This provides a unique interpretation to a hypothesis of the form\n\\[H_0: \\beta_j = 0 \\qquad \\text{vs.} \\qquad H_1: \\beta_j \\neq 0.\\]\nThis hypothesis is really asking whether the \\(j\\)-th predictor is linearly associated with the mean response after accounting for the impact of the other predictors in the model.\n\nExample 23.2 (Rehabilitation Therapy Continued) Example 23.1 described a study to investigate recovery time among patients who have undergone a corrective knee surgery. Suppose we are willing to believe that the mean recovery time is linearly related to the age of a patient. Further, suppose that we believe that given the age of the patient, the recovery time will follow an Exponential distribution. Write down the likelihood for the observed data conditional on the age of the patient.\n\n\nSolution. Recall that for an exponential distribution, the mean response is given by the scale parameter; therefore, the mean response fully characterizes the distribution. If we are willing to assume that given a patient’s age, the recovering time of one patient is unrelated to the recovery time of any other patient, then based on the description, we have\n\\[(\\text{Recovery Time})_i \\mid (\\text{Age})_i, \\beta_0, \\beta_1 \\stackrel{\\text{Ind}}{\\sim}Exp\\left(\\beta_0 + \\beta_1 (\\text{Age})_i\\right),\\]\nwhere the Exponential distribution is parameterized by its scale parameter. This results in a likelihood of\n\\[\n\\begin{aligned}\n  f\\left(\\mathbf{Recovery Time} \\right. &\\mid \\left. \\mathbf{Age}, \\beta_0, \\beta_1\\right)  \n    = \\prod_{i=1}^{n} \\frac{1}{\\beta_0 + \\beta_1 (\\text{Age})_i} \\exp\\left\\{- \\frac{(\\text{Recovery Time})_i}{\\beta_0 + \\beta_1 (\\text{Age})_i}\\right\\} \\\\\n    &= \\left[\\prod_{i=1}^{n} \\frac{1}{\\beta_0 + \\beta_1 (\\text{Age})_i}\\right] \\exp\\left\\{-\\sum_{i=1}^{n} \\frac{(\\text{Recovery Time})_i}{\\beta_0 + \\beta_1 (\\text{Age})_i}\\right\\}.\n\\end{aligned}\n\\tag{23.6}\\]\nThe likelihood does not reduce to a simple form.\n\n\n\n\n\n\n\nReparameterization\n\n\n\nConsider the proposed solution to Example 23.2. Recall that the scale parameter of an Exponential distribution must be positive; however, nothing in the above specification requires the linear predictor\n\\[\\beta_0 + \\beta_1 (\\text{Age})_i\\]\nbe positive for all ages. Often times, this is not a problem; the region of reasonable values of the parameter will result in a positive value of the scale parameter within the range of our data (again, extrapolation could be problematic). However, we might be in a case where reasonable values of the parameter lead to negative mean responses; or, it could be that the MCMC algorithm wanders into such regions creating numerical instability in the algorithm itself. Regardless, if we would like to enforce the constraint that the mean response be positive, we have two options.\nFirst, we could address the constraint by ensuring that both \\(\\beta_0\\) and \\(\\beta_1\\) are restricted to be positive. This could be done by choosing priors which have support on the positive real line, for example. Alternatively, we could reparameterize the model to force \\(\\beta_0\\) and \\(\\beta_1\\) to be positive. That is, we write\n\\[(\\text{Recovery Time})_i \\mid (\\text{Age})_i, \\gamma_0, \\gamma_1 \\stackrel{\\text{Ind}}{\\sim}Exp\\left(e^{\\gamma_0} + e^{\\gamma_1} (\\text{Age})_i\\right),\\]\nwhere we have substituted \\(\\beta_0 = e^{\\gamma_0}\\) and \\(\\beta_1 = e^{\\gamma_1}\\). Notice that \\(\\beta_0\\) and \\(\\beta_1\\) will be positive for any value of \\(\\gamma_0\\) and \\(\\gamma_1\\); therefore, we now have unconstrained parameters \\(\\gamma_0\\) and \\(\\gamma_1\\) and yet have constrained the mean response to be positive.\nThere is yet a third option. We could reparameterize the mean response directly. That is, we consider\n\\[(\\text{Recovery Time})_i \\mid (\\text{Age})_i, \\beta_0, \\beta_1 \\stackrel{\\text{Ind}}{\\sim}Exp\\left(e^{\\beta_0 + \\beta_1 (\\text{Age})_i}\\right).\\]\nIn this specification, there are no constraints on \\(\\beta_0\\) and \\(\\beta_1\\), but we have ensured that the mean response is positive. This has come at a cost, however; in this specification, the mean recovery time is no longer linearly related to age.\nReparameterization is a helpful tool to consider to enforce constraints in a numerically stable way."
  },
  {
    "objectID": "06c-reg-extensions.html#including-categorical-predictors",
    "href": "06c-reg-extensions.html#including-categorical-predictors",
    "title": "24  Extensions to the Linear Model",
    "section": "24.1 Including Categorical Predictors",
    "text": "24.1 Including Categorical Predictors\nWhen our collection of predictors only consists of quantitative variables, applying Equation 24.1 is straightforward. However, when one of the predictors is a categorical variable (for example, the prior activity level of a patient in Example 23.1), it may not be obvious how to incorporate these into Equation 24.1. This is actually related to the models we considered in Chapter 21; we revisit the idea from a regression perspective.\n\nExample 24.1 (Anxiety Among College Students) Suppose we conduct a small survey to determine whether the distribution of the anxiety level of college students differs depending on their class standing (freshman, sophomore, junior, senior). Let \\(\\theta_i\\) represent the parameter of interest governing the distribution of anxiety for the \\(i\\)-th student. We want to let \\(\\theta_i\\) depend on class standing within the form illustrated in Equation 24.1.\n\nWe approached this problem in Chapter 21 essentially by saying\n\\[(\\text{Anxiety})_i \\mid \\boldsymbol{\\theta} \\stackrel{\\text{Ind}}{\\sim}f\\left((\\text{Anxiety})_i \\mid \\theta_j\\right),\\]\nwhere \\(\\theta_1\\) is the parameter for freshman students, \\(\\theta_2\\) the parameter for sophomores, \\(\\theta_3\\) the parameter for juniors, and \\(\\theta_4\\) the parameter for seniors. Illustrating the impact of class standing on the distribution of anxiety, we might define a new variable\n\\[(\\text{Class})_i = \\begin{cases}\n1 & \\text{if i-th student is a freshman} \\\\\n2 & \\text{if i-th student is a sophomore} \\\\\n3 & \\text{if i-th student is a junior} \\\\\n4 & \\text{if i-th student is a senior} \\end{cases}\\]\nand then proceeded to say\n\\[(\\text{Anxiety})_i \\mid (\\text{Class})_i, \\boldsymbol{\\theta} \\stackrel{\\text{Ind}}{\\sim}f\\left((\\text{Anxiety})_i \\mid \\theta_{(\\text{Class})_i}\\right). \\tag{24.2}\\]\nAgain, this was essentially our approach in Chapter 21, and it illustrates that we can allow the parameter to depend upon the predictor (we are doing regression). It just feels disjoint from the approach we have taken in Chapter 23. At first glance, we might be tempted to say\n\\[\n\\begin{aligned}\n(\\text{Anxiety})_i \\mid (\\text{Class})_i, \\theta_i\n  &\\stackrel{\\text{Ind}}{\\sim}f\\left((\\text{Anxiety})_i \\mid \\theta_i\\right) \\\\\n\\theta_i\n  &= \\beta_0 + \\beta_1 (\\text{Class})_i\n\\end{aligned}\n\\tag{24.3}\\]\nmaking use of the numeric variable \\((\\text{Class})_i\\) we defined above. However, this imposes additional structure on the model. In particular, it suggests that the parameter \\(\\theta\\) changes linearly as we move between class standing (freshman to sophomore, junior to senior). Notice that the additional structure is captured by a reduced number of parameters. The original model in Equation 24.2 had four parameters: \\(\\theta_1, \\theta_2, \\theta_3\\) and \\(\\theta_4\\). The approach in Equation 24.3, however, only has two parameters: \\(\\beta_0\\) and \\(\\beta_1\\). These two models are clearly not equivalent. However, that does not mean that we cannot embed categorical predictors into the linear framework of Equation 24.1; we just need a different approach, and that approach involves indicator variables.\n\nDefinition 24.1 (Indicator Variable) An indicator variable is a binary variable (takes on the value 0 or 1), taking the value 1 when a specific event occurs. A collection of \\(k-1\\) indicator variables can be used to capture a categorical variable with \\(k\\) levels in a regression model.\n\nThe “reference group” (or reference level) is the group (level) defined by setting all indicator variables in a regression model to 0.\n\n\nFor the Anxiety example, we define \\(4 - 1 = 3\\) indicator variables to capture class status:\n\\[\n\\begin{aligned}\n  (\\text{Sophomore})_i &= \\begin{cases}\n    1 & \\text{if i-th student is a Sophomore} \\\\\n    0 & \\text{otherwise} \\end{cases} \\\\\n  (\\text{Junior})_i &= \\begin{cases}\n    1 & \\text{if i-th student is a Junior} \\\\\n    0 & \\text{otherwise} \\end{cases} \\\\\n  (\\text{Senior})_i &= \\begin{cases}\n    1 & \\text{if i-th student is a Senior} \\\\\n    0 & \\text{otherwise.} \\end{cases} \\\\\n\\end{aligned}\n\\]\nWe can then place these into the model following the format of Equation 24.1; specifically, we have\n\\[\n\\begin{aligned}\n(\\text{Anxiety})_i \\mid \\theta_i\n  &\\stackrel{\\text{Ind}}{\\sim}f\\left((\\text{Anxiety})_i \\mid \\theta_i\\right) \\\\\n\\theta_i\n  &= \\beta_0 + \\beta_1 (\\text{Sophomore})_i + \\beta_2 (\\text{Junior})_i + \\beta_3 (\\text{Senior})_i.\n\\end{aligned}\n\\tag{24.4}\\]\nIt may at first seem that this model neglects the freshman class; however, the freshman class serves as the reference group (not sophomores or juniors or seniors) so the parameter for these students is represented by \\(\\beta_0\\). Comparing the modeling strategy in Equation 24.4 to that in Equation 24.2, we note that\n\\[\n\\begin{aligned}\n\\theta_1 &= \\beta_0 \\\\\n\\theta_2 &= \\beta_0 + \\beta_1 \\\\\n\\theta_3 &= \\beta_0 + \\beta_2 \\\\\n\\theta_4 &= \\beta_0 + \\beta_3.\n\\end{aligned}\n\\]\nOur revised model incorporates the categorical predictor while maintaining the general structure/complexity (we have not reduced the number of parameters).\n\n\n\n\n\n\nBig Idea\n\n\n\nA model which is linear in the parameters can accommodate categorical predictors through the inclusion of indicator variables.\n\n\n\nExample 24.2 (Rehabilitation Therapy Continued) Example 23.1 described a study to investigate recovery time among patients who have undergone a corrective knee surgery. Suppose we are willing to believe that the mean recovery time is linearly related to the age of a patient, but we also believe that the mean recovery time may differ depending on the patient’s level of activity prior to the surgery.\nFurther, suppose that we believe that given the age of the patient and their prior activity level, the recovery time will follow an Exponential distribution. Write down the likelihood for the observed data conditional on the age of the patient and their prior activity level.\n\n\nSolution. Generalizing our solution in Example 23.2, we can say that\n\\[(\\text{Recovery Time})_i \\mid (\\mathbf{Predictors})_i, \\boldsymbol{\\beta} \\stackrel{\\text{Ind}}{\\sim}Exp\\left(\\theta_i\\right)\\]\nwhere \\(\\theta_i\\), the scale term, will depend on the age and prior activity level of the patient, giving a likelihood of\n\\[\n\\begin{aligned}\nf\\left(\\mathbf{Recovery Time} \\right. &\\mid \\left. \\mathbf{Predictors}, \\boldsymbol{\\beta}\\right)\n    = \\prod_{i=1}^{n} \\frac{1}{\\theta_i} \\exp\\left\\{- \\frac{(\\text{Recovery Time})_i}{\\theta_i}\\right\\} \\\\\n    &= \\left[\\prod_{i=1}^{n} \\frac{1}{\\theta_i}\\right] \\exp\\left\\{-\\sum_{i=1}^{n} \\frac{(\\text{Recovery Time})_i}{\\theta_i}\\right\\}.\n\\end{aligned}\n\\]\nIt just remains to define the relationship between the predictors and the scale parameter \\(\\theta_i\\). If we adopt the linear structure of Equation 24.1, we have\n\\[\\theta_i = \\beta_0 + \\beta_1 (\\text{Age})_i + \\beta_2 (\\text{Moderate})_i + \\beta_3 (\\text{High})_i,\\]\nwhere\n\\[\n\\begin{aligned}\n  (\\text{Moderate})_i\n    &= \\begin{cases} 1 & \\text{if i-th patient has a moderate amount of activity prior to surgery} \\\\ 0 & \\text{otherwise} \\end{cases} \\\\\n  (\\text{High})_i\n    &= \\begin{cases} 1 & \\text{if i-th patient has a high amount of activity prior to surgery} \\\\ 0 & \\text{otherwise}, \\end{cases}\n\\end{aligned}\n\\]\nleaving the “low activity” group as the reference group."
  },
  {
    "objectID": "06c-reg-extensions.html#curvature",
    "href": "06c-reg-extensions.html#curvature",
    "title": "24  Extensions to the Linear Model",
    "section": "24.2 Curvature",
    "text": "24.2 Curvature\nEquation 24.1 is typically referred to as a “linear model.” So, it may seem at first glance that such a model restricts us to only consider cases in which the predictor linearly impacts the parameter of interest (the mean response being linearly related to the predictor, for example). However, Equation 24.1 is linear in the parameters; and therefore, it is flexible enough to capture curvature.\nWhile there are more sophisticated approaches, to simply illustrate the idea, we can incorporate curvature through higher-order terms. For example, the following model fits nicely within the linear framework of Equation 24.1:\n\\[\\theta_i = \\beta_0 + \\beta_1 (\\text{Predictor 1})_i + \\beta_2 (\\text{Predictor 1})_i^2.\\]\nThis is linear in the parameters, even though it is not linear in the predictor. Essentially, we can simply define\n\\[(\\text{Predictor 2})_i = (\\text{Predictor 1})_i^2\\]\nand then it aligns directly with Equation 24.1.\n\n\n\n\n\n\nNote\n\n\n\nWe need not restrict ourselves to polynomial terms. For example, it is possible to use cubic splines in regression models, and it has even been shown that carefully chosen splines can approximate nearly any form of curvature.\n\n\nOf course, we are not restricted to only considering functions which are linear in the parameters; they tend to be commonly used for their simplicity of interpretation when scientific models do not suggest a particular form for the model. However, we could consider models like\n\\[\\theta_i = \\beta_0 e^{\\beta_1 (\\text{Predictor})_i},\\]\nwhich are not linear in the parameters. These are also valid models and easy to make inference on in the Bayesian framework."
  },
  {
    "objectID": "06d-reg-priors.html",
    "href": "06d-reg-priors.html",
    "title": "25  Default Priors in Regression Models",
    "section": "",
    "text": "As the number of predictors we take into account grows, the number of unknown parameters governing the distribution of the response grows. Ideally, the prior distribution for each parameter would be elicited from the beliefs of discipline experts. However, with so many parameters, it can be difficult to elicit enough information from experts to form a joint prior distribution across all parameters. With so many parameters, there is often a demand for what to do in “no information” settings.\nA popular approach at one time was to use a “spike and slab” prior that places a point mass at 0 mixed with a relatively flat distribution on the real line. For example, the prior\n\\[\\pi\\left(\\beta_j\\right) = 0.5 \\delta\\left(\\beta_j - 0\\right) + 0.5 \\frac{1}{1000 \\sqrt{2 \\pi}} \\exp\\left\\{-\\frac{1}{1000^2} \\left(\\beta_j - 0 \\right)^2\\right\\}\\]\nmixes a point mass at 0 (with probability 0.5) with a Normal distribution centered at 0 with a standard deviation of 1000 (with probability 0.5). The large variance within the Normal distribution component of the prior distribution spreads the mass thinly across the real line.\nThese priors were once popular because they mixed the typical null distribution \\(\\left(\\beta_j = 0\\right)\\) with a vague prior that allowed the parameter to take nearly any value. However, there have been some recent recommendations against such priors.\nThe authors of the Stan programming language (which implements the Hamiltonian Monte Carlo approach) make the following suggestions regarding default priors.\n\nDo not use vague priors.\nFlat, bounded, priors are helpful when you have some idea of the range.\nTo conduct an analysis that is robust to outliers, use a Cauchy distribution for location parameters and a Half-Cauchy for scale parameters.\nGiven enough data, it is possible to use flat priors for default or sensitivity analyses.\n\nThe first suggestion comes from the idea that vague priors put a lot of weight on values we often do not believe are reasonable. However, others argue that vague priors are safer than unbounded flat priors because each puts weight on unreasonable values, but unbounded flat priors run the risk of producing an improper posterior distribution.\nThe second suggestion really highlights that often we have some information. If really pressed, we are often able to at least suggest unreasonable values for a parameter — an upper or lower bound, for example. This then suggests the possibility of a flat prior over a closed interval.\nThe Cauchy distribution is a bell-shaped distribution similar to a Normal distribution. However, while it is unimodal (with mode 0), it does not have a finite mean. Essentially, the distribution is so variable that it has no moments. This acts similarly to the idea behind a vague prior, placing most mass in a reasonable range (near 0), but still having a mass that extends out in both directions. A Half-Cauchy distribution limits the support to the positive real line.\nA sensitivity analysis allows us to see how dependent our results are on the choice of prior. If our results would change dramatically depending on the choice of prior, that at a minimum, warrants a discussion.\n\nExample 25.1 (Rehabilitation Therapy Continued) Example 23.1 described a study to investigate recovery time among patients who have undergone a corrective knee surgery. In Example 24.2, we developed a model for the distribution of the response given the age and prior activity level of the patient:\n\\[(\\text{Recovery Time})_i \\mid (\\mathbf{Predictors})_i, \\boldsymbol{\\beta} \\stackrel{\\text{Ind}}{\\sim}Exp\\left(\\theta_i\\right)\\]\nwhere \\(\\theta_i\\), the scale term, will depend on the age and prior activity level of the patient, through the function\n\\[\\theta_i = \\beta_0 + \\beta_1 (\\text{Age})_i + \\beta_2 (\\text{Moderate})_i + \\beta_3 (\\text{High})_i,\\]\nwhere\n\\[\n\\begin{aligned}\n  (\\text{Moderate})_i\n    &= \\begin{cases} 1 & \\text{if i-th patient has a moderate amount of activity prior to surgery} \\\\ 0 & \\text{otherwise} \\end{cases} \\\\\n  (\\text{High})_i\n    &= \\begin{cases} 1 & \\text{if i-th patient has a high amount of activity prior to surgery} \\\\ 0 & \\text{otherwise}. \\end{cases}\n\\end{aligned}\n\\]\nSpecify reasonable default priors for each of the unknown parameters.\n\n\nSolution. As discussed in Example 23.2, the mean response for the Exponential distribution should be positive. One way to ensure a positive mean response is to ensure each parameter is positive. Therefore, a reasonable default prior could be a Half-Cauchy distribution."
  },
  {
    "objectID": "06e-qr-factorization.html",
    "href": "06e-qr-factorization.html",
    "title": "26  QR Factorization",
    "section": "",
    "text": "As a regression model grows in complexity, we need to consider the computational efficiency of the algorithms used to fit the model. QR factorization is a well-known computational step that can increase the efficiency of an MCMC algorithm in a regression model.\nConsider a regression model in which the parameter \\(\\theta\\) is allowed to vary according to a linear function of the predictors:\n\\[\\theta_i = \\sum_{j=1}^{p} \\beta_j (\\text{Predictor } j)_i.\\]\nWe have eliminated the “intercept” term \\(\\beta_0\\), but this is done without loss of generality as we could view “Predictor 1” as the value 1 for all subjects, resulting in \\(\\beta_1\\) acting as the intercept. We can represent this model in matrix notation as\n\\[\\boldsymbol{\\theta} = \\mathbf{X} \\boldsymbol{\\beta},\\]\nwhere \\(\\boldsymbol{\\beta} = \\left(\\beta_1, \\dotsc, \\beta_p\\right)^\\top\\) is the parameter vector; and, the \\(j\\)-th column of \\(\\mathbf{X}\\) is the vector of length \\(n\\) containing the values of the \\(j\\)-th predictor for the \\(n\\) subjects. That is,\n\\[\\mathbf{X}_{i, j} = (\\text{Predictor } j)_i \\qquad j=1,2,\\dots,p.\\]\nThe matrix \\(\\mathbf{X}\\) is known as the design matrix.\nAs long as the number of observations \\(n\\) exceeds the number of predictors \\(p\\) in the model, we can decompose the design matrix \\(\\mathbf{X}\\) as\n\\[\\mathbf{X} = \\mathbf{Q}\\mathbf{R}\\]\nwhere \\(\\mathbf{Q}\\) is an orthogonal \\(n \\times p\\) matrix and \\(\\mathbf{R}\\) is an upper-triangular \\(p \\times p\\) matrix. Then, we can write the linear predictor for the mean response as\n\\[\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{Q}\\mathbf{R}\\boldsymbol{\\beta}.\\]\nTherefore, the regression is conducted with the “new” design matrix \\(\\mathbf{Q}\\) with parameters \\(\\boldsymbol{\\eta} = \\mathbf{R}\\boldsymbol{\\beta}\\). These parameters are then transformed back into the parameters of interest \\(\\boldsymbol{\\beta}\\) by acknowledging that\n\\[\\boldsymbol{\\beta} = \\mathbf{R}^{-1} \\boldsymbol{\\eta}.\\]\nAdmittedly, this feels like only algebraic manipulation. The reason this works is that the columns of the “new” design matrix \\(\\mathbf{Q}\\) are orthogonal. This allows the MCMC algorithm to move more easily through the parameter space because changing one column has no effect on the other columns in the optimization routine. The columns of this new design matrix are also on the same scale; that is, the impact of one variable (like yearly income) taking on extreme values while another (like an indicator variable) taking on smaller units is reduced. Having variables on the same scale allows the MCMC algorithm to move around the parameter space with a small number of large steps. As a result, the numerical accuracy of the algorithm is improved.\n\n\n\n\n\n\nBig Idea\n\n\n\nQR factorization improves the computational efficiency of the regression and MCMC algorithms. There is no change to the actual distribution of the parameters."
  },
  {
    "objectID": "06f-reg-conditions.html",
    "href": "06f-reg-conditions.html",
    "title": "27  Assessment for Regression Models for the Mean",
    "section": "",
    "text": "While we have tried to emphasize the flexibility of regression models, the most common regression model is one of the form\n\\[\n  \\begin{aligned}\n    (\\text{Response})_i\n      &\\mid (\\text{Predictors})_i, \\boldsymbol{\\beta}, \\boldsymbol{\\theta} \\stackrel{\\text{Ind}}{\\sim}f\\left(\\mu_i,\\boldsymbol{\\theta}\\right) \\\\\n    \\mu_i &= \\beta_0 + \\sum_{j=1}^{p} \\beta_j (\\text{Predictor } j)_i,\n  \\end{aligned}\n\\tag{27.1}\\]\nwhere \\(\\mu_i\\) represents the (conditional) mean response and \\(\\boldsymbol{\\theta}\\) captures additional parameters that do not depend on the predictors (often scale parameters). Equation 27.1 represents a model focused on the mean response.\nIt is reasonable to ask if the model we have constructed is reasonable for the data we have observed. When our models are of the form described in Equation 27.1, a large assumption is that we have correctly specified the mean response. This can be assessed graphically using residuals.\n\nDefinition 27.1 (Residual) A residual is the difference between an observed response and the predicted mean response for that same individual:\n\\[(\\text{Residual})_i = (\\text{Response})_i - (\\text{Fitted Value})_i,\\]\nwhere\n\\[(\\text{Fitted Value})_i = \\widehat{\\beta}_0 + \\sum_{j=1}^{p} \\widehat{\\beta}_j (\\text{Predictor} j)_i.\\]\n\n\n\n\n\n\n\nNote\n\n\n\nIn a classical introductory statistics course, the least squares estimates are used when computing the residuals. However, from the Bayesian framework, we have explored various point estimates. It is common to use the posterior mean for each parameter when computing the residuals; however, nothing prohibits the use of the posterior median or another point estimate. Transparency is critical; you should be clear about the analysis you have conducted.\n\n\nIn order to assess that the form of the mean model is correctly specified, it is common to construct a graphic of the residuals against the fitted values. If the form of the mean model is correct, there should not be any distinguishable pattern in the location of the graphic. Trends in the location suggest the form of the mean model is incorrectly specified (see Figure 27.1).\n\n\n\n\n\nFigure 27.1: Plot of the residuals against the fitted values for two hypothetical models. One illustrates what we would expect under a correctly specified mean model; the second is an example of a graphic indicating the mean model is misspecified.\n\n\n\n\n\n\n\n\n\n\nAssessing Specification of Mean Response\n\n\n\nIf the mean response is correctly specified, we would expect the residuals to balance around 0, regardless of the estimated mean response. When examining a plot of the residuals against the fitted values, any trends in the location suggest the functional form of the mean response has been incorrectly specified.\n\n\n\nExample 27.1 (Rehabilitation Therapy Continued) Example 23.1 described a study to investigate recovery time among patients who have undergone a corrective knee surgery. Suppose we are willing to believe that the mean recovery time is linearly related to the age of a patient. Combining the model for the likelihood suggested in Example 23.2 and the advice on default priors specified in Chapter 25, consider the following model:\n\\[\n\\begin{aligned}\n  (\\text{Recovery Time})_i &\\mid (\\text{Age})_i, \\boldsymbol{\\beta} \\stackrel{\\text{Ind}}{\\sim}Exp\\left(\\theta_i\\right) \\\\\n  \\theta_i &= \\beta_0 + \\beta_1 (\\text{Age})_i \\\\\n  \\beta_0 &\\sim Unif(0, 25) \\\\\n  \\beta_1 &\\sim Unif(0, 5).\n\\end{aligned}\n\\]\nThis model was fit using an MCMC algorithm with 3 chains; a burn-in of 2000 was applied to each of the chains, and a total of 5000 samples were generated for each chain (for a total of 9000 variates after the burn-in period). The posterior mean was used to estimate each of the unknown parameters. Figure 27.2 presents the plot of the residuals against the fitted values for this model. Comment on the assumption that the mean response is properly specified.\n\n\n\n\n\n\nFigure 27.2: Assessment of the mean response model for the Therapy example.\n\n\n\n\n\nSolution. If the mean response model is correctly specified, we would expect the residuals to balance around 0 for all fitted values. Notice that the residuals are centered below for the majority of the graphic, and only balance around 0 for large fitted values. This trend in the location of the residuals suggests the mean response model was not correct specified.\nIn particular, this slight upward trend suggests that perhaps we were incorrect in forcing the intercept to be positive (remember, our prior distribution forced the support for the intercept to be positive). We had done this because the mean response for an exponential distribution must always be positive. However, because of extrapolation, forcing this to be the case at an age of 0 seems to be problematic. There are two approaches we could consider in addressing this:\n\nWe could consider a different prior that allows the intercept to be negative, accepting that it is nonsensical and that the model will not predict well for small values of age.\nWe could center the age variable (by subtracting the average observed age from each observation). Center the age variable does not impact the slope, but it changes the interpretation of the intercept. In particular, the intercept would represent the average recovery time for a patient of average age. This avoids the problem of extrapolation when interpreting the intercept and might address the problems we are seeing above.\n\n\nThe other primary assumption that we make when fitting a regression model is that the conditional distribution of the response is appropriate. In Example 27.1, for example, we are assuming that the Exponential distribution for the response (conditional on the age) is appropriate, as opposed to a Normal distribution, for example. One technique for assessing whether this distributional assumption is appropriate is to compare the posterior predictive distribution with the observed distribution. As we have seen, the posterior predictive distribution (Definition 14.2) can be challenging to derive; fortunately, it is easily simulated using a sample from the posterior distribution. With the general model of Equation 27.1 in mind, we can generate the posterior predictive distribution as follows:\n\nObtain a sample of size \\(M\\) from the posterior distribution of each unknown parameter: \\(\\boldsymbol{\\beta}^{(1)}, \\boldsymbol{\\beta}^{(2)}, \\dotsc, \\boldsymbol{\\beta}^{(M)}\\) and \\(\\boldsymbol{\\theta}^{(1)}, \\boldsymbol{\\theta}^{(2)}, \\dotsc, \\boldsymbol{\\theta}^{(M)}\\).\nFor each sample from the posterior, generate a new sample of \\(n\\) responses according to \\((\\text{Predicted Response})_i^{(m)} \\sim f\\left(\\mu_i^{(m)}, \\boldsymbol{\\theta}^{(m)}\\right)\\) for \\(m = 1, 2, \\dotsc, M\\). If we are conditioning on the predictors, they are taken to be those from the original sample.\n\nThe above two steps produces \\(M\\) new samples; each generated sample will produce a unique distribution of the responses across the \\(n\\) observations. We can summarize each of these \\(M\\) distributions using a density plot, overlaid on the same graphic. Then, we can overlay the density from the observed response to get a sense of how they compare. Figure 27.3 gives an example of what this plot might look like.\n\n\n\n\n\n\nNote\n\n\n\nDue to the computational intensity of this graphic, it is common to do this for a random sample of variates instead of all \\(M\\) variates generated by the MCMC algorithm.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIt is important to remember that comparing the posterior predictive distribution to that of the observed distribution is combining multiple conditions/assumptions together: the complete form of the distribution as well as how the individual observations vary compared to how the aggregate dataset varies. While our modeling is conditioned, the density of the observed response marginalizes across the predictors. Care must be taken not to over-interpret this graphic as proving we have the correct model.\n\n\n\n\n\n\n\nFigure 27.3: Two plots of the posterior predictive distributions from a regression model with the observed distribution. One illustrates what we would expect when the distribution accurately captures the process; the second is an example of a graphic indicating the distributional assumptions are incorrect.\n\n\n\n\n\n\n\n\n\n\nAssessing the Likelihood\n\n\n\nIf the model for the likelihood is correctly specified, then the marginal distribution of the observed response should be similar to the posterior predictive distribution given the observed data. For a regression model, this is done by generating several samples of the same size given the posterior variates; if the distributional model is appropriate, a density plot of the observed response should line up with the density plots of those samples generated from the posterior variates. Any major differences in these shapes would suggest some aspect of the model (including the distributional form) is incorrect.\n\n\n\nExample 27.2 (Rehabilitation Therapy Continued) Example 27.1 presented a model for the study described in Example 23.1. Figure 27.4 is a plot of the posterior predicted distribution of the responses (using 250 randomly selected posterior variates) against the observed distribution of the response. Comment on the assumption that the distributional model specified is appropriate.\n\n\n\n\n\n\nFigure 27.4: Assessment of the distributional assumptions for the Therapy example.\n\n\n\n\n\nSolution. The observed marginal distribution of the response is very different than the predicted distributions. While this could be due in part to the misspecification of the mean response model (as noted in Example 27.1), the level of departure here suggests the distributional assumption on the likelihood is also incorrect. If we wanted to be sure, we should refit the model using a different mean response function first, and then repeat this process."
  },
  {
    "objectID": "06g-categorical-reg.html#considerations-for-a-binary-response",
    "href": "06g-categorical-reg.html#considerations-for-a-binary-response",
    "title": "28  Regression Models for Categorical Responses",
    "section": "28.1 Considerations for a Binary Response",
    "text": "28.1 Considerations for a Binary Response\nSuppose our response is binary, taking the value 1 when an event of interest occurs (a “success”) and taking the value 0 when the event does not occur (a “failure”). A starting point for this model is\n\\[(\\text{Response})_i \\stackrel{\\text{Ind}}{\\sim}Ber\\left(\\theta_i\\right),\\]\nwhere we are allowing the probability of success \\(\\theta_i\\) to potentially vary from one observation to the next. This model continues to assume the response from one individual is independent of the response from any other individual. Our initial attempt at generalizing to a regression setting may be to borrow the linear model structure of the previous chapters and consider\n\\[\\theta_i = \\beta_0 + \\sum_{j=1}^{p} \\beta_j (\\text{Predictor } j)_i.\\]\nUnfortunately, this can be a poor strategy. Notice that there is nothing ensuring that this linear function produces values of \\(\\theta_i\\) which are consistent with its support (this is the same problem we encountered in Example 23.2). That is, we know that since \\(\\theta_i\\) is a probability, its support is the interval \\((0, 1)\\). The linear function, however, could quite easily produce values which are negative or exceed 1; instead, we desire a function that is always bounded between 0 and 1, similar to that represented in Figure 28.1.\n\n\n\n\n\nFigure 28.1: Comparison of two functional forms of a single predictor for the success probability in a regression for a binary response. The first indicates the problems with a linear functional form, while the second illustrates a function that is bounded on the correct support.\n\n\n\n\nOne of the most common choices for the functional form is the logistic function.\n\nDefinition 28.1 (Logistic Regression) Given a binary response; logistic regression assumes that\n\\[\n\\begin{aligned}\n  (\\text{Response})_i &\\mid (\\text{Predictors})_i, \\boldsymbol{\\beta} \\stackrel{\\text{Ind}}{\\sim}Ber\\left(\\theta_i\\right) \\\\\n  \\theta_i &= \\frac{e^{\\beta_0 + \\sum_{j=1}^{p} \\beta_j (\\text{Predictor } j)_i}}{1 + e^{\\beta_0 + \\sum_{j=1}^{p} \\beta_j (\\text{Predictor } j)_i}}.\n\\end{aligned}\n\\]\n\nThis is known as logistic regression because the functional form of the probability matches that of the CDF of the Standard Logistic Distribution:\n\\[\\frac{e^x}{1+e^{x}}.\\]\n\n\n\n\n\n\nNote\n\n\n\nWhen performing regression with a binary response, any CDF could be used for the functional form relating the predictors to the success probability; however, the Standard Logistic (“logistic regression”) and Standard Normal (“probit regression”) distributions are most common.\n\n\nUsing a CDF for the functional form ensures that for any choice of \\(\\boldsymbol{\\beta}\\) and the predictors, the function will always take a value between 0 and 1. Notice that our choice of \\(\\theta_i\\) is nonlinear in the parameters \\(\\boldsymbol{\\beta}\\); however, it still has that feel of a linear model because of the linear predictor\n\\[\\beta_0 + \\sum_{j=1}^{p} \\beta_j (\\text{Predictor } j)_i.\\]\nNothing prohibits us from considering a nonlinear function instead of the linear predictor; it is just common practice to use the linear predictor, as we have already seen it is flexible enough to accommodate categorical predictors and curvature.\nFor a logistic regression, the \\(j\\)-th coefficient \\(\\beta_j\\) is the log odds ratio of the response occurring when the \\(j\\)-th predictor is increased by one unit compared to its current value, holding all other predictors fixed. For probit regression, the \\(j\\)-th coefficient \\(\\beta_j\\) has no intuitive interpretation (hence the popularity of logistic regression)."
  },
  {
    "objectID": "06g-categorical-reg.html#considerations-for-count-data",
    "href": "06g-categorical-reg.html#considerations-for-count-data",
    "title": "28  Regression Models for Categorical Responses",
    "section": "28.2 Considerations for Count Data",
    "text": "28.2 Considerations for Count Data\nAs in the binary response setting, we take care to ensure that the functional form relating the predictors to key parameters in the conditional distribution of the response enforces constraints on the support.\nConsider a response which counts the number of successes out of a fixed number of trials. We might consider a model of the form\n\\[\n\\begin{aligned}\n  (\\text{Response})_i &\\mid (\\text{Predictors})_i, \\boldsymbol{\\beta} \\stackrel{\\text{Ind}}{\\sim}Bin\\left(m_i, \\theta_i\\right) \\\\\n\\log\\left(\\frac{\\theta_i}{1-\\theta_i}\\right) &= \\beta_0 + \\sum_{j=1}^{p} \\beta_j (\\text{Predictor } j)_i.\n\\end{aligned}\n\\]\nWhile we have written this in a slightly different form, we are using the same functional form for linking the response probability to the predictors; here, we have written it in terms of the link function.\n\nDefinition 28.2 (Link Function) The functional form “linking” the linear predictor\n\\[\\beta_0 + \\sum_{j=1}^{p} \\beta_j (\\text{Predictor } j)_i\\]\nto the mean response of the model in a regression model. In particular, it is the function \\(g\\) such that\n\\[g\\left(\\theta_i\\right) = \\beta_0 + \\sum_{j=1}^{p} \\beta_j (\\text{Predictor } j)_i.\\]\nCommon link functions include:\n\nIdentity link: \\(g\\left(\\theta_i\\right) = \\theta_i\\)\nLogit link: \\(g\\left(\\theta_i\\right) = \\log\\left(\\frac{\\theta_i}{1 - \\theta_i}\\right)\\)\nLog link: \\(g\\left(\\theta_i\\right) = \\log\\left(\\theta_i\\right)\\)\nInverse link: \\(g\\left(\\theta_i\\right) = \\frac{1}{\\theta_i}\\)\nNegative inverse link: \\(g\\left(\\theta_i\\right) = -\\frac{1}{\\theta_i}\\)\nInverse squared link: \\(g\\left(\\theta_i\\right) = \\frac{1}{\\theta_i^2}\\)\n\n\nThe logistic distribution function is chosen since \\(\\theta_i\\) should be constrained to the interval \\((0, 1)\\). Notice that using the logit link for the success probability in the Binomial distribution on the response does not specify the mean response directly; instead, it is simply used to link the predictors to the mean response such that\n\\[E\\left[(\\text{Response}) \\mid (\\text{Predictors}), \\boldsymbol{\\beta}\\right] = m  \\frac{e^{\\beta_0 + \\sum_{j=1}^{p} \\beta_j (\\text{Predictor } j)}}{1 + e^{\\beta_0 + \\sum_{j=1}^{p} \\beta_j (\\text{Predictor } j)}},\\]\nwhere \\(m\\) is the number of trials.\n\n\n\n\n\n\nNote\n\n\n\nAny distribution for which the mean response is defined through the “success” probability (Bernoulli, Binomial, Geometric) could potentially make use of the logit link.\n\n\nThe Poisson distribution is common for modeling the number of rare events in a large population, or for arbitrary counts that have no upper bound. Extending this into a regression model generally takes the form\n\\[\n\\begin{aligned}\n  (\\text{Response})_i &\\mid (\\text{Predictors})_i, \\boldsymbol{\\beta} \\stackrel{\\text{Ind}}{\\sim}Poisson\\left(\\lambda_i\\right) \\\\\n  \\log\\left(\\lambda_i\\right) &= \\beta_0 + \\sum_{j=1}^{p} \\beta_j (\\text{Predictor } j)_i.\n\\end{aligned}\n\\]\nThe log link function is chosen to ensure that \\(\\lambda &gt; 0\\) for all possible choices of the parameter vector \\(\\boldsymbol{\\beta}\\) and predictors.\nIn each of the cases considered in this chapter, the model specifies the mean response; however, it also specifies the variability in the response. For example, the mean of a Poisson distribution is \\(\\lambda_i\\), and the variance is also given by \\(\\lambda_i\\). These distributional models are unique in that knowing the mean response uniquely determines the variability in the response. Occasionally, we come across data for which the response behaves nearly like one of these distributions; however, additional variability is present. There are ways of generalizing these models to account for this additional dispersion."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Doyle, Sir Arthur Conan. 1890. The Sign of the Four. Spencer\nBlackett.\n\n\nDudeck, A E, and C H Peeacock. 1981. “Effects of Several\nOverseeded Ryegrasses on Turf Quality, Traffic Tolerance and Ball\nRoll.” In Proceedings of the Fourth International Turfgrass\nResearch Conference, edited by R W Sheard, 75–81.\n\n\nGoldstein, Bernard D, Howard J Osofsky, and Maureen Y Lichtveld. 2011.\n“The Gulf Oil Spill.” The New England Journal of\nMedicine 364: 1334–48. https://doi.org/10.1056/NEJMra1007197.\n\n\nJohnson, Eric J, and Daniel Goldstein. 2003. “Do Defaults Save\nLives?” Science 302: 1338–39.\n\n\nKruschke, John K. 2015. Doing Bayesian Data Analysis: A Tutorial\nwith r, JAGS, and Stan. 2nd ed. Elsevier.\n\n\nLee, J. 1992. “Relationships Between Properties of Pulp-Fibre and\nPaper.”\n\n\nTintle, Nathan, Beth L Chance, A J Rossman, S Roy, T Swanson, and J\nVanderStoep. 2015. Introduction to Statistical Investigations.\nWiley."
  },
  {
    "objectID": "app-glossary.html",
    "href": "app-glossary.html",
    "title": "Appendix A — Glossary",
    "section": "",
    "text": "The following key terms were defined in the text; each term is presented with a link to where the term was first encountered in the text.\n\nAlternative Hypothesis (Definition 5.10)\n\nThe statement (or theory) about the parameter capturing what we would like to provide evidence for; this is the opposite of the null hypothesis. This is denoted \\(H_1\\) or \\(H_a\\), read “H-one” and “H-A” respectively.\n\nAverage (Definition 7.2)\n\nAlso known as the “mean,” this measure of location represents the balance point for the distribution. If \\(x_i\\) represents the \\(i\\)-th value of the variable \\(x\\) in the sample, the sample mean is typically denoted by \\(\\bar{x}\\).\n\n\nFor a sample of size \\(n\\), it is computed by \\[\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i.\\]\nWhen referencing the average for a population, the mean is also called the “Expected Value,” and is often denoted by \\(\\mu\\).\n\nAxioms of Probability (Definition 1.3)\n\nLet \\(\\mathcal{S}\\) be the sample space of a random process. Suppose that to each event \\(A\\) within \\(\\mathcal{S}\\), a number denoted by \\(Pr(A)\\) is associated with \\(A\\). If the map \\(Pr(\\cdot)\\) satisfies the following three axioms, then it is called a probability:\n\n\n\n\\(Pr(A) \\geq 0\\)\n\\(Pr(\\mathcal{S}) = 1\\)\nIf \\(\\left\\{A_1, A_2, \\dotsc\\right\\}\\) is a sequence of mutually exclusive events in \\(\\mathcal{S}\\), then\n\n\\[Pr\\left(\\bigcup_{i = 1}^{\\infty} A_i\\right) = \\sum_{i = 1}^{\\infty} Pr\\left(A_i\\right).\\]\n\\(Pr(A)\\) is said to be the “probability of \\(A\\)” or the “probability \\(A\\) occurs.”\n\nBayes Factor (Definition 15.2)\n\nA measure of how the observed data alters your prior beliefs about a hypothesis. Let \\(H_j\\) denote the hypothesis that \\(\\theta \\in \\Theta_j\\) for some region \\(\\Theta_j\\). The Bayes Factor in favor of \\(H_j\\) is the ratio of the posterior odds in favor of \\(H_j\\) to the prior odds in favor of \\(H_j\\):\n\n\n\\[BF_j = \\left(\\frac{Pr\\left(\\theta \\in \\Theta_j \\mid \\mathbf{y}\\right)}{Pr\\left(\\theta \\notin \\Theta_j \\mid \\mathbf{y}\\right)}\\right)\\left(\\frac{Pr\\left(\\theta \\notin \\Theta_j\\right)}{Pr\\left(\\theta \\in \\Theta_j\\right)}\\right).\\]\n\nBayes Factor for Model Comparison (Definition 15.5)\n\nThe Bayes Factor, in favor of Model 1, is\n\n\n\\[\n\\begin{aligned}\n  BF_{1} &= \\left(\\frac{Pr(\\mathcal{M}_1 \\mid \\mathbf{y})}{Pr(\\mathcal{M}_0 \\mid \\mathbf{y})}\\right)\\left(\\frac{Pr(\\mathcal{M}_0)}{Pr(\\mathcal{M}_1)}\\right) \\\\\n    &= \\left(\\frac{f_1(\\mathbf{y} \\mid \\mathcal{M}_1) Pr(\\mathcal{M}_1)}{f_0(\\mathbf{y} \\mid \\mathcal{M}_0) Pr(\\mathcal{M}_0)}\\right)\\left(\\frac{Pr(\\mathcal{M}_0)}{Pr(\\mathcal{M}_1)}\\right) \\\\\n    &= \\frac{f_1(\\mathbf{y} \\mid \\mathcal{M}_1)}{f_0(\\mathbf{y} \\mid \\mathcal{M}_0)}.\n\\end{aligned}\n\\]\nThat is, the Bayes Factor is a ratio of the evidence for each model.\n\nBias (Definition 6.1)\n\nA set of measurements is said to be biased if they are consistently too high (or too low). Similarly, an estimate of a parameter is said to be biased if it is consistently too high (or too low).\n\nBlocking (Definition 20.7)\n\nBlocking is a way of minimizing the variability contributed by an inherent characteristic that results in dependent observations. In some cases, the blocks are the unit of observation which is sampled from a larger population, and multiple observations are taken on each unit. In other cases, the blocks are formed by grouping the units of observations according to an inherent characteristic; in these cases that shared characteristic can be thought of having a value that was sampled from a larger population.\n\n\nIn both cases, the observed blocks can be thought of as a random sample; within each block, we have multiple observations, and the observations from the same block are more similar than observations from different blocks.\n\nBridge Sampling (Definition 21.1)\n\nThe bridge sampling estimator of the marginal likelihood \\(m(\\mathbf{y})\\) is given by\n\n\n\\[\n\\begin{aligned}\n  m(\\mathbf{y})\n    &= \\int f(\\mathbf{y} \\mid \\boldsymbol{\\theta}) \\pi(\\boldsymbol{\\theta}) d\\boldsymbol{\\theta} \\\\\n    &= \\frac{E_g\\left[h(\\boldsymbol{\\theta}) f(\\mathbf{y} \\mid \\boldsymbol{\\theta}) \\pi(\\boldsymbol{\\theta})\\right]}{E_{\\pi}\\left[h(\\boldsymbol{\\theta}) g(\\boldsymbol{\\theta}) \\right]} \\\\\n    &\\approx \\frac{m^{-1}\\sum_{j=1}^{m} h\\left(\\tilde{\\boldsymbol{\\theta}}_j\\right) f\\left(\\mathbf{y} \\mid \\tilde{\\boldsymbol{\\theta}}_j\\right) \\pi\\left(\\tilde{\\boldsymbol{\\theta}}_j\\right)}{m^{-1}\\sum_{i=1}^{m} h\\left(\\boldsymbol{\\theta}^*_j\\right) g\\left(\\boldsymbol{\\theta}^*_j\\right)}\n\\end{aligned}\n\\]\nwhere \\(h(\\boldsymbol{\\theta})\\) is called the bridge function and \\(g(\\boldsymbol{\\theta})\\) is the proposal distribution. Here, \\(\\tilde{\\boldsymbol{\\theta}}\\) denotes a random variate from the proposal distribution and \\(\\boldsymbol{\\theta}^*\\) a random variate from the posterior; \\(E_g\\) denotes taking an expectation with respect to the proposal distribution and \\(E_\\pi\\) denotes taking an expectation with respect to the posterior distribution.\n\nCategorical Variable (Definition 3.5)\n\nAlso called a “qualitative variable,” a measurement on a subject which denotes a grouping or categorization.\n\nCodebook (Definition 3.7)\n\nAlso called a “data dictionary,” these provide complete information regarding the variables contained within a dataset.\n\nConditional Density (Definition 9.3)\n\nLet \\(X\\) and \\(Y\\) be two random variables; the conditional density of \\(X\\) given \\(Y\\) is\n\n\n\\[f_{X \\mid Y}(y \\mid x) = \\frac{f_{X, Y}(x, y)}{f_Y(y)}\\]\n\nConditional Density (Definition 9.3)\n\nLet \\(\\mathbf{X}\\) be a random vector; without loss of generality, partition \\(\\mathbf{X}\\) such that\n\n\n\\[\\mathbf{X} = \\begin{pmatrix} \\mathbf{X}_1 \\\\ \\mathbf{X}_2 \\end{pmatrix}\\]\nwhere \\(\\mathbf{X}_1\\) represents the first \\(k\\) components and \\(\\mathbf{X}_2\\) represents the remaining \\(n-k\\) components. Then, the conditional density of \\(\\mathbf{X}_1\\) given \\(\\mathbf{X}_2\\) is\n\\[f_{\\mathbf{X}_1 \\mid \\mathbf{X}_2}(\\mathbf{x}_1 \\mid \\mathbf{x}_2) = \\frac{f_{\\mathbf{X}}(\\mathbf{x})}{f_{\\mathbf{X}_2}(\\mathbf{x}_2)}.\\]\n\nConditional Independence (Definition 14.3)\n\nTwo random variables \\(X\\) and \\(Y\\) are said to be independent, conditional on (or “given”) \\(Z\\) if, and only if,\n\n\n\\[f_{(X,Y) \\mid Z} (x, y \\mid z) = f_{X \\mid Z}(x \\mid z) f_{Y \\mid Z}(y \\mid z).\\]\n\nConfounding (Definition 20.3)\n\nWhen the effect of a variable on the response is mis-represented due to the presence of a third, potentially unobserved, variable known as a confounder.\n\nConjugate Prior (Definition 16.1)\n\nA prior distribution chosen such that the posterior distribution belongs to the same family as the prior distribution, with the (hyper)parameters that govern the family updated based on the observed data.\n\nContinuous and Discrete Random Variable (Definition 2.3)\n\nThe random variable \\(X\\) is said to be a discrete random variable if its corresponding support is countable. The random variable \\(X\\) is said to be a continuous random variable if the corresponding support is uncountable (such as an interval or a union of intervals on the real line).\n\nControlled Experiment (Definition 20.2)\n\nA study in which each subject is randomly assigned to one of the groups being compared in the study.\n\nCredible Interval (Definition 13.3)\n\nA \\(100c\\)% credible interval is an interval \\((a, b)\\) such that\n\n\n\\[Pr(a \\leq \\theta \\leq b \\mid \\mathbf{y}) = \\int_{a}^{b} \\pi(\\theta \\mid \\mathbf{y})d\\theta = c.\\]\n\nCumulative Distribution Function (CDF) (Definition 2.10)\n\nLet \\(X\\) be a random variable; the cumulative distribution function (CDF) is defined as\n\n\n\\[F(u) = Pr(X \\leq u).\\]\nFor a continuous random variable, we have that\n\\[F(u) = \\int_{-\\infty}^{u} f(x) dx\\]\nimplying that the density function is the derivative of the CDF. For a discrete random variable\n\\[F(u) = \\sum_{x \\leq u} f(x).\\]\n\nDensity Function (Definition 2.4)\n\nA density function \\(f\\) relates the values in the support of a random variable with the probability of observing those values.\n\n\nLet \\(X\\) be a continuous random variable, then its density function \\(f\\) is the function such that\n\\[Pr(a \\leq X \\leq b) = \\int_a^b f(x) dx\\]\nfor any real numbers \\(a\\) and \\(b\\) in the support.\nLet \\(X\\) be a discrete random variable, then its density function \\(f\\) is the function such that\n\\[Pr(X = u) = f(u)\\]\nfor any real number \\(u\\) in the support.\n\nDirac Delta Function (Definition 10.3)\n\nThe Dirac delta function is the function (not in a rigorous sense) \\(\\delta\\) such that\n\n\n\\[\\int_{-\\infty}^{\\infty} \\delta(x) dx = 1\\]\nand\n\\[\\int_{-\\infty}^{\\infty} f(x) \\delta(x) dx = f(0)\\]\nfor any real-valued function \\(f\\).\nThe Dirac delta function allows us to describe a discrete distribution, which places mass at a single point, as a continuous function on the real line.\n\nDistribution (Definition 5.3)\n\nThe pattern of variability corresponding to a set of values.\n\nDistribution of the Population (Definition 7.9)\n\nThe pattern of variability in values of a variable at the population level. Generally, this is impossible to know, but we might model it.\n\nDistribution of the Sample (Definition 7.6)\n\nThe pattern of variability in the observed values of a variable.\n\nEffective Sample Size (Definition 19.1)\n\nThe effective sample size (ESS) is given by\n\n\n\\[ESS = \\frac{N}{1 + 2\\sum_{k=1}^{\\infty} ACF(k)}\\]\nwhere ACF is the auto-correlation function of degree \\(k\\).\n\nEqual-Tailed Credible Interval (Definition 13.4)\n\nThe equal-tailed credible interval, which is probably the most commonly used in practice, chooses endpoints such that\n\n\n\\[Pr(\\theta &lt; a \\mid \\mathbf{y}) = \\frac{1-c}{2} = Pr(\\theta &gt; b \\mid \\mathbf{y}).\\]\n\nEstimation (Definition 5.7)\n\nUsing the sample to approximate the value of a parameter from the underlying population.\n\nEvent (Definition 1.2)\n\nA subset of the sample space that is of particular interest.\n\nEvidence for a Model (Definition 15.4)\n\nUnder the Model Comparison framework defined above, the evidence for model \\(\\mathcal{M}_j\\) is defined as\n\n\n\\[f_j(\\mathbf{y} \\mid \\mathcal{M}_j) = \\int f_j(\\mathbf{y} \\mid \\theta_j, \\mathcal{M}_j) \\pi_j(\\theta_j \\mid \\mathcal{M}_j) d\\theta_j.\\]\n\nExpectation of a Function (Definition 2.7)\n\nLet \\(X\\) be a random variable with density function \\(f\\) over the support \\(\\mathcal{S}\\), and let \\(g\\) be a real-valued function. Then,\n\n\n\\[E\\left[g(X)\\right] = \\int_{\\mathcal{S}} g(x) f(x) dx\\]\nfor continuous random variables and\n\\[E\\left[g(X)\\right] = \\sum_{\\mathcal{S}} g(x) f(x)\\]\nfor discrete random variables.\n\nExpected Value (Mean) (Definition 2.5)\n\nLet \\(X\\) be a random variable with density function \\(f\\) defined over the support \\(\\mathcal{S}\\). The expected value of a random variable, also called the mean and denoted \\(E(X)\\), is given by\n\n\n\\[E(X) = \\int_{\\mathcal{S}} x f(x) dx\\]\nfor continuous random variables and\n\\[E(X) = \\sum_{\\mathcal{S}} x f(x)\\]\nfor discrete random variables.\n\nExtrapolation (Definition 23.2)\n\nExtrapolation occurs when we use a model to predict outside of the region for which data is available.\n\nFrequency (Definition 5.4)\n\nThe number of observations in a sample falling into a particular group (level) defined by a categorical variable.\n\nFrequentist Interpretation of Probability (Definition 1.5)\n\nIn this perspective, the probability of \\(A\\) describes the long-run behavior of the event. Specifically, consider repeating the random process \\(m\\) times, and let \\(f(A)\\) represent the number of times the event \\(A\\) occurs out of those \\(m\\) replications. Then,\n\n\n\\[Pr(A) = \\lim_{m \\rightarrow \\infty} \\frac{f(A)}{m}.\\]\n\nGeneral Mixture Distribution (Definition 16.3)\n\nLet \\(\\theta\\) be a parameter with support \\(\\Theta\\), and let \\(\\pi_k(\\theta)\\) be a valid distribution on the support, for \\(k = 1, 2, \\dotsc, K\\). Then,\n\n\n\\[\\pi(\\theta) = \\sum_{k=1}^{K} w_k \\pi_k(\\theta)\\]\nis a valid prior distribution provided \\(\\sum_{k=1}^{K} w_k = 1\\).\n\nHighest Density Interval (Definition 13.5)\n\nThe highest density interval, often called an HDI or HPD (for highest posterior density), chooses the endpoints such that the interval is as short as possible.\n\n\nWhen the density is unimodal, this can be accomplished by choosing the endpoints \\(a\\) and \\(b\\) such that\n\\[\\pi(\\theta \\mid \\mathbf{y}) \\mid_{\\theta = a} = \\pi(\\theta \\mid \\mathbf{y}) \\mid_{\\theta = b}\\]\nand\n\\[\\int_{a}^{b} \\pi(\\theta \\mid \\mathbf{y} d\\theta = c.\\]\n\nHistogram Approach to Constructing a Prior (Definition 16.2)\n\nUsing expert information, attach probability to various intervals for the parameter. Specifically,\n\n\n\nDefine \\(m\\) intervals \\(\\left(\\theta_{j-1}, \\theta_j\\right)\\) for \\(j = 1, 2, \\dotsc, m\\) that partition the parameter space; define \\(\\theta_0\\) as the lower bound of the support for the parameter, and define \\(\\theta_m\\) as the upper bound of the support for the parameter.\nEliciting expert opinions, assign probability \\(\\pi_j\\) to each interval: \\(\\pi_j = Pr\\left(\\theta_{j-1} &lt; \\theta &lt; \\theta_j\\right)\\) for each \\(j = 1, 2, \\dotsc, m\\).\nSet the prior \\(\\pi(\\theta)\\) to be the piecewise distribution over this interval where \\(\\sum_{j=1}^{m} \\pi_j = 1\\).\n\n\nHyperparameter (Definition 10.2)\n\nA constant term of a prior distribution that characterizes the family we are considering.\n\nHypothesis Testing (Definition 5.8)\n\nUsing a sample to determine if the data is consistent with a working theory or if there is evidence to suggest the data is not consistent with the theory.\n\nIdentically Distributed (Definition 9.5)\n\nWe say that random variables \\(X\\) and \\(Y\\) are identically distributed if \\(F_X(u) = F_Y(u)\\) for all \\(u\\). This is equivalent to saying the two random variables have the same density function \\(f\\).\n\nIndependence (Definition 9.4)\n\nRandom variables \\(X_1, X_2, \\dotsc, X_n\\) are said to be mutually independent (or just “independent”) if and only if\n\n\n\\[Pr\\left(X_1 \\in A_1, X_2 \\in A_2, \\dotsb, X_n \\in A_n\\right) = \\prod_{i=1}^{n} Pr\\left(X_i \\in A_i\\right),\\]\nwhere \\(A_1, A_2, \\dotsc, A_n\\) are arbitrary sets. Perhaps more helpful, \\(X_1, X_2, \\dotsc, X_n\\) are said to be mutually independent if and only if\n\\[f_{\\mathbf{X}}(\\mathbf{x}) = \\prod_{i=1}^{n} f_{X_i}\\left(x_i\\right).\\]\n\nIndicator Variable (Definition 24.1)\n\nAn indicator variable is a binary variable (takes on the value 0 or 1), taking the value 1 when a specific event occurs. A collection of \\(k-1\\) indicator variables can be used to capture a categorical variable with \\(k\\) levels in a regression model.\n\n\n\nThe “reference group” (or reference level) is the group (level) defined by setting all indicator variables in a regression model to 0.\n\n\nInterquartile Range (Definition 7.5)\n\nOften abbreviated as IQR, this is the distance between the first and third quartiles. This measure of spread indicates the range over which the middle 50% of the data is spread.\n\nInterval Estimation (Definition 13.2)\n\nInterval estimation is the process of estimating a parameter with a range of values. This is like trying to capture a target with a ring.\n\nJoint Density (Definition 9.1)\n\nFor a random vector \\(\\mathbf{X}\\), the function \\(f_{\\mathbf{X}}(\\mathbf{x})\\) such that for any set \\(A \\in \\mathbb{R}^n\\), we have\n\n\n\\[Pr(\\mathbf{X} \\in A) = \\int \\dotsi \\int_{A} f_{\\mathbf{X}}(\\mathbf{x}) dx_1 \\dotsb dx_n\\]\nis called the joint density function; this is also referred to as the likelihood. Integrals are replaced by sums when appropriate.\n\nKernel of a Distribution (Definition 2.9)\n\nLet \\(k(x)\\) be a non-negative function of \\(x\\) over some region \\(\\mathcal{S}_X\\). Then, a valid density function \\(f\\) over the support \\(\\mathcal{S}_X\\) can be constructed by taking\n\n\n\\[f(x) = a k(x)\\]\nwhere \\(a &gt; 0\\) is a suitably chosen scaling constant to ensure the density integrates (or sums) to 1 over the support. The function \\(k\\) is known as the kernel of the distribution, and it can be used to identify the distributional family for a random variable.\n\nLaplace Prior (Definition 16.4)\n\nThe Laplace prior, also known as a “flat” prior, considers the form\n\n\n\\[\\pi(\\theta) = 1 \\qquad \\forall \\theta \\in \\Theta.\\]\n\nLink Function (Definition 28.2)\n\nThe functional form “linking” the linear predictor\n\n\n\\[\\beta_0 + \\sum_{j=1}^{p} \\beta_j (\\text{Predictor } j)_i\\]\nto the mean response of the model in a regression model. In particular, it is the function \\(g\\) such that\n\\[g\\left(\\theta_i\\right) = \\beta_0 + \\sum_{j=1}^{p} \\beta_j (\\text{Predictor } j)_i.\\]\nCommon link functions include:\n\nIdentity link: \\(g\\left(\\theta_i\\right) = \\theta_i\\)\nLogit link: \\(g\\left(\\theta_i\\right) = \\log\\left(\\frac{\\theta_i}{1 - \\theta_i}\\right)\\)\nLog link: \\(g\\left(\\theta_i\\right) = \\log\\left(\\theta_i\\right)\\)\nInverse link: \\(g\\left(\\theta_i\\right) = \\frac{1}{\\theta_i}\\)\nNegative inverse link: \\(g\\left(\\theta_i\\right) = -\\frac{1}{\\theta_i}\\)\nInverse squared link: \\(g\\left(\\theta_i\\right) = \\frac{1}{\\theta_i^2}\\)\n\n\nLogistic Regression (Definition 28.1)\n\nGiven a binary response; logistic regression assumes that\n\n\n\\[\n\\begin{aligned}\n  (\\text{Response})_i &\\mid (\\text{Predictors})_i, \\boldsymbol{\\beta} \\stackrel{\\text{Ind}}{\\sim}Ber\\left(\\theta_i\\right) \\\\\n  \\theta_i &= \\frac{e^{\\beta_0 + \\sum_{j=1}^{p} \\beta_j (\\text{Predictor } j)_i}}{1 + e^{\\beta_0 + \\sum_{j=1}^{p} \\beta_j (\\text{Predictor } j)_i}}.\n\\end{aligned}\n\\]\n\nMarginal Density (Definition 9.2)\n\nFor a random vector \\(\\mathbf{X}\\), the marginal density of the first component \\(X_1\\) (without loss of generality) is\n\n\n\\[f_{X_1}(u) = \\int \\dotsi \\int f_{\\mathbf{X}}(\\mathbf{x}) dx_2 \\dotsb dx_n.\\]\n\nMarkov Chain (Definition 18.2)\n\nA sequence of random vectors \\(\\theta^{(0)}, \\theta^{(1)}, \\theta^{(2)}, \\dotsc, \\theta^{(n)}\\) is a Markov Chain with stationary transition probabilities if for any set \\(A\\) and any \\(k \\leq n\\)\n\n\n\\[\n\\begin{aligned}\n  Pr\\left(\\theta^{(k)} \\in A \\mid \\theta^{(1)}, \\theta^{(2)}, \\dotsc, \\theta^{(k-1)}\\right)\n    &= Pr\\left(\\theta^{(k)} \\in A \\mid \\theta^{(k-1)}\\right) \\\\\n    &= \\int_{A} q\\left(\\theta^{(k)} \\mid \\theta^{(k-1)}\\right) d\\theta^{(k)}\n\\end{aligned}\n\\]\nwhere \\(q\\) is called the transition kernel.\n\nMethod of Distribution Functions (Definition 2.11)\n\nLet \\(X\\) be a continuous random variable with density \\(f\\) and cumulative distribution function \\(F\\). Consider \\(Y = h(X)\\). The following process provides the density function \\(g\\) of \\(Y\\) by first finding its cumulative distribution function \\(G\\).\n\n\n\nFind the set \\(A\\) for which \\(h(X) \\leq t\\) if and only if \\(X \\in A\\).\nRecognize that \\(G(y) = Pr(Y \\leq y) = Pr\\left(h(X) \\leq y\\right) = Pr(X \\in A)\\).\nIf interested in \\(g(y)\\), note that \\(g(y) = \\frac{\\partial}{\\partial y} G(y)\\).\n\n\nMetropolis Algorithm (Definition 18.1)\n\nSuppose we want to generate random variates from the density \\(\\pi(\\theta \\mid \\mathbf{y})\\). We perform the following steps:\n\n\n\nGenerate an initial value \\(\\theta^{(0)}\\).\n\nAt the \\(k\\)-th step, generate \\(\\theta^*\\) (a candidate) according to a symmetric proposal density \\(q\\left(\\theta \\mid \\theta^{(k-1)}\\right)\\).\n\nCompute \\(A\\left(\\theta^*, \\theta^{(k-1)}\\right)\\) where \\[A\\left(\\theta^*, \\theta^{(k-1)}\\right) = \\frac{\\pi\\left(\\theta^* \\mid \\mathbf{y}\\right)}{\\pi\\left(\\theta^{(k-1)} \\mid \\mathbf{y}\\right)} = \\frac{f\\left(\\mathbf{y} \\mid \\theta^*\\right) \\pi\\left(\\theta^*\\right)}{f\\left(\\mathbf{y} \\mid \\theta^{(k-1)}\\right) \\pi\\left(\\theta^{(k-1)}\\right)}.\\]\nGenerate \\(U \\sim Unif(0,1)\\). If \\(U \\leq A\\left(\\theta^*, \\theta^{(k-1)}\\right)\\), then set \\(\\theta^{(k)} = \\theta^*\\); else, set \\(\\theta^{(k)} = \\theta^{(k-1)}\\).\nRepeat Steps 2-4 \\(m\\) times, for some large \\(m\\).\n\nWhen generating an initial value, \\(\\theta^{(0)}\\), we could choose \\(\\theta^{(0)} \\sim \\pi(\\theta)\\) if the prior is easy to generate from. While it is common to choose \\(q(\\cdot)\\) to be a Normal distribution with mean \\(\\theta^{(k-1)}\\), it is not a requirement to do so; when a Normal distribution is used, it can be difficult to determine a reasonable variance (too large, and you drift too far away; too small, and you do not move at all).\n\nMixture Distribution (Definition 15.3)\n\nLet \\(X\\) be a random variable and \\(f(x)\\) and \\(g(x)\\) be valid density functions defined on a common support. Then,\n\n\n\\[h(x) = wf(x) + (1 - w) g(x),\\]\nwhere \\(0 &lt; w &lt; 1\\), is known as a mixture distribution.\n\nMonte Carlo Error (Definition 17.2)\n\nAlso called the standard error for an approximation of the form \\(m^{-1} \\sum\\limits_{k=1}^{m} g\\left(X_k\\right)\\), the MC error is given by\n\n\n\\[\\sqrt{\\frac{1}{m(m-1)} \\sum_{k=1}^{m} \\left[g\\left(X_k\\right) - \\frac{1}{m} \\sum_{j=1}^{m} g\\left(X_j\\right)\\right]^2}\\]\nwhich is the sample standard deviation of the generated variates divided by the square root of the number of replications.\n\nMonte Carlo Integration (Definition 17.1)\n\nConsider an integral of the form\n\n\n\\[\\int_{\\mathcal{S}} g(x) f(x) dx\\]\nwhere \\(f(x)\\) is a valid density function for a random variable \\(X\\) with support \\(\\mathcal{S}\\). Then, the following algorithm, known as Monte Carlo (or MC) Integration, gives a numerical approximation to the integral:\n\nTake a random sample \\(X_1, X_2, \\dotsc, X_m\\) such that \\(X_i \\sim f(x)\\) for all \\(i\\), where \\(m\\) is large.\nCompute \\(m^{-1} \\sum_{i=1}^{m} g\\left(X_i\\right)\\).\n\nBy the Law of Large Numbers,\n\\[\\frac{1}{m} \\sum_{i=1}^{m} g\\left(X_i\\right) \\approx \\int_{\\mathcal{S}} g(x) f(x) dx.\\]\n\nNoninformative Prior (Definition 16.5)\n\nA prior distribution which is derived solely based on the form of the likelihood.\n\nNull Hypothesis (Definition 5.9)\n\nThe statement (or theory) about the parameter that we would like to disprove. This is denoted \\(H_0\\), read “H-naught” or “H-zero”.\n\nNull Value (Definition 5.11)\n\nThe value associated with the equality component of the null hypothesis; it forms the threshold or boundary between the hypotheses. Note: not all questions of interest require a null value be specified.\n\nNumeric Variable (Definition 3.6)\n\nAlso called a “quantitative variable,” a measurement on a subject which takes on a numeric value and for which ordinary arithmetic makes sense.\n\nObservational Study (Definition 20.1)\n\nA study in which each subject “self-selects” into one of groups being compared in the study. The phrase “self-selects” is used very loosely here and can include studies for which the groups are defined by an inherent characteristic or are chosen haphazardly.\n\nOutlier (Definition 7.7)\n\nAn individual observation which is so extreme, relative to the rest of the observations in the sample, that it does not appear to conform to the same distribution.\n\nParameter (Definition 5.6)\n\nNumeric quantity which summarizes the distribution of a variable within the population of interest. Generally denoted by Greek letters in statistical formulas.\n\nPercentile (Definition 7.1)\n\nThe \\(k\\)-th percentile is the value \\(q\\) such that \\(k\\)% of the values in the distribution are less than or equal to \\(q\\). For example,\n\n\n\n25% of values in a distribution are less than or equal to the 25-th percentile (known as the “first quartile” and denoted \\(Q_1\\)).\n50% of values in a distribution are less than or equal to the 50-th percentile (known as the “median”).\n75% of values in a distribution are less than or equal to the 75-th percentile (known as the “third quartile” and denoted \\(Q_3\\)).\n\n\nPercentile for a Random Variable (Definition 2.8)\n\nLet \\(X\\) be a random variable with density function \\(f\\). The \\(100k\\) percentile is the value \\(q\\) such that\n\n\n\\[Pr(X \\leq q) = k.\\]\n\nPoint Estimation (Definition 13.1)\n\nPoint estimation is the process of estimating a parameter with a single statistic. This is like trying to hit an infinitesimally small target with a dart.\n\nPopulation (Definition 3.1)\n\nThe collection of subjects we would like to say something about.\n\nPosterior Distribution (Definition 11.1)\n\nA distribution quantifying our beliefs about the uncertainty in the parameter(s) of the underlying sampling distribution after observing data. This is often denoted by \\(\\pi(\\boldsymbol{\\theta} \\mid \\mathbf{y})\\) where \\(\\boldsymbol{\\theta}\\) is the parameter vector and \\(\\mathbf{y}\\) the observe data.\n\n\nGiven the likelihood \\(f(\\mathbf{y} \\mid \\boldsymbol{\\theta})\\) and a prior distribution on the parameters \\(\\pi(\\boldsymbol{\\theta})\\), the posterior distribution is computed using Bayes Theorem:\n\\[\\pi(\\boldsymbol{\\theta} \\mid \\mathbf{y}) = \\frac{f(\\mathbf{y} \\mid \\boldsymbol{\\theta}) \\pi(\\theta)}{\\int f(\\mathbf{y} \\mid \\boldsymbol{\\theta}) \\pi(\\boldsymbol{\\theta}) d\\boldsymbol{\\theta}}.\\]\n\nPosterior Mean (Definition 12.1)\n\nThe posterior mean is the average value of the parameter, given the data:\n\n\n\\[E\\left[\\boldsymbol{\\theta} \\mid \\mathbf{y}\\right] = \\int \\boldsymbol{\\theta} \\pi(\\theta \\mid \\mathbf{y}) d\\boldsymbol{\\theta}.\\]\n\nPosterior Median (Definition 12.2)\n\nWe are 50% sure, given the data, the parameter falls below the posterior median. Formally, the posterior median is the value \\(q\\) such that\n\n\n\\[0.5 = \\int_{-\\infty}^{q} \\pi(\\theta \\mid \\mathbf{y}) d\\theta.\\]\n\nPosterior Mode (Definition 12.3)\n\nWe think of the posterior mode as the most likely value of the parameter, given the data. If the posterior distribution is continuous, the posterior mode is the value of the parameter that maximizes the posterior distribution. Formally, the posterior mode is given by\n\n\n\\[\\arg \\max_{\\theta} \\pi(\\theta \\mid \\mathbf{y}).\\]\n\nPosterior Odds (Definition 15.1)\n\nLet \\(H_j\\) denote the hypothesis that \\(\\theta \\in \\Theta_j\\) for some region \\(\\Theta_j\\). Then, the posterior odds in favor of \\(H_j\\) is given by\n\n\n\\[\\frac{Pr\\left(\\theta \\in \\Theta_j \\mid \\mathbf{y}\\right)}{Pr\\left(\\theta \\notin \\Theta_j \\mid \\mathbf{y}\\right)}.\\]\n\nPosterior Predictive Distribution (Definition 14.2)\n\nLet \\(\\mathbf{Y}^*\\) represent a collection of \\(m\\) future observations. The distribution of these future observations given the observed data \\(\\mathbf{Y}\\) (of length \\(n\\)), called the posterior predictive distribution, is given by\n\n\n\\[\\pi\\left(\\mathbf{y}^* \\mid \\mathbf{y}\\right) = \\int f\\left(\\mathbf{y}^* \\mid \\theta\\right) \\pi(\\theta \\mid \\mathbf{y}) d\\theta.\\]\n\nPotential (Definition 18.4)\n\nThe potential of a value \\(\\theta\\) is the negative logarithm of the posterior evaluated at \\(\\theta\\). In practice, we need only know the potential up to a constant. That is, it suffices to define the potential as\n\n\n\\[\\text{Potential}(\\theta) = -\\log\\left[f(\\mathbf{y} \\mid \\theta) \\pi(\\theta)\\right].\\]\n\nPrior Distribution (Definition 10.1)\n\nA distribution quantifying our beliefs about uncertainty in the parameter(s) of the underlying sampling distribution prior to observing any data. This is often denoted by \\(\\pi(\\boldsymbol{\\theta})\\) where \\(\\boldsymbol{\\theta}\\) is the parameter vector.\n\n\n\nThis relies on a subjective view of probability.\nAs prior beliefs are subjective, there is no “one” prior, but each individual may have a unique prior.\n\n\nPrior Predictive Distribution (Definition 14.1)\n\nThe prior predictive distribution is the marginal distribution of the response(s) prior to observing any data:\n\n\n\\[m(\\mathbf{y}) = \\int f(\\mathbf{y} \\mid \\theta) \\pi(\\theta) d\\theta.\\]\nThe distribution marginalizes the parameter out of the likelihood using the beliefs from the prior distribution.\n\nRandom Sample (Definition 9.6)\n\nA random sample of size \\(n\\) refers to a collection of \\(n\\) random variables \\(X_1, X_2, \\dotsc, X_n\\) such that the random variables are mutually independent, and the distribution of each random variable is identical.\n\nRandom Variable (Definition 2.1)\n\nLet \\(\\mathcal{S}\\) be the sample space corresponding to a random process; a random variable \\(X\\) is a function mapping elements of the sample space to the real line.\n\n\nRandom variables represent a measurement that will be collected during the course of a study. Random variables are typically represented by a capital letter.\n\nRandom Vector (Definition 8.2)\n\nLet \\(X_1, X_2, \\dots, X_n\\) be \\(n\\) random variables. Then, the vector \\(\\mathbf{X} = \\left(X_1, X_2, \\dots, X_n\\right)^\\top\\) is a random vector of length \\(n\\).\n\nRandomization (Definition 20.5)\n\nRandomization can refer to random selection or random allocation. Random selection refers to the use of a random mechanism (e.g., a simple random sample, Definition 6.2, or a stratified random sample, Definition 6.3) to select units from the population. Random selection minimizes bias.\n\n\nRandom allocation refers to the use of a random mechanism when assigning units to a specific treatment group in a controlled experiment (Definition 20.2). Random allocation eliminates confounding and permits causal interpretations.\n\nReduction of Noise (Definition 20.6)\n\nReducing extraneous sources of variability can be accomplished by fixing extraneous variables or blocking (Definition 20.7). These actions reduce the number of differences between the units under study.\n\nRegression (Definition 23.1)\n\nA regression model is one for which the parameter(s) governing the data generating process depends on one or more predictors. “Parametric” regression models do this through specifying a functional form for the dependence of the parameter(s) on the predictor(s).\n\nRelative Frequency (Definition 5.5)\n\nAlso called the “proportion,” the fraction of observations falling into a particular group (level) of a categorical variable.\n\nReplication (Definition 20.4)\n\nReplication results from taking measurements on different units (or subjects), for which you expect the results to be similar. That is, any variability across the units is due to natural variability within the population.\n\nResidual (Definition 27.1)\n\nA residual is the difference between an observed response and the predicted mean response for that same individual:\n\n\n\\[(\\text{Residual})_i = (\\text{Response})_i - (\\text{Fitted Value})_i,\\]\nwhere\n\\[(\\text{Fitted Value})_i = \\widehat{\\beta}_0 + \\sum_{j=1}^{p} \\widehat{\\beta}_j (\\text{Predictor} j)_i.\\]\n\nResponse (Definition 5.2)\n\nThe primary variable of interest within a study. This is the variable you would either like to explain or estimate.\n\nSample (Definition 3.2)\n\nThe collection of subjects for which we actually obtain measurements (data).\n\nSample Space (Definition 1.1)\n\nThe sample space for a random process is the collection of all possible results that we might observe.\n\nSimple Random Sample (Definition 6.2)\n\nOften abbreviated SRS, this is a sample of size \\(n\\) such that every collection of size \\(n\\) is equally likely to be the resulting sample. This is equivalent to a lottery.\n\nStandard Deviation (Definition 7.4)\n\nA measure of spread, this is the square root of the variance.\n\nStationary Distribution (Definition 18.3)\n\nLet \\(\\theta^{(0)}, \\theta^{(1)}, \\theta^{(2)}, \\dotsc, \\theta^{(n)}\\) be a Markov Chain. The stationary distribution of the Markov Chain is the distribution \\(p(\\theta)\\) such that\n\n\n\\[Pr\\left(\\theta^{(k)} \\in A\\right) = \\int_{A} p(\\theta) d\\theta.\\]\n\nStatistic (Definition 7.8)\n\nNumeric quantity which summarizes the distribution of a variable within a sample.\n\nStatistical Inference (Definition 3.3)\n\nThe process of using a sample to characterize some aspect of the underlying population.\n\nStratified Random Sample (Definition 6.3)\n\nA sample in which the population is first divided into groups, or strata, based on a characteristic of interest; a simple random sample is then taken within each group.\n\nSubjective Interpretation of Probability (Definition 1.4)\n\nIn this perspective, the probability of \\(A\\) describes the individual’s uncertainty about event \\(A\\).\n\nSupport (Definition 2.2)\n\nThe support of a random variable is the set of all possible values the random variable can take.\n\nVariability (Definition 5.1)\n\nThe notion that measurements differ from one observation to another.\n\nVariable (Definition 3.4)\n\nA measurement, or category, describing some aspect of the subject.\n\nVariance (Definition 7.3)\n\nLet \\(X\\) be a random variable with density function \\(f\\) defined over the support \\(\\mathcal{S}\\). The variance of a random variable, denoted \\(Var(X)\\), is given by\n\n\n\\[Var(X) = E\\left[X - E(X)\\right]^2 = E\\left(X^2\\right) - E^2(X).\\]\nIf we let \\(\\mu = E(X)\\), then this is equivalent to\n\\[\\int_{\\mathcal{S}} (x - \\mu)^2 f(x) dx\\]\nfor continuous random variables and\n\\[\\sum_{\\mathcal{S}} (x - \\mu)^2 f(x)\\]\nfor discrete random variables.\n\nVariance (Definition 7.3)\n\nA measure of spread, this roughly captures the average distance values in the distribution are from the mean.\n\n\nFor a sample of size \\(n\\), it is computed by \\[s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} \\left(x_i - \\bar{x}\\right)^2\\]\nwhere \\(\\bar{x}\\) is the sample mean and \\(x_i\\) is the \\(i\\)-th value in the sample. The division by \\(n-1\\) instead of \\(n\\) removes bias in the statistic.\nThe symbol \\(\\sigma^2\\) is often used to denote the variance in the population."
  }
]