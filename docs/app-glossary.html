<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Bayesian Data Analysis - Appendix A — Glossary</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./references.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="mystyles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./app-glossary.html">Appendices</a></li><li class="breadcrumb-item"><a href="./app-glossary.html"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Glossary</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayesian Data Analysis</a> 
        <div class="sidebar-tools-main">
    <a href="./ma483-text.pdf" rel="" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./01a-probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Unit I: Essential Probability</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01b-fundamentals.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Essential Probability</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01c-randomvariables.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Random Variables and Distributions</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./02a-language.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Unit II: Language of Data</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02b-basics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Statistical Process</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02c-casedeepwater.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Case Study: Health Effects of the Deepwater Horizon Oil Spill</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02d-questions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Asking the Right Questions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02e-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Gathering the Evidence (Data Collection)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02f-summaries.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Presenting the Evidence (Summarizing Data)</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./03a-fundamentals.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Unit III: Fundamentals of Bayesian Inference</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03b-bayesrule.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Bayes Rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03c-modelingsamples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Modeling Samples</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03d-priors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Quantifying/Modeling Prior Information</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03e-posteriors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Updating Prior Beliefs (Posterior Distributions)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03f-point-estimation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Point Estimation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03g-interval-estimation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Interval Estimation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03h-prediction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Prediction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03i-hypothesis-testing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03j-constructing-priors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Constructing Prior Distributions</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./04a-computation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Unit IV: Numerical Approaches to Bayesian Computations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04b-mc-integration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Monte Carlo Integration</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04c-mcmc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Markov Chain Monte Carlo (MCMC)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04d-mcmc-assessment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Assessing MCMC Samples</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./05a-comparing-groups.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Unit V: Hierarchical Models for Comparing Groups</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05b-study-design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Elements of Good Study Design</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05c-independent-groups.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Considerations when Comparing Independent Groups</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05d-dependent-groups.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Considerations when Comparing Related Groups</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./06a-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Unit VI: Introduction to Regression Modeling</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06b-linear-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Regression Models for a Quantitative Response</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06c-reg-extensions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Extensions to the Linear Model</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06d-reg-priors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Default Priors in Regression Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06e-qr-factorization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">QR Factorization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06f-reg-conditions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Assessment for Regression Models for the Mean</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06g-categorical-reg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Regression Models for Categorical Responses</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-glossary.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Glossary</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Appendix A — Glossary</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>The following key terms were defined in the text; each term is presented with a link to where the term was first encountered in the text.</p>
<dl>
<dt>Alternative Hypothesis (<a href="02d-questions.html#def-alternative-hypothesis">Definition&nbsp;<span>5.10</span></a>)</dt>
<dd>
The statement (or theory) about the parameter capturing what we would like to provide evidence <em>for</em>; this is the opposite of the null hypothesis. This is denoted <span class="math inline">\(H_1\)</span> or <span class="math inline">\(H_a\)</span>, read “H-one” and “H-A” respectively.
</dd>
<dt>Average (<a href="02f-summaries.html#def-average">Definition&nbsp;<span>7.2</span></a>)</dt>
<dd>
Also known as the “mean,” this measure of location represents the balance point for the distribution. If <span class="math inline">\(x_i\)</span> represents the <span class="math inline">\(i\)</span>-th value of the variable <span class="math inline">\(x\)</span> in the sample, the sample mean is typically denoted by <span class="math inline">\(\bar{x}\)</span>.
</dd>
</dl>
<p>For a sample of size <span class="math inline">\(n\)</span>, it is computed by <span class="math display">\[\bar{x} = \frac{1}{n}\sum_{i=1}^{n} x_i.\]</span></p>
<p>When referencing the average for a population, the mean is also called the “Expected Value,” and is often denoted by <span class="math inline">\(\mu\)</span>.</p>
<dl>
<dt>Axioms of Probability (<a href="01b-fundamentals.html#def-axioms">Definition&nbsp;<span>1.3</span></a>)</dt>
<dd>
Let <span class="math inline">\(\mathcal{S}\)</span> be the sample space of a random process. Suppose that to each event <span class="math inline">\(A\)</span> within <span class="math inline">\(\mathcal{S}\)</span>, a number denoted by <span class="math inline">\(Pr(A)\)</span> is associated with <span class="math inline">\(A\)</span>. If the map <span class="math inline">\(Pr(\cdot)\)</span> satisfies the following three axioms, then it is called a <strong>probability</strong>:
</dd>
</dl>
<ol type="1">
<li><span class="math inline">\(Pr(A) \geq 0\)</span></li>
<li><span class="math inline">\(Pr(\mathcal{S}) = 1\)</span></li>
<li>If <span class="math inline">\(\left\{A_1, A_2, \dotsc\right\}\)</span> is a sequence of mutually exclusive events in <span class="math inline">\(\mathcal{S}\)</span>, then</li>
</ol>
<p><span class="math display">\[Pr\left(\bigcup_{i = 1}^{\infty} A_i\right) = \sum_{i = 1}^{\infty} Pr\left(A_i\right).\]</span></p>
<p><span class="math inline">\(Pr(A)\)</span> is said to be the “probability of <span class="math inline">\(A\)</span>” or the “probability <span class="math inline">\(A\)</span> occurs.”</p>
<dl>
<dt>Bayes Factor (<a href="03i-hypothesis-testing.html#def-bayes-factor">Definition&nbsp;<span>15.2</span></a>)</dt>
<dd>
A measure of how the observed data <em>alters</em> your prior beliefs about a hypothesis. Let <span class="math inline">\(H_j\)</span> denote the hypothesis that <span class="math inline">\(\theta \in \Theta_j\)</span> for some region <span class="math inline">\(\Theta_j\)</span>. The Bayes Factor <em>in favor of</em> <span class="math inline">\(H_j\)</span> is the ratio of the posterior odds in favor of <span class="math inline">\(H_j\)</span> to the prior odds in favor of <span class="math inline">\(H_j\)</span>:
</dd>
</dl>
<p><span class="math display">\[BF_j = \left(\frac{Pr\left(\theta \in \Theta_j \mid \mathbf{y}\right)}{Pr\left(\theta \notin \Theta_j \mid \mathbf{y}\right)}\right)\left(\frac{Pr\left(\theta \notin \Theta_j\right)}{Pr\left(\theta \in \Theta_j\right)}\right).\]</span></p>
<dl>
<dt>Bayes Factor for Model Comparison (<a href="03i-hypothesis-testing.html#def-bayes-factor-models">Definition&nbsp;<span>15.5</span></a>)</dt>
<dd>
The Bayes Factor, in favor of Model 1, is
</dd>
</dl>
<p><span class="math display">\[
\begin{aligned}
  BF_{1} &amp;= \left(\frac{Pr(\mathcal{M}_1 \mid \mathbf{y})}{Pr(\mathcal{M}_0 \mid \mathbf{y})}\right)\left(\frac{Pr(\mathcal{M}_0)}{Pr(\mathcal{M}_1)}\right) \\
    &amp;= \left(\frac{f_1(\mathbf{y} \mid \mathcal{M}_1) Pr(\mathcal{M}_1)}{f_0(\mathbf{y} \mid \mathcal{M}_0) Pr(\mathcal{M}_0)}\right)\left(\frac{Pr(\mathcal{M}_0)}{Pr(\mathcal{M}_1)}\right) \\
    &amp;= \frac{f_1(\mathbf{y} \mid \mathcal{M}_1)}{f_0(\mathbf{y} \mid \mathcal{M}_0)}.
\end{aligned}
\]</span></p>
<p>That is, the Bayes Factor is a ratio of the evidence for each model.</p>
<dl>
<dt>Bias (<a href="02e-data.html#def-bias">Definition&nbsp;<span>6.1</span></a>)</dt>
<dd>
A set of measurements is said to be biased if they are <em>consistently</em> too high (or too low). Similarly, an estimate of a parameter is said to be biased if it is <em>consistently</em> too high (or too low).
</dd>
<dt>Blocking (<a href="05b-study-design.html#def-blocking">Definition&nbsp;<span>20.7</span></a>)</dt>
<dd>
Blocking is a way of minimizing the variability contributed by an inherent characteristic that results in dependent observations. In some cases, the blocks are the unit of observation which is sampled from a larger population, and multiple observations are taken on each unit. In other cases, the blocks are formed by grouping the units of observations according to an inherent characteristic; in these cases that shared characteristic can be thought of having a value that was sampled from a larger population.
</dd>
</dl>
<p>In both cases, the observed blocks can be thought of as a random sample; within each block, we have multiple observations, and the observations from the same block are more similar than observations from different blocks.</p>
<dl>
<dt>Bridge Sampling (<a href="05c-independent-groups.html#def-bridge-sampling">Definition&nbsp;<span>21.1</span></a>)</dt>
<dd>
The bridge sampling estimator of the marginal likelihood <span class="math inline">\(m(\mathbf{y})\)</span> is given by
</dd>
</dl>
<p><span class="math display">\[
\begin{aligned}
  m(\mathbf{y})
    &amp;= \int f(\mathbf{y} \mid \boldsymbol{\theta}) \pi(\boldsymbol{\theta}) d\boldsymbol{\theta} \\
    &amp;= \frac{E_g\left[h(\boldsymbol{\theta}) f(\mathbf{y} \mid \boldsymbol{\theta}) \pi(\boldsymbol{\theta})\right]}{E_{\pi}\left[h(\boldsymbol{\theta}) g(\boldsymbol{\theta}) \right]} \\
    &amp;\approx \frac{m^{-1}\sum_{j=1}^{m} h\left(\tilde{\boldsymbol{\theta}}_j\right) f\left(\mathbf{y} \mid \tilde{\boldsymbol{\theta}}_j\right) \pi\left(\tilde{\boldsymbol{\theta}}_j\right)}{m^{-1}\sum_{i=1}^{m} h\left(\boldsymbol{\theta}^*_j\right) g\left(\boldsymbol{\theta}^*_j\right)}
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(h(\boldsymbol{\theta})\)</span> is called the bridge function and <span class="math inline">\(g(\boldsymbol{\theta})\)</span> is the proposal distribution. Here, <span class="math inline">\(\tilde{\boldsymbol{\theta}}\)</span> denotes a random variate from the proposal distribution and <span class="math inline">\(\boldsymbol{\theta}^*\)</span> a random variate from the posterior; <span class="math inline">\(E_g\)</span> denotes taking an expectation with respect to the proposal distribution and <span class="math inline">\(E_\pi\)</span> denotes taking an expectation with respect to the posterior distribution.</p>
<dl>
<dt>Categorical Variable (<a href="02b-basics.html#def-categorical">Definition&nbsp;<span>3.5</span></a>)</dt>
<dd>
Also called a “qualitative variable,” a measurement on a subject which denotes a grouping or categorization.
</dd>
<dt>Codebook (<a href="02b-basics.html#def-codebook">Definition&nbsp;<span>3.7</span></a>)</dt>
<dd>
Also called a “data dictionary,” these provide complete information regarding the variables contained within a dataset.
</dd>
<dt>Conditional Density (<a href="03c-modelingsamples.html#def-conditional-density">Definition&nbsp;<span>9.3</span></a>)</dt>
<dd>
Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be two random variables; the conditional density of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y\)</span> is
</dd>
</dl>
<p><span class="math display">\[f_{X \mid Y}(y \mid x) = \frac{f_{X, Y}(x, y)}{f_Y(y)}\]</span></p>
<dl>
<dt>Conditional Density (<a href="03c-modelingsamples.html#def-conditional-density">Definition&nbsp;<span>9.3</span></a>)</dt>
<dd>
Let <span class="math inline">\(\mathbf{X}\)</span> be a random vector; without loss of generality, partition <span class="math inline">\(\mathbf{X}\)</span> such that
</dd>
</dl>
<p><span class="math display">\[\mathbf{X} = \begin{pmatrix} \mathbf{X}_1 \\ \mathbf{X}_2 \end{pmatrix}\]</span></p>
<p>where <span class="math inline">\(\mathbf{X}_1\)</span> represents the first <span class="math inline">\(k\)</span> components and <span class="math inline">\(\mathbf{X}_2\)</span> represents the remaining <span class="math inline">\(n-k\)</span> components. Then, the conditional density of <span class="math inline">\(\mathbf{X}_1\)</span> given <span class="math inline">\(\mathbf{X}_2\)</span> is</p>
<p><span class="math display">\[f_{\mathbf{X}_1 \mid \mathbf{X}_2}(\mathbf{x}_1 \mid \mathbf{x}_2) = \frac{f_{\mathbf{X}}(\mathbf{x})}{f_{\mathbf{X}_2}(\mathbf{x}_2)}.\]</span></p>
<dl>
<dt>Conditional Independence (<a href="03h-prediction.html#def-conditional-independence">Definition&nbsp;<span>14.3</span></a>)</dt>
<dd>
Two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are said to be independent, conditional on (or “given”) <span class="math inline">\(Z\)</span> if, and only if,
</dd>
</dl>
<p><span class="math display">\[f_{(X,Y) \mid Z} (x, y \mid z) = f_{X \mid Z}(x \mid z) f_{Y \mid Z}(y \mid z).\]</span></p>
<dl>
<dt>Confounding (<a href="05b-study-design.html#def-confounding">Definition&nbsp;<span>20.3</span></a>)</dt>
<dd>
When the effect of a variable on the response is mis-represented due to the presence of a third, potentially unobserved, variable known as a confounder.
</dd>
<dt>Conjugate Prior (<a href="03j-constructing-priors.html#def-conjugate-prior">Definition&nbsp;<span>16.1</span></a>)</dt>
<dd>
A prior distribution chosen such that the posterior distribution belongs to the same family as the prior distribution, with the (hyper)parameters that govern the family updated based on the observed data.
</dd>
<dt>Continuous and Discrete Random Variable (<a href="01c-randomvariables.html#def-rvtypes">Definition&nbsp;<span>2.3</span></a>)</dt>
<dd>
The random variable <span class="math inline">\(X\)</span> is said to be a discrete random variable if its corresponding support is countable. The random variable <span class="math inline">\(X\)</span> is said to be a continuous random variable if the corresponding support is uncountable (such as an interval or a union of intervals on the real line).
</dd>
<dt>Controlled Experiment (<a href="05b-study-design.html#def-controlled-experiment">Definition&nbsp;<span>20.2</span></a>)</dt>
<dd>
A study in which each subject is <em>randomly</em> assigned to one of the groups being compared in the study.
</dd>
<dt>Credible Interval (<a href="03g-interval-estimation.html#def-credible-interval">Definition&nbsp;<span>13.3</span></a>)</dt>
<dd>
A <span class="math inline">\(100c\)</span>% credible interval is an interval <span class="math inline">\((a, b)\)</span> such that
</dd>
</dl>
<p><span class="math display">\[Pr(a \leq \theta \leq b \mid \mathbf{y}) = \int_{a}^{b} \pi(\theta \mid \mathbf{y})d\theta = c.\]</span></p>
<dl>
<dt>Cumulative Distribution Function (CDF) (<a href="01c-randomvariables.html#def-cdf">Definition&nbsp;<span>2.10</span></a>)</dt>
<dd>
Let <span class="math inline">\(X\)</span> be a random variable; the cumulative distribution function (CDF) is defined as
</dd>
</dl>
<p><span class="math display">\[F(u) = Pr(X \leq u).\]</span></p>
<p>For a continuous random variable, we have that</p>
<p><span class="math display">\[F(u) = \int_{-\infty}^{u} f(x) dx\]</span></p>
<p>implying that the density function is the derivative of the CDF. For a discrete random variable</p>
<p><span class="math display">\[F(u) = \sum_{x \leq u} f(x).\]</span></p>
<dl>
<dt>Density Function (<a href="01c-randomvariables.html#def-density-function">Definition&nbsp;<span>2.4</span></a>)</dt>
<dd>
A density function <span class="math inline">\(f\)</span> relates the values in the support of a random variable with the probability of observing those values.
</dd>
</dl>
<p>Let <span class="math inline">\(X\)</span> be a continuous random variable, then its density function <span class="math inline">\(f\)</span> is the function such that</p>
<p><span class="math display">\[Pr(a \leq X \leq b) = \int_a^b f(x) dx\]</span></p>
<p>for any real numbers <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> in the support.</p>
<p>Let <span class="math inline">\(X\)</span> be a discrete random variable, then its density function <span class="math inline">\(f\)</span> is the function such that</p>
<p><span class="math display">\[Pr(X = u) = f(u)\]</span></p>
<p>for any real number <span class="math inline">\(u\)</span> in the support.</p>
<dl>
<dt>Dirac Delta Function (<a href="03d-priors.html#def-dirac-delta">Definition&nbsp;<span>10.3</span></a>)</dt>
<dd>
The Dirac delta function is the function (not in a rigorous sense) <span class="math inline">\(\delta\)</span> such that
</dd>
</dl>
<p><span class="math display">\[\int_{-\infty}^{\infty} \delta(x) dx = 1\]</span></p>
<p>and</p>
<p><span class="math display">\[\int_{-\infty}^{\infty} f(x) \delta(x) dx = f(0)\]</span></p>
<p>for any real-valued function <span class="math inline">\(f\)</span>.</p>
<p>The Dirac delta function allows us to describe a discrete distribution, which places mass at a single point, as a continuous function on the real line.</p>
<dl>
<dt>Distribution (<a href="02d-questions.html#def-distribution">Definition&nbsp;<span>5.3</span></a>)</dt>
<dd>
The pattern of variability corresponding to a set of values.
</dd>
<dt>Distribution of the Population (<a href="02f-summaries.html#def-distribution-population">Definition&nbsp;<span>7.9</span></a>)</dt>
<dd>
The pattern of variability in values of a variable at the population level. Generally, this is impossible to know, but we might model it.
</dd>
<dt>Distribution of the Sample (<a href="02f-summaries.html#def-distribution-sample">Definition&nbsp;<span>7.6</span></a>)</dt>
<dd>
The pattern of variability in the observed values of a variable.
</dd>
<dt>Effective Sample Size (<a href="04d-mcmc-assessment.html#def-ess">Definition&nbsp;<span>19.1</span></a>)</dt>
<dd>
The effective sample size (ESS) is given by
</dd>
</dl>
<p><span class="math display">\[ESS = \frac{N}{1 + 2\sum_{k=1}^{\infty} ACF(k)}\]</span></p>
<p>where ACF is the auto-correlation function of degree <span class="math inline">\(k\)</span>.</p>
<dl>
<dt>Equal-Tailed Credible Interval (<a href="03g-interval-estimation.html#def-equal-tail-interval">Definition&nbsp;<span>13.4</span></a>)</dt>
<dd>
The equal-tailed credible interval, which is probably the most commonly used in practice, chooses endpoints such that
</dd>
</dl>
<p><span class="math display">\[Pr(\theta &lt; a \mid \mathbf{y}) = \frac{1-c}{2} = Pr(\theta &gt; b \mid \mathbf{y}).\]</span></p>
<dl>
<dt>Estimation (<a href="02d-questions.html#def-estimation">Definition&nbsp;<span>5.7</span></a>)</dt>
<dd>
Using the sample to approximate the value of a parameter from the underlying population.
</dd>
<dt>Event (<a href="01b-fundamentals.html#def-event">Definition&nbsp;<span>1.2</span></a>)</dt>
<dd>
A subset of the sample space that is of particular interest.
</dd>
<dt>Evidence for a Model (<a href="03i-hypothesis-testing.html#def-evidence">Definition&nbsp;<span>15.4</span></a>)</dt>
<dd>
Under the Model Comparison framework defined above, the evidence for model <span class="math inline">\(\mathcal{M}_j\)</span> is defined as
</dd>
</dl>
<p><span class="math display">\[f_j(\mathbf{y} \mid \mathcal{M}_j) = \int f_j(\mathbf{y} \mid \theta_j, \mathcal{M}_j) \pi_j(\theta_j \mid \mathcal{M}_j) d\theta_j.\]</span></p>
<dl>
<dt>Expectation of a Function (<a href="01c-randomvariables.html#def-expectation">Definition&nbsp;<span>2.7</span></a>)</dt>
<dd>
Let <span class="math inline">\(X\)</span> be a random variable with density function <span class="math inline">\(f\)</span> over the support <span class="math inline">\(\mathcal{S}\)</span>, and let <span class="math inline">\(g\)</span> be a real-valued function. Then,
</dd>
</dl>
<p><span class="math display">\[E\left[g(X)\right] = \int_{\mathcal{S}} g(x) f(x) dx\]</span></p>
<p>for continuous random variables and</p>
<p><span class="math display">\[E\left[g(X)\right] = \sum_{\mathcal{S}} g(x) f(x)\]</span></p>
<p>for discrete random variables.</p>
<dl>
<dt>Expected Value (Mean) (<a href="01c-randomvariables.html#def-mean">Definition&nbsp;<span>2.5</span></a>)</dt>
<dd>
Let <span class="math inline">\(X\)</span> be a random variable with density function <span class="math inline">\(f\)</span> defined over the support <span class="math inline">\(\mathcal{S}\)</span>. The expected value of a random variable, also called the mean and denoted <span class="math inline">\(E(X)\)</span>, is given by
</dd>
</dl>
<p><span class="math display">\[E(X) = \int_{\mathcal{S}} x f(x) dx\]</span></p>
<p>for continuous random variables and</p>
<p><span class="math display">\[E(X) = \sum_{\mathcal{S}} x f(x)\]</span></p>
<p>for discrete random variables.</p>
<dl>
<dt>Extrapolation (<a href="06b-linear-regression.html#def-extrapolation">Definition&nbsp;<span>23.2</span></a>)</dt>
<dd>
Extrapolation occurs when we use a model to predict outside of the region for which data is available.
</dd>
<dt>Frequency (<a href="02d-questions.html#def-frequency">Definition&nbsp;<span>5.4</span></a>)</dt>
<dd>
The number of observations in a sample falling into a particular group (level) defined by a categorical variable.
</dd>
<dt>Frequentist Interpretation of Probability (<a href="01b-fundamentals.html#def-frequentist-interpretation">Definition&nbsp;<span>1.5</span></a>)</dt>
<dd>
In this perspective, the probability of <span class="math inline">\(A\)</span> describes the long-run behavior of the event. Specifically, consider repeating the random process <span class="math inline">\(m\)</span> times, and let <span class="math inline">\(f(A)\)</span> represent the number of times the event <span class="math inline">\(A\)</span> occurs out of those <span class="math inline">\(m\)</span> replications. Then,
</dd>
</dl>
<p><span class="math display">\[Pr(A) = \lim_{m \rightarrow \infty} \frac{f(A)}{m}.\]</span></p>
<dl>
<dt>General Mixture Distribution (<a href="03j-constructing-priors.html#def-general-mixture-distribution">Definition&nbsp;<span>16.3</span></a>)</dt>
<dd>
Let <span class="math inline">\(\theta\)</span> be a parameter with support <span class="math inline">\(\Theta\)</span>, and let <span class="math inline">\(\pi_k(\theta)\)</span> be a valid distribution on the support, for <span class="math inline">\(k = 1, 2, \dotsc, K\)</span>. Then,
</dd>
</dl>
<p><span class="math display">\[\pi(\theta) = \sum_{k=1}^{K} w_k \pi_k(\theta)\]</span></p>
<p>is a valid prior distribution provided <span class="math inline">\(\sum_{k=1}^{K} w_k = 1\)</span>.</p>
<dl>
<dt>Highest Density Interval (<a href="03g-interval-estimation.html#def-hdi">Definition&nbsp;<span>13.5</span></a>)</dt>
<dd>
The highest density interval, often called an HDI or HPD (for highest posterior density), chooses the endpoints such that the interval is as short as possible.
</dd>
</dl>
<p>When the density is unimodal, this can be accomplished by choosing the endpoints <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> such that</p>
<p><span class="math display">\[\pi(\theta \mid \mathbf{y}) \mid_{\theta = a} = \pi(\theta \mid \mathbf{y}) \mid_{\theta = b}\]</span></p>
<p>and</p>
<p><span class="math display">\[\int_{a}^{b} \pi(\theta \mid \mathbf{y} d\theta = c.\]</span></p>
<dl>
<dt>Histogram Approach to Constructing a Prior (<a href="03j-constructing-priors.html#def-histogram-prior">Definition&nbsp;<span>16.2</span></a>)</dt>
<dd>
Using expert information, attach probability to various intervals for the parameter. Specifically,
</dd>
</dl>
<ul>
<li>Define <span class="math inline">\(m\)</span> intervals <span class="math inline">\(\left(\theta_{j-1}, \theta_j\right)\)</span> for <span class="math inline">\(j = 1, 2, \dotsc, m\)</span> that partition the parameter space; define <span class="math inline">\(\theta_0\)</span> as the lower bound of the support for the parameter, and define <span class="math inline">\(\theta_m\)</span> as the upper bound of the support for the parameter.</li>
<li>Eliciting expert opinions, assign probability <span class="math inline">\(\pi_j\)</span> to each interval: <span class="math inline">\(\pi_j = Pr\left(\theta_{j-1} &lt; \theta &lt; \theta_j\right)\)</span> for each <span class="math inline">\(j = 1, 2, \dotsc, m\)</span>.</li>
<li>Set the prior <span class="math inline">\(\pi(\theta)\)</span> to be the piecewise distribution over this interval where <span class="math inline">\(\sum_{j=1}^{m} \pi_j = 1\)</span>.</li>
</ul>
<dl>
<dt>Hyperparameter (<a href="03d-priors.html#def-hyperparameter">Definition&nbsp;<span>10.2</span></a>)</dt>
<dd>
A constant term of a prior distribution that characterizes the family we are considering.
</dd>
<dt>Hypothesis Testing (<a href="02d-questions.html#def-hypothesis-testing">Definition&nbsp;<span>5.8</span></a>)</dt>
<dd>
Using a sample to determine if the data is consistent with a working theory or if there is evidence to suggest the data is not consistent with the theory.
</dd>
<dt>Identically Distributed (<a href="03c-modelingsamples.html#def-identically-distributed">Definition&nbsp;<span>9.5</span></a>)</dt>
<dd>
We say that random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are identically distributed if <span class="math inline">\(F_X(u) = F_Y(u)\)</span> for all <span class="math inline">\(u\)</span>. This is equivalent to saying the two random variables have the same density function <span class="math inline">\(f\)</span>.
</dd>
<dt>Independence (<a href="03c-modelingsamples.html#def-independence">Definition&nbsp;<span>9.4</span></a>)</dt>
<dd>
Random variables <span class="math inline">\(X_1, X_2, \dotsc, X_n\)</span> are said to be mutually independent (or just “independent”) if and only if
</dd>
</dl>
<p><span class="math display">\[Pr\left(X_1 \in A_1, X_2 \in A_2, \dotsb, X_n \in A_n\right) = \prod_{i=1}^{n} Pr\left(X_i \in A_i\right),\]</span></p>
<p>where <span class="math inline">\(A_1, A_2, \dotsc, A_n\)</span> are arbitrary sets. Perhaps more helpful, <span class="math inline">\(X_1, X_2, \dotsc, X_n\)</span> are said to be mutually independent if and only if</p>
<p><span class="math display">\[f_{\mathbf{X}}(\mathbf{x}) = \prod_{i=1}^{n} f_{X_i}\left(x_i\right).\]</span></p>
<dl>
<dt>Indicator Variable (<a href="06c-reg-extensions.html#def-indicator-variable">Definition&nbsp;<span>24.1</span></a>)</dt>
<dd>
An indicator variable is a binary variable (takes on the value 0 or 1), taking the value 1 when a specific event occurs. A collection of <span class="math inline">\(k-1\)</span> indicator variables can be used to capture a categorical variable with <span class="math inline">\(k\)</span> levels in a regression model.
</dd>
</dl>
<ul>
<li>The “reference group” (or reference level) is the group (level) defined by setting all indicator variables in a regression model to 0.</li>
</ul>
<dl>
<dt>Interquartile Range (<a href="02f-summaries.html#def-interquartile-range">Definition&nbsp;<span>7.5</span></a>)</dt>
<dd>
Often abbreviated as IQR, this is the distance between the first and third quartiles. This measure of spread indicates the range over which the middle 50% of the data is spread.
</dd>
<dt>Interval Estimation (<a href="03g-interval-estimation.html#def-interval-estimation">Definition&nbsp;<span>13.2</span></a>)</dt>
<dd>
Interval estimation is the process of estimating a parameter with a range of values. This is like trying to capture a target with a ring.
</dd>
<dt>Joint Density (<a href="03c-modelingsamples.html#def-joint-density">Definition&nbsp;<span>9.1</span></a>)</dt>
<dd>
For a random vector <span class="math inline">\(\mathbf{X}\)</span>, the function <span class="math inline">\(f_{\mathbf{X}}(\mathbf{x})\)</span> such that for any set <span class="math inline">\(A \in \mathbb{R}^n\)</span>, we have
</dd>
</dl>
<p><span class="math display">\[Pr(\mathbf{X} \in A) = \int \dotsi \int_{A} f_{\mathbf{X}}(\mathbf{x}) dx_1 \dotsb dx_n\]</span></p>
<p>is called the joint density function; this is also referred to as the <em>likelihood</em>. Integrals are replaced by sums when appropriate.</p>
<dl>
<dt>Kernel of a Distribution (<a href="01c-randomvariables.html#def-kernel">Definition&nbsp;<span>2.9</span></a>)</dt>
<dd>
Let <span class="math inline">\(k(x)\)</span> be a non-negative function of <span class="math inline">\(x\)</span> over some region <span class="math inline">\(\mathcal{S}_X\)</span>. Then, a valid density function <span class="math inline">\(f\)</span> over the support <span class="math inline">\(\mathcal{S}_X\)</span> can be constructed by taking
</dd>
</dl>
<p><span class="math display">\[f(x) = a k(x)\]</span></p>
<p>where <span class="math inline">\(a &gt; 0\)</span> is a suitably chosen scaling constant to ensure the density integrates (or sums) to 1 over the support. The function <span class="math inline">\(k\)</span> is known as the kernel of the distribution, and it can be used to identify the distributional family for a random variable.</p>
<dl>
<dt>Laplace Prior (<a href="03j-constructing-priors.html#def-laplace-prior">Definition&nbsp;<span>16.4</span></a>)</dt>
<dd>
The Laplace prior, also known as a “flat” prior, considers the form
</dd>
</dl>
<p><span class="math display">\[\pi(\theta) = 1 \qquad \forall \theta \in \Theta.\]</span></p>
<dl>
<dt>Link Function (<a href="06g-categorical-reg.html#def-link-function">Definition&nbsp;<span>28.2</span></a>)</dt>
<dd>
The functional form “linking” the linear predictor
</dd>
</dl>
<p><span class="math display">\[\beta_0 + \sum_{j=1}^{p} \beta_j (\text{Predictor } j)_i\]</span></p>
<p>to the mean response of the model in a regression model. In particular, it is the function <span class="math inline">\(g\)</span> such that</p>
<p><span class="math display">\[g\left(\theta_i\right) = \beta_0 + \sum_{j=1}^{p} \beta_j (\text{Predictor } j)_i.\]</span></p>
<p>Common link functions include:</p>
<ul>
<li>Identity link: <span class="math inline">\(g\left(\theta_i\right) = \theta_i\)</span></li>
<li>Logit link: <span class="math inline">\(g\left(\theta_i\right) = \log\left(\frac{\theta_i}{1 - \theta_i}\right)\)</span></li>
<li>Log link: <span class="math inline">\(g\left(\theta_i\right) = \log\left(\theta_i\right)\)</span></li>
<li>Inverse link: <span class="math inline">\(g\left(\theta_i\right) = \frac{1}{\theta_i}\)</span></li>
<li>Negative inverse link: <span class="math inline">\(g\left(\theta_i\right) = -\frac{1}{\theta_i}\)</span></li>
<li>Inverse squared link: <span class="math inline">\(g\left(\theta_i\right) = \frac{1}{\theta_i^2}\)</span></li>
</ul>
<dl>
<dt>Logistic Regression (<a href="06g-categorical-reg.html#def-logistic-regression">Definition&nbsp;<span>28.1</span></a>)</dt>
<dd>
Given a binary response; logistic regression assumes that
</dd>
</dl>
<p><span class="math display">\[
\begin{aligned}
  (\text{Response})_i &amp;\mid (\text{Predictors})_i, \boldsymbol{\beta} \stackrel{\text{Ind}}{\sim}Ber\left(\theta_i\right) \\
  \theta_i &amp;= \frac{e^{\beta_0 + \sum_{j=1}^{p} \beta_j (\text{Predictor } j)_i}}{1 + e^{\beta_0 + \sum_{j=1}^{p} \beta_j (\text{Predictor } j)_i}}.
\end{aligned}
\]</span></p>
<dl>
<dt>Marginal Density (<a href="03c-modelingsamples.html#def-marginal-density">Definition&nbsp;<span>9.2</span></a>)</dt>
<dd>
For a random vector <span class="math inline">\(\mathbf{X}\)</span>, the marginal density of the first component <span class="math inline">\(X_1\)</span> (without loss of generality) is
</dd>
</dl>
<p><span class="math display">\[f_{X_1}(u) = \int \dotsi \int f_{\mathbf{X}}(\mathbf{x}) dx_2 \dotsb dx_n.\]</span></p>
<dl>
<dt>Markov Chain (<a href="04c-mcmc.html#def-markov-chain">Definition&nbsp;<span>18.2</span></a>)</dt>
<dd>
A sequence of random vectors <span class="math inline">\(\theta^{(0)}, \theta^{(1)}, \theta^{(2)}, \dotsc, \theta^{(n)}\)</span> is a Markov Chain with stationary transition probabilities if for any set <span class="math inline">\(A\)</span> and any <span class="math inline">\(k \leq n\)</span>
</dd>
</dl>
<p><span class="math display">\[
\begin{aligned}
  Pr\left(\theta^{(k)} \in A \mid \theta^{(1)}, \theta^{(2)}, \dotsc, \theta^{(k-1)}\right)
    &amp;= Pr\left(\theta^{(k)} \in A \mid \theta^{(k-1)}\right) \\
    &amp;= \int_{A} q\left(\theta^{(k)} \mid \theta^{(k-1)}\right) d\theta^{(k)}
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(q\)</span> is called the transition kernel.</p>
<dl>
<dt>Method of Distribution Functions (<a href="01c-randomvariables.html#def-method-of-distribution-functions">Definition&nbsp;<span>2.11</span></a>)</dt>
<dd>
Let <span class="math inline">\(X\)</span> be a continuous random variable with density <span class="math inline">\(f\)</span> and cumulative distribution function <span class="math inline">\(F\)</span>. Consider <span class="math inline">\(Y = h(X)\)</span>. The following process provides the density function <span class="math inline">\(g\)</span> of <span class="math inline">\(Y\)</span> by first finding its cumulative distribution function <span class="math inline">\(G\)</span>.
</dd>
</dl>
<ol type="1">
<li>Find the set <span class="math inline">\(A\)</span> for which <span class="math inline">\(h(X) \leq t\)</span> if and only if <span class="math inline">\(X \in A\)</span>.</li>
<li>Recognize that <span class="math inline">\(G(y) = Pr(Y \leq y) = Pr\left(h(X) \leq y\right) = Pr(X \in A)\)</span>.</li>
<li>If interested in <span class="math inline">\(g(y)\)</span>, note that <span class="math inline">\(g(y) = \frac{\partial}{\partial y} G(y)\)</span>.</li>
</ol>
<dl>
<dt>Metropolis Algorithm (<a href="04c-mcmc.html#def-metropolis-algorithm">Definition&nbsp;<span>18.1</span></a>)</dt>
<dd>
Suppose we want to generate random variates from the density <span class="math inline">\(\pi(\theta \mid \mathbf{y})\)</span>. We perform the following steps:
</dd>
</dl>
<ol type="1">
<li>Generate an initial value <span class="math inline">\(\theta^{(0)}\)</span>.<br>
</li>
<li>At the <span class="math inline">\(k\)</span>-th step, generate <span class="math inline">\(\theta^*\)</span> (a candidate) according to a symmetric proposal density <span class="math inline">\(q\left(\theta \mid \theta^{(k-1)}\right)\)</span>.<br>
</li>
<li>Compute <span class="math inline">\(A\left(\theta^*, \theta^{(k-1)}\right)\)</span> where <span class="math display">\[A\left(\theta^*, \theta^{(k-1)}\right) = \frac{\pi\left(\theta^* \mid \mathbf{y}\right)}{\pi\left(\theta^{(k-1)} \mid \mathbf{y}\right)} = \frac{f\left(\mathbf{y} \mid \theta^*\right) \pi\left(\theta^*\right)}{f\left(\mathbf{y} \mid \theta^{(k-1)}\right) \pi\left(\theta^{(k-1)}\right)}.\]</span></li>
<li>Generate <span class="math inline">\(U \sim Unif(0,1)\)</span>. If <span class="math inline">\(U \leq A\left(\theta^*, \theta^{(k-1)}\right)\)</span>, then set <span class="math inline">\(\theta^{(k)} = \theta^*\)</span>; else, set <span class="math inline">\(\theta^{(k)} = \theta^{(k-1)}\)</span>.</li>
<li>Repeat Steps 2-4 <span class="math inline">\(m\)</span> times, for some large <span class="math inline">\(m\)</span>.</li>
</ol>
<p>When generating an initial value, <span class="math inline">\(\theta^{(0)}\)</span>, we could choose <span class="math inline">\(\theta^{(0)} \sim \pi(\theta)\)</span> if the prior is easy to generate from. While it is common to choose <span class="math inline">\(q(\cdot)\)</span> to be a Normal distribution with mean <span class="math inline">\(\theta^{(k-1)}\)</span>, it is not a requirement to do so; when a Normal distribution is used, it can be difficult to determine a reasonable variance (too large, and you drift too far away; too small, and you do not move at all).</p>
<dl>
<dt>Mixture Distribution (<a href="03i-hypothesis-testing.html#def-mixture-distribution">Definition&nbsp;<span>15.3</span></a>)</dt>
<dd>
Let <span class="math inline">\(X\)</span> be a random variable and <span class="math inline">\(f(x)\)</span> and <span class="math inline">\(g(x)\)</span> be valid density functions defined on a common support. Then,
</dd>
</dl>
<p><span class="math display">\[h(x) = wf(x) + (1 - w) g(x),\]</span></p>
<p>where <span class="math inline">\(0 &lt; w &lt; 1\)</span>, is known as a mixture distribution.</p>
<dl>
<dt>Monte Carlo Error (<a href="04b-mc-integration.html#def-mc-error">Definition&nbsp;<span>17.2</span></a>)</dt>
<dd>
Also called the standard error for an approximation of the form <span class="math inline">\(m^{-1} \sum\limits_{k=1}^{m} g\left(X_k\right)\)</span>, the MC error is given by
</dd>
</dl>
<p><span class="math display">\[\sqrt{\frac{1}{m(m-1)} \sum_{k=1}^{m} \left[g\left(X_k\right) - \frac{1}{m} \sum_{j=1}^{m} g\left(X_j\right)\right]^2}\]</span></p>
<p>which is the sample standard deviation of the generated variates divided by the square root of the number of replications.</p>
<dl>
<dt>Monte Carlo Integration (<a href="04b-mc-integration.html#def-mc-integration">Definition&nbsp;<span>17.1</span></a>)</dt>
<dd>
Consider an integral of the form
</dd>
</dl>
<p><span class="math display">\[\int_{\mathcal{S}} g(x) f(x) dx\]</span></p>
<p>where <span class="math inline">\(f(x)\)</span> is a valid density function for a random variable <span class="math inline">\(X\)</span> with support <span class="math inline">\(\mathcal{S}\)</span>. Then, the following algorithm, known as Monte Carlo (or MC) Integration, gives a numerical approximation to the integral:</p>
<ol type="1">
<li>Take a random sample <span class="math inline">\(X_1, X_2, \dotsc, X_m\)</span> such that <span class="math inline">\(X_i \sim f(x)\)</span> for all <span class="math inline">\(i\)</span>, where <span class="math inline">\(m\)</span> is large.</li>
<li>Compute <span class="math inline">\(m^{-1} \sum_{i=1}^{m} g\left(X_i\right)\)</span>.</li>
</ol>
<p>By the Law of Large Numbers,</p>
<p><span class="math display">\[\frac{1}{m} \sum_{i=1}^{m} g\left(X_i\right) \approx \int_{\mathcal{S}} g(x) f(x) dx.\]</span></p>
<dl>
<dt>Noninformative Prior (<a href="03j-constructing-priors.html#def-noninformative-prior">Definition&nbsp;<span>16.5</span></a>)</dt>
<dd>
A prior distribution which is derived solely based on the form of the likelihood.
</dd>
<dt>Null Hypothesis (<a href="02d-questions.html#def-null-hypothesis">Definition&nbsp;<span>5.9</span></a>)</dt>
<dd>
The statement (or theory) about the parameter that we would like to <em>disprove</em>. This is denoted <span class="math inline">\(H_0\)</span>, read “H-naught” or “H-zero”.
</dd>
<dt>Null Value (<a href="02d-questions.html#def-null-value">Definition&nbsp;<span>5.11</span></a>)</dt>
<dd>
The value associated with the equality component of the null hypothesis; it forms the threshold or boundary between the hypotheses. Note: not all questions of interest require a null value be specified.
</dd>
<dt>Numeric Variable (<a href="02b-basics.html#def-numeric">Definition&nbsp;<span>3.6</span></a>)</dt>
<dd>
Also called a “quantitative variable,” a measurement on a subject which takes on a numeric value <em>and</em> for which ordinary arithmetic makes sense.
</dd>
<dt>Observational Study (<a href="05b-study-design.html#def-observational-study">Definition&nbsp;<span>20.1</span></a>)</dt>
<dd>
A study in which each subject “self-selects” into one of groups being compared in the study. The phrase “self-selects” is used very loosely here and can include studies for which the groups are defined by an inherent characteristic or are chosen haphazardly.
</dd>
<dt>Outlier (<a href="02f-summaries.html#def-outlier">Definition&nbsp;<span>7.7</span></a>)</dt>
<dd>
An individual observation which is so extreme, relative to the rest of the observations in the sample, that it does not appear to conform to the same distribution.
</dd>
<dt>Parameter (<a href="02d-questions.html#def-parameter">Definition&nbsp;<span>5.6</span></a>)</dt>
<dd>
Numeric quantity which summarizes the distribution of a variable within the <em>population</em> of interest. Generally denoted by Greek letters in statistical formulas.
</dd>
<dt>Percentile (<a href="02f-summaries.html#def-percentile">Definition&nbsp;<span>7.1</span></a>)</dt>
<dd>
The <span class="math inline">\(k\)</span>-th percentile is the value <span class="math inline">\(q\)</span> such that <span class="math inline">\(k\)</span>% of the values in the distribution are less than or equal to <span class="math inline">\(q\)</span>. For example,
</dd>
</dl>
<ul>
<li>25% of values in a distribution are less than or equal to the 25-th percentile (known as the “first quartile” and denoted <span class="math inline">\(Q_1\)</span>).</li>
<li>50% of values in a distribution are less than or equal to the 50-th percentile (known as the “median”).</li>
<li>75% of values in a distribution are less than or equal to the 75-th percentile (known as the “third quartile” and denoted <span class="math inline">\(Q_3\)</span>).</li>
</ul>
<dl>
<dt>Percentile for a Random Variable (<a href="01c-randomvariables.html#def-population-percentile">Definition&nbsp;<span>2.8</span></a>)</dt>
<dd>
Let <span class="math inline">\(X\)</span> be a random variable with density function <span class="math inline">\(f\)</span>. The <span class="math inline">\(100k\)</span> percentile is the value <span class="math inline">\(q\)</span> such that
</dd>
</dl>
<p><span class="math display">\[Pr(X \leq q) = k.\]</span></p>
<dl>
<dt>Point Estimation (<a href="03g-interval-estimation.html#def-point-estimation">Definition&nbsp;<span>13.1</span></a>)</dt>
<dd>
Point estimation is the process of estimating a parameter with a single statistic. This is like trying to hit an infinitesimally small target with a dart.
</dd>
<dt>Population (<a href="02b-basics.html#def-population">Definition&nbsp;<span>3.1</span></a>)</dt>
<dd>
The collection of subjects we would like to say something about.
</dd>
<dt>Posterior Distribution (<a href="03e-posteriors.html#def-posterior-distribution">Definition&nbsp;<span>11.1</span></a>)</dt>
<dd>
A distribution quantifying our beliefs about the uncertainty in the parameter(s) of the underlying sampling distribution <em>after</em> observing data. This is often denoted by <span class="math inline">\(\pi(\boldsymbol{\theta} \mid \mathbf{y})\)</span> where <span class="math inline">\(\boldsymbol{\theta}\)</span> is the parameter vector and <span class="math inline">\(\mathbf{y}\)</span> the observe data.
</dd>
</dl>
<p>Given the likelihood <span class="math inline">\(f(\mathbf{y} \mid \boldsymbol{\theta})\)</span> and a prior distribution on the parameters <span class="math inline">\(\pi(\boldsymbol{\theta})\)</span>, the posterior distribution is computed using Bayes Theorem:</p>
<p><span class="math display">\[\pi(\boldsymbol{\theta} \mid \mathbf{y}) = \frac{f(\mathbf{y} \mid \boldsymbol{\theta}) \pi(\theta)}{\int f(\mathbf{y} \mid \boldsymbol{\theta}) \pi(\boldsymbol{\theta}) d\boldsymbol{\theta}}.\]</span></p>
<dl>
<dt>Posterior Mean (<a href="03f-point-estimation.html#def-posterior-mean">Definition&nbsp;<span>12.1</span></a>)</dt>
<dd>
The posterior mean is the average value of the parameter, given the data:
</dd>
</dl>
<p><span class="math display">\[E\left[\boldsymbol{\theta} \mid \mathbf{y}\right] = \int \boldsymbol{\theta} \pi(\theta \mid \mathbf{y}) d\boldsymbol{\theta}.\]</span></p>
<dl>
<dt>Posterior Median (<a href="03f-point-estimation.html#def-posterior-median">Definition&nbsp;<span>12.2</span></a>)</dt>
<dd>
We are 50% sure, given the data, the parameter falls below the posterior median. Formally, the posterior median is the value <span class="math inline">\(q\)</span> such that
</dd>
</dl>
<p><span class="math display">\[0.5 = \int_{-\infty}^{q} \pi(\theta \mid \mathbf{y}) d\theta.\]</span></p>
<dl>
<dt>Posterior Mode (<a href="03f-point-estimation.html#def-posterior-mode">Definition&nbsp;<span>12.3</span></a>)</dt>
<dd>
We think of the posterior mode as the most likely value of the parameter, given the data. If the posterior distribution is continuous, the posterior mode is the value of the parameter that maximizes the posterior distribution. Formally, the posterior mode is given by
</dd>
</dl>
<p><span class="math display">\[\arg \max_{\theta} \pi(\theta \mid \mathbf{y}).\]</span></p>
<dl>
<dt>Posterior Odds (<a href="03i-hypothesis-testing.html#def-posterior-odds">Definition&nbsp;<span>15.1</span></a>)</dt>
<dd>
Let <span class="math inline">\(H_j\)</span> denote the hypothesis that <span class="math inline">\(\theta \in \Theta_j\)</span> for some region <span class="math inline">\(\Theta_j\)</span>. Then, the posterior odds <em>in favor of</em> <span class="math inline">\(H_j\)</span> is given by
</dd>
</dl>
<p><span class="math display">\[\frac{Pr\left(\theta \in \Theta_j \mid \mathbf{y}\right)}{Pr\left(\theta \notin \Theta_j \mid \mathbf{y}\right)}.\]</span></p>
<dl>
<dt>Posterior Predictive Distribution (<a href="03h-prediction.html#def-posterior-predictive-distribution">Definition&nbsp;<span>14.2</span></a>)</dt>
<dd>
Let <span class="math inline">\(\mathbf{Y}^*\)</span> represent a collection of <span class="math inline">\(m\)</span> <em>future</em> observations. The distribution of these future observations given the observed data <span class="math inline">\(\mathbf{Y}\)</span> (of length <span class="math inline">\(n\)</span>), called the posterior predictive distribution, is given by
</dd>
</dl>
<p><span class="math display">\[\pi\left(\mathbf{y}^* \mid \mathbf{y}\right) = \int f\left(\mathbf{y}^* \mid \theta\right) \pi(\theta \mid \mathbf{y}) d\theta.\]</span></p>
<dl>
<dt>Potential (<a href="04c-mcmc.html#def-potential">Definition&nbsp;<span>18.4</span></a>)</dt>
<dd>
The potential of a value <span class="math inline">\(\theta\)</span> is the negative logarithm of the posterior evaluated at <span class="math inline">\(\theta\)</span>. In practice, we need only know the potential up to a constant. That is, it suffices to define the potential as
</dd>
</dl>
<p><span class="math display">\[\text{Potential}(\theta) = -\log\left[f(\mathbf{y} \mid \theta) \pi(\theta)\right].\]</span></p>
<dl>
<dt>Prior Distribution (<a href="03d-priors.html#def-prior-distribution">Definition&nbsp;<span>10.1</span></a>)</dt>
<dd>
A distribution quantifying our beliefs about uncertainty in the <em>parameter(s)</em> of the underlying sampling distribution <em>prior to</em> observing any data. This is often denoted by <span class="math inline">\(\pi(\boldsymbol{\theta})\)</span> where <span class="math inline">\(\boldsymbol{\theta}\)</span> is the parameter vector.
</dd>
</dl>
<ul>
<li>This relies on a <em>subjective</em> view of probability.</li>
<li>As prior beliefs are subjective, there is no “one” prior, but each individual may have a unique prior.</li>
</ul>
<dl>
<dt>Prior Predictive Distribution (<a href="03h-prediction.html#def-prior-predictive-distribution">Definition&nbsp;<span>14.1</span></a>)</dt>
<dd>
The prior predictive distribution is the marginal distribution of the response(s) prior to observing any data:
</dd>
</dl>
<p><span class="math display">\[m(\mathbf{y}) = \int f(\mathbf{y} \mid \theta) \pi(\theta) d\theta.\]</span></p>
<p>The distribution marginalizes the parameter out of the likelihood using the beliefs from the prior distribution.</p>
<dl>
<dt>Random Sample (<a href="03c-modelingsamples.html#def-random-sample">Definition&nbsp;<span>9.6</span></a>)</dt>
<dd>
A random sample of size <span class="math inline">\(n\)</span> refers to a collection of <span class="math inline">\(n\)</span> random variables <span class="math inline">\(X_1, X_2, \dotsc, X_n\)</span> such that the random variables are mutually independent, and the distribution of each random variable is identical.
</dd>
<dt>Random Variable (<a href="01c-randomvariables.html#def-random-variable">Definition&nbsp;<span>2.1</span></a>)</dt>
<dd>
Let <span class="math inline">\(\mathcal{S}\)</span> be the sample space corresponding to a random process; a random variable <span class="math inline">\(X\)</span> is a function mapping elements of the sample space to the real line.
</dd>
</dl>
<p>Random variables represent a measurement that will be collected during the course of a study. Random variables are typically represented by a capital letter.</p>
<dl>
<dt>Random Vector (<a href="03b-bayesrule.html#def-random-vector">Definition&nbsp;<span>8.2</span></a>)</dt>
<dd>
Let <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> be <span class="math inline">\(n\)</span> random variables. Then, the vector <span class="math inline">\(\mathbf{X} = \left(X_1, X_2, \dots, X_n\right)^\top\)</span> is a random vector of length <span class="math inline">\(n\)</span>.
</dd>
<dt>Randomization (<a href="05b-study-design.html#def-randomization">Definition&nbsp;<span>20.5</span></a>)</dt>
<dd>
Randomization can refer to random <em>selection</em> or random <em>allocation</em>. Random selection refers to the use of a random mechanism (e.g., a simple random sample, <a href="02e-data.html#def-simple-random-sample">Definition&nbsp;<span>6.2</span></a>, or a stratified random sample, <a href="02e-data.html#def-stratified-random-sample">Definition&nbsp;<span>6.3</span></a>) to select units from the population. Random selection minimizes bias.
</dd>
</dl>
<p>Random allocation refers to the use of a random mechanism when assigning units to a specific treatment group in a controlled experiment (<a href="05b-study-design.html#def-controlled-experiment">Definition&nbsp;<span>20.2</span></a>). Random allocation eliminates confounding and permits causal interpretations.</p>
<dl>
<dt>Reduction of Noise (<a href="05b-study-design.html#def-noise-reduction">Definition&nbsp;<span>20.6</span></a>)</dt>
<dd>
Reducing extraneous sources of variability can be accomplished by fixing extraneous variables or blocking (<a href="05b-study-design.html#def-blocking">Definition&nbsp;<span>20.7</span></a>). These actions reduce the number of differences between the units under study.
</dd>
<dt>Regression (<a href="06b-linear-regression.html#def-regression">Definition&nbsp;<span>23.1</span></a>)</dt>
<dd>
A regression model is one for which the parameter(s) governing the data generating process depends on one or more predictors. “Parametric” regression models do this through specifying a functional form for the dependence of the parameter(s) on the predictor(s).
</dd>
<dt>Relative Frequency (<a href="02d-questions.html#def-relative-frequency">Definition&nbsp;<span>5.5</span></a>)</dt>
<dd>
Also called the “proportion,” the fraction of observations falling into a particular group (level) of a categorical variable.
</dd>
<dt>Replication (<a href="05b-study-design.html#def-replication">Definition&nbsp;<span>20.4</span></a>)</dt>
<dd>
Replication results from taking measurements on different units (or subjects), for which you expect the results to be similar. That is, any variability across the units is due to natural variability within the population.
</dd>
<dt>Residual (<a href="06f-reg-conditions.html#def-residual">Definition&nbsp;<span>27.1</span></a>)</dt>
<dd>
A residual is the difference between an observed response and the predicted mean response for that same individual:
</dd>
</dl>
<p><span class="math display">\[(\text{Residual})_i = (\text{Response})_i - (\text{Fitted Value})_i,\]</span></p>
<p>where</p>
<p><span class="math display">\[(\text{Fitted Value})_i = \widehat{\beta}_0 + \sum_{j=1}^{p} \widehat{\beta}_j (\text{Predictor} j)_i.\]</span></p>
<dl>
<dt>Response (<a href="02d-questions.html#def-response">Definition&nbsp;<span>5.2</span></a>)</dt>
<dd>
The primary variable of interest within a study. This is the variable you would either like to explain or estimate.
</dd>
<dt>Sample (<a href="02b-basics.html#def-sample">Definition&nbsp;<span>3.2</span></a>)</dt>
<dd>
The collection of subjects for which we actually obtain measurements (data).
</dd>
<dt>Sample Space (<a href="01b-fundamentals.html#def-sample-space">Definition&nbsp;<span>1.1</span></a>)</dt>
<dd>
The sample space for a random process is the collection of all possible results that we might observe.
</dd>
<dt>Simple Random Sample (<a href="02e-data.html#def-simple-random-sample">Definition&nbsp;<span>6.2</span></a>)</dt>
<dd>
Often abbreviated SRS, this is a sample of size <span class="math inline">\(n\)</span> such that <em>every</em> collection of size <span class="math inline">\(n\)</span> is equally likely to be the resulting sample. This is equivalent to a lottery.
</dd>
<dt>Standard Deviation (<a href="02f-summaries.html#def-standard-deviation">Definition&nbsp;<span>7.4</span></a>)</dt>
<dd>
A measure of spread, this is the square root of the variance.
</dd>
<dt>Stationary Distribution (<a href="04c-mcmc.html#def-stationary-distribution">Definition&nbsp;<span>18.3</span></a>)</dt>
<dd>
Let <span class="math inline">\(\theta^{(0)}, \theta^{(1)}, \theta^{(2)}, \dotsc, \theta^{(n)}\)</span> be a Markov Chain. The stationary distribution of the Markov Chain is the distribution <span class="math inline">\(p(\theta)\)</span> such that
</dd>
</dl>
<p><span class="math display">\[Pr\left(\theta^{(k)} \in A\right) = \int_{A} p(\theta) d\theta.\]</span></p>
<dl>
<dt>Statistic (<a href="02f-summaries.html#def-statistic">Definition&nbsp;<span>7.8</span></a>)</dt>
<dd>
Numeric quantity which summarizes the distribution of a variable within a <em>sample</em>.
</dd>
<dt>Statistical Inference (<a href="02b-basics.html#def-inference">Definition&nbsp;<span>3.3</span></a>)</dt>
<dd>
The process of using a sample to characterize some aspect of the underlying population.
</dd>
<dt>Stratified Random Sample (<a href="02e-data.html#def-stratified-random-sample">Definition&nbsp;<span>6.3</span></a>)</dt>
<dd>
A sample in which the population is first divided into groups, or strata, based on a characteristic of interest; a simple random sample is then taken within each group.
</dd>
<dt>Subjective Interpretation of Probability (<a href="01b-fundamentals.html#def-subjective-interpretation">Definition&nbsp;<span>1.4</span></a>)</dt>
<dd>
In this perspective, the probability of <span class="math inline">\(A\)</span> describes the individual’s uncertainty about event <span class="math inline">\(A\)</span>.
</dd>
<dt>Support (<a href="01c-randomvariables.html#def-support">Definition&nbsp;<span>2.2</span></a>)</dt>
<dd>
The support of a random variable is the set of all possible values the random variable can take.
</dd>
<dt>Variability (<a href="02d-questions.html#def-variability">Definition&nbsp;<span>5.1</span></a>)</dt>
<dd>
The notion that measurements differ from one observation to another.
</dd>
<dt>Variable (<a href="02b-basics.html#def-variable">Definition&nbsp;<span>3.4</span></a>)</dt>
<dd>
A measurement, or category, describing some aspect of the subject.
</dd>
<dt>Variance (<a href="02f-summaries.html#def-variance">Definition&nbsp;<span>7.3</span></a>)</dt>
<dd>
Let <span class="math inline">\(X\)</span> be a random variable with density function <span class="math inline">\(f\)</span> defined over the support <span class="math inline">\(\mathcal{S}\)</span>. The variance of a random variable, denoted <span class="math inline">\(Var(X)\)</span>, is given by
</dd>
</dl>
<p><span class="math display">\[Var(X) = E\left[X - E(X)\right]^2 = E\left(X^2\right) - E^2(X).\]</span></p>
<p>If we let <span class="math inline">\(\mu = E(X)\)</span>, then this is equivalent to</p>
<p><span class="math display">\[\int_{\mathcal{S}} (x - \mu)^2 f(x) dx\]</span></p>
<p>for continuous random variables and</p>
<p><span class="math display">\[\sum_{\mathcal{S}} (x - \mu)^2 f(x)\]</span></p>
<p>for discrete random variables.</p>
<dl>
<dt>Variance (<a href="02f-summaries.html#def-variance">Definition&nbsp;<span>7.3</span></a>)</dt>
<dd>
A measure of spread, this roughly captures the average distance values in the distribution are from the mean.
</dd>
</dl>
<p>For a sample of size <span class="math inline">\(n\)</span>, it is computed by <span class="math display">\[s^2 = \frac{1}{n-1}\sum_{i=1}^{n} \left(x_i - \bar{x}\right)^2\]</span></p>
<p>where <span class="math inline">\(\bar{x}\)</span> is the sample mean and <span class="math inline">\(x_i\)</span> is the <span class="math inline">\(i\)</span>-th value in the sample. The division by <span class="math inline">\(n-1\)</span> instead of <span class="math inline">\(n\)</span> removes bias in the statistic.</p>
<p>The symbol <span class="math inline">\(\sigma^2\)</span> is often used to denote the variance in the population.</p>



</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./references.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">References</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->



</body></html>