# Modeling Samples {#sec-modelingsamples}

{{< include _setupcode.qmd >}}

Rarely is our data a single observation. Instead, we collect a sample of observations. As a result, we must be able to comfortably model a _collection_ of random variables.

In a probability course, we are generally concerned with modeling a single random variable. The course may build into modeling the joint distribution of two random variables, but generally not further. However, if each random variable represents a single measurement taken on a unit observation, then taking measurements on a sample of $n$ observations is actually a collection of $n$ random variables. Part of quantifying our uncertainty in a parameter is first modeling the process that generated the data as a function of that parameter; that means, we need to model the distribution of the responses. 

:::{.callout-note}
We often talk about "modeling the data," but that is not precise language. The _data_ is fixed; it is not modeled. We are modeling the _process_ which generated that data --- it is this process that produces random values which are then observed.
:::

Let $X_1, X_2, \dotsc, X_n$ represent $n$ observations of the same variable we intend to make (note the future tense).  We group these in a random vector $\bm{X}$ of length $n$.  Not only are we interested in how each _element_ in $\bm{X}$ is distributed, we are also interested in modeling how they are interrelated.

:::{#def-joint-density}
## Joint Density
For a random vector $\bm{X}$, the function $f_{\bm{X}}(\bm{x})$ such that for any set $A \in \mathbb{R}^n$, we have

$$Pr(\bm{X} \in A) = \int \dotsi \int_{A} f_{\bm{X}}(\bm{x}) dx_1 \dotsb dx_n$$

is called the joint density function; this is also referred to as the _likelihood_.  Integrals are replaced by sums when appropriate.  
:::

:::{.callout-tip}
## Big Idea
Probabilities involving multiple random variables involve integration over the joint density.
:::

The joint density describes how the elements move together; if we are interested in only a single element, we consider the marginal density, which is accomplished by integrating (or summing) over all possible values for the _other_ elements.

:::{#def-marginal-density}
## Marginal Density
For a random vector $\bm{X}$, the marginal density of the first component $X_1$ (without loss of generality) is

$$f_{X_1}(u) = \int \dotsi \int f_{\bm{X}}(\bm{x}) dx_2 \dotsb dx_n.$$
:::

Bayes Theorem, and therefore Bayesian data analysis, is primarily concerned with conditional densities, which also generalize to random vectors.

:::{#def-conditional-density}
## Conditional Density
Let $\bm{X}$ be a random vector; without loss of generality, partition $\bm{X}$ such that

$$\bm{X} = \begin{pmatrix} \bm{X}_1 \\ \bm{X}_2 \end{pmatrix}$$

where $\bm{X}_1$ represents the first $k$ components and $\bm{X}_2$ represents the remaining $n-k$ components.  Then, the conditional density of $\bm{X}_1$ given $\bm{X}_2$ is 

$$f_{\bm{X}_1 \mid \bm{X}_2}(\bm{x}_1 \mid \bm{x}_2) = \frac{f_{\bm{X}}(\bm{x})}{f_{\bm{X}_2}(\bm{x}_2)}.$$
:::

It is worth noting that with respect to the components of interest $\bm{X}_1$, the denominator in the conditional density is just a constant scaling factor to ensure the density integrates/sums to 1; that is, _with respect to the variables of interest_, the denominator is a constant.

:::{.callout-note}
When we are working with named distributions, using statistical software to compute probabilities is often superior to generic calculus software. This is because the algorithms for computing these probabilities are more stable for known distributions than general all-purpose numerical integration methods.
:::


## Independent and Identically Distributed
The above discussion, while accurate, is unrealistic in that it begins with a completely formed likelihood.  In reality, we must posit models which correspond to the data generating process.  Positing a model for the distribution of an individual observation (element of $\bm{X}$) often means choosing from among well-known named probability models.  Regardless of whether a named model is used or a custom model constructed, the process always involves examining the context to determine an appropriate structure --- the shape and support --- of the distribution.  We then allow the parameters of this distribution to remain unknown.  This is where we turn from probability to statistics --- suddenly, our models are only partly known, and there are some aspects (the parameters governing the behavior of the model) which are unknown. We will use data to make some statements about these parameters to address questions of interest which are framed in terms of these parameters.

Instead of trying to model the joint distribution of the observed data directly, we often model the variability in the individual observations. We then place additional conditions on the relationship between the observations in order to develop the joint distribution. One of the most popular conditions is that of independence.

:::{#def-independence}
## Independence
Random variables $X_1, X_2, \dotsc, X_n$ are said to be mutually independent (or just "independent") if and only if

$$Pr\left(X_1 \in A_1, X_2 \in A_2, \dotsb, X_n \in A_n\right) = \prod_{i=1}^{n} Pr\left(X_i \in A_i\right),$$

where $A_1, A_2, \dotsc, A_n$ are arbitrary sets.  Perhaps more helpful, $X_1, X_2, \dotsc, X_n$ are said to be mutually independent if and only if

$$f_{\bm{X}}(\bm{x}) = \prod_{i=1}^{n} f_{X_i}\left(x_i\right).$$
:::

:::{.callout-note}
For those not familiar, $\prod_{i=1}^n a_i$ is the _product operator_.  It is analogous to $\sum_{i=1}^{n} a_i$, but uses products instead of sums.
:::

Essentially, a random variable $X$ is said to be independent of $Y$ if the likelihood that $X$ takes a particular value is the same regardless of the value $Y$ takes.  
   
Assuming independence allows us to easily construct joint densities by taking the product of the marginal density for each observation.  Independence is a powerful condition when constructing likelihoods. However, it cannot be blindly enforced; we should take caution when assuming independence. This requires considering the method in which the data was obtained to determine if it is reasonable that the value of one observation does not affect the likelihood of any other observation.

Ideally, when we take a sample, each observation is representative of the same process.  This is what allows us to use all observations in a sample in order to make inference --- we believe that each observation is able to contribute information about the unknown parameter.  Believing that each observation is representative of the same process is essentially assume that each corresponding random variable (prior to observing the data) has the same distribution.  

:::{#def-identically-distributed}
## Identically Distributed
We say that random variables $X$ and $Y$ are identically distributed if $F_X(u) = F_Y(u)$ for all $u$.  This is equivalent to saying the two random variables have the same density function $f$.
:::

:::{.callout-warning}
Let $X$ and $Y$ be identically distributed random variables.  This does not mean that $X = Y$.  "Identically distributed" says the two random variables have the same distribution, not the same value.  As a result, they share the same mean, variance, etc.
:::

When the observations in our sample are both independent and identically distributed, we say we have a "random sample."

:::{#def-random-sample}
## Random Sample
A random sample of size $n$ refers to a collection of $n$ random variables $X_1, X_2, \dotsc, X_n$ such that the random variables are mutually independent, and the distribution of each random variable is identical.
:::


:::{#exm-csec}
## Delivery by Cesarean Section (C-section)
It is sometimes necessary for babies to be delivered through a surgical procedure known as a Cesarean Section (C-section).  As surgical procedures carry risk, a C-section is typically performed when a vaginal delivery would place the infant or mother in undue risk of complications.  Suppose we are interested in characterizing the hospital experiences of mothers who have undergone a C-section at Union Hospital in Terre Haute, Indiana.  

For this community health project, we would like to survey $n = 15$ mothers who have undergone a C-section.  Of course, not every delivery is a C-section; let $X_i$ represent the number of vaginal deliveries that occur _between_ the $i$-th C-section and the previous C-section we observe.

Suppose we are willing to believe that (absent any additional information on the pregnancy) each patient in the labor and delivery ward has the same probability of undergoing a C-section; further, whether one patient undergoes a C-section is independent of any other patient undergoing a C-section.  Develop a model for the likelihood of the data to be observed.
:::

:::{.solution}
We begin by thinking about the specific context.  Note, for example, that $X_i$ is a non-negative integer; that is, $X_i \in \{0, 1, 2, \dotsc \}$.  Let $\theta$ represent the probability that a patient undergoes a C-section (and therefore $1 - \theta$ represents the probability of a vaginal birth).  Since we believe the method of delivery for one patient is independent of the method of delivery for all other patients, and that each probability of a delivery by C-section is the same for each patient, then it is reasonable to state that

$$Pr\left(X_i = x\right) = \theta (1 - \theta)^x \qquad x = 0, 1, 2, \dotsc.$$

That is, $X_i$ follows a Geometric distribution with parameter $\theta$.  This distribution captures the idea that $x$ vaginal deliveries occur (each with probability $1 - \theta$) before we see the $i$-th C-section (which occurs with probability $\theta$).

Further, since each birth is independent, we can consider $X_1, X_2, \dotsc, X_n$ to be a random sample.  Letting $\bm{X} = \left(X_1, X_2, \dotsc, X_n\right)^\top$ be the random vector of observations, the likelihood is given by

$$
\begin{aligned}
  f(\bm{x} \mid \theta)
    &= \prod_{i=1}^{n} f_{X_i}\left(x_i \mid \theta\right) \\
    &= \prod_{i=1}^{n} \theta (1 - \theta)^{x_i} \\
    &= \theta^n (1 - \theta)^{\sum_{i=1}^{n} x_i} \\
    &= \theta^n (1 - \theta)^{n\bar{x}}.
\end{aligned}
$$ {#eq-csec-likelihood}

Note that line (1) makes use of the independence to say the likelihood is the product of the marginal density functions of each observation.  Line (2) makes use of that each observation is identically distributed; this means that each observation has the same functional family for the density and is governed by the same parameter.  This still allows $x_i$ to differ from $x_j$, but the distribution is the same.  Line (3) brings the product through the expression, with the product of exponentials with the same base resulting in adding the exponents.  Line (4) simplifies the expression; for notational simplicity, we prefer $n\bar{x}$ to $\sum_{i=1}^{n} x_i$, though the two are equivalent.

We also note that the likelihood expressly acknowledges the dependence on the parameter $\theta$ by using $f(\bm{x} \mid \theta)$ instead of just $f(\bm{x})$.
:::

:::{.callout-note}
It is helpful to be in the habit acknowledging the dependence of the likelihood on the parameter.
:::

:::{.callout-tip}
## Big Idea
By placing conditions on how the data is generated, we are able to model the joint distribution of the responses.  This is sometimes referred to as the _likelihood_ of the unknown parameters; we also refer to it as the model for the data generating process, as it explains the variability in the observed data.
:::